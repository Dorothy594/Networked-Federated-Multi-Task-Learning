{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import functools\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "def get_matrices(weight_vec, B):\n",
    "    Sigma = np.diag(np.full(weight_vec.shape, 0.9 / 2))\n",
    "\n",
    "    D = B\n",
    "\n",
    "    Gamma_vec = np.array((1.0 / (np.sum(abs(B), 0)))).ravel()\n",
    "    Gamma = np.diag(Gamma_vec)\n",
    "\n",
    "    if np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(Gamma ** 0.5), 2) > 1:\n",
    "        print ('product norm', np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(Gamma ** 0.5), 2))\n",
    "        # raise Exception('higher than 1')\n",
    "    return Sigma, Gamma, Gamma_vec, D\n",
    "\n",
    "\n",
    "def algorithm_1(K, B, weight_vec, X, Y, samplingset, lambda_lasso, score_func=mean_squared_error, loss_func='linear_reg'):\n",
    "    # calculate the needed matrices for the algorithm 1 from adjacency matrix and weight vector of the empirical graph G\n",
    "    Sigma, Gamma, Gamma_vec, D = get_matrices(weight_vec, B)\n",
    "\n",
    "    E, N = B.shape\n",
    "    m, n = X[0].shape\n",
    "\n",
    "    # initialize the loss funcion\n",
    "    if loss_func == 'linear_reg':\n",
    "        optimizer = LinearOptimizer(samplingset, Gamma_vec, X, Y)\n",
    "    elif loss_func == 'logistic_reg':\n",
    "        optimizer = LogisticOptimizer(Gamma_vec, X, Y)\n",
    "    else:\n",
    "        print('invalid loss_func')\n",
    "        return\n",
    "\n",
    "    new_w = np.array([np.zeros(n) for i in range(N)])\n",
    "    new_u = np.array([np.zeros(n) for i in range(E)])\n",
    "\n",
    "    iteration_scores = []\n",
    "    limit = np.array([np.zeros(n) for i in range(E)])\n",
    "    for i in range(n):\n",
    "        limit[:, i] = lambda_lasso * weight_vec\n",
    "\n",
    "    for iterk in range(K):\n",
    "        if iterk % 100 == 0:\n",
    "            print ('iter:', iterk)\n",
    "            \n",
    "        prev_w = np.copy(new_w)\n",
    "\n",
    "        hat_w = new_w - np.dot(Gamma, np.dot(D.T, new_u))\n",
    "\n",
    "        for i in range(N):\n",
    "            if i in samplingset:\n",
    "                # line 3 of algorithm 1\n",
    "                new_w[i] = optimizer.optimize(i, hat_w)\n",
    "            else:\n",
    "                # line 6 of algorithm 1\n",
    "                new_w[i] = hat_w[i]\n",
    "\n",
    "        # line 8 of algorithm 1\n",
    "        new_u = new_u + np.dot(Sigma, np.dot(D, 2 * new_w - prev_w))\n",
    "\n",
    "        # line 9 of algorithm 1\n",
    "        normalized_u = np.where(abs(new_u) >= limit)\n",
    "        new_u[normalized_u] = limit[normalized_u] * new_u[normalized_u] / abs(new_u[normalized_u])\n",
    "\n",
    "        Y_pred = []\n",
    "        for i in range(N):\n",
    "            Y_pred.append(np.dot(X[i], new_w[i]))\n",
    "\n",
    "        iteration_scores.append(score_func(Y.reshape(N, m), Y_pred))\n",
    "\n",
    "    # print (np.max(abs(new_w - prev_w)))\n",
    "\n",
    "    return iteration_scores, new_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_Width = 150\n",
    "Image_Height = 150\n",
    "Image_Size = (Image_Width, Image_Height)\n",
    "Image_Channels = 3\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "# get the base(pre-trained) model data\n",
    "def get_base_model_data():\n",
    "    data_augmentation = keras.Sequential(\n",
    "        [\n",
    "            layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "            layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    base_model = keras.applications.Xception(\n",
    "        weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
    "        input_shape=(Image_Width, Image_Height, Image_Channels),\n",
    "        include_top=False,\n",
    "    )  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "    # Freeze the base_model\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Create new model on top\n",
    "    inputs = keras.Input(shape=(Image_Width, Image_Height, Image_Channels))\n",
    "    x = data_augmentation(inputs)  # Apply random data augmentation\n",
    "\n",
    "    # Pre-trained Xception weights requires that input be normalized\n",
    "    # from (0, 255) to a range (-1., +1.), the normalization layer\n",
    "    # does the following, outputs = (inputs - mean) / sqrt(var)\n",
    "    norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
    "    mean = np.array([127.5] * 3)\n",
    "    var = mean ** 2\n",
    "    # Scale inputs to [-1, +1]\n",
    "    x = norm_layer(x)\n",
    "    norm_layer.set_weights([mean, var])\n",
    "\n",
    "    # The base model contains batchnorm layers. We want to keep them in inference mode\n",
    "    # when we unfreeze the base model for fine-tuning, so we make sure that the\n",
    "    # base_model is running in inference mode here.\n",
    "    x = base_model(x, training=False)\n",
    "    return x, inputs\n",
    "\n",
    "\n",
    "# get the base(pre-trained) model\n",
    "def get_base_model():\n",
    "    x, inputs = get_base_model_data()\n",
    "    outputs = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# get the new(trainable) model\n",
    "def get_new_model():\n",
    "    inputs = keras.Input(shape=(2048,))\n",
    "\n",
    "    x = keras.layers.Dropout(0.2)(inputs)\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "\n",
    "    extra_model = keras.Model(inputs, outputs)\n",
    "\n",
    "    extra_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "\n",
    "    return extra_model\n",
    "\n",
    "\n",
    "# get the model\n",
    "def get_NN_model():\n",
    "    # base model\n",
    "    x, inputs = get_base_model_data()\n",
    "\n",
    "    # new model\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "\n",
    "    for layer in model.layers[:-1]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# calculate base model output and true labels for all images\n",
    "def get_base_model_output():\n",
    "\n",
    "    # get the base model\n",
    "    base_model = get_base_model()\n",
    "\n",
    "    # load the data from tensorflow dataset (all the images)\n",
    "    (dataset,), metadata = tfds.load(\n",
    "        \"cats_vs_dogs\",\n",
    "        split=[\"train[:100%]\"],\n",
    "        shuffle_files=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "\n",
    "    # resize the images of the dataset to the standard size\n",
    "    dataset = dataset.map(lambda item: (tf.image.resize(item['image'], Image_Size), item['label']))\n",
    "    dataset = dataset.cache().batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
    "\n",
    "    # get the output of the base model for the dataset\n",
    "    base_model_outputs = base_model.predict(dataset)\n",
    "    '''\n",
    "    base_model_output: A list containing the output of the base(pre-trained) model for all the images\n",
    "    '''\n",
    "\n",
    "    # obtain the true labels for the dataset\n",
    "    true_labels = []\n",
    "    for obj in dataset:\n",
    "        true_labels += list(np.array(obj[1]))\n",
    "    true_labels = np.array(true_labels)\n",
    "    '''\n",
    "    true_labels: A list containing the true label of all the images (which is 0 or 1 for each image)\n",
    "    '''\n",
    "\n",
    "    return base_model_outputs, true_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved(Trained) Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Return the output of the base model and the train vector for the training dataset of this model\n",
    "def get_model_train_images_data(train_data_image_names, all_images_indices, all_images_size, base_model_outputs, true_labels):\n",
    "    train_images_vector = np.zeros(all_images_size)\n",
    "    base_model_output = []\n",
    "    model_labels = []\n",
    "    for train_image_name in train_data_image_names:\n",
    "        index = all_images_indices[train_image_name]\n",
    "        train_images_vector[index] = 1\n",
    "        model_labels.append(true_labels[index])\n",
    "        item_predict = np.concatenate((base_model_outputs[index], [1]))  # [1] is for the bias (b)\n",
    "        base_model_output.append(item_predict)\n",
    "    return base_model_output, train_images_vector, model_labels\n",
    "\n",
    "\n",
    "def get_all_images_dict(data):\n",
    "    all_images_indices = {}\n",
    "    cnt = 0\n",
    "    for train_data in data:\n",
    "        for i, file_name in enumerate(train_data['train_df']):\n",
    "            if file_name not in all_images_indices:\n",
    "                all_images_indices[file_name] = cnt\n",
    "                cnt += 1\n",
    "    return all_images_indices\n",
    "\n",
    "\n",
    "def get_trained_model_weights(raw_model_weights):\n",
    "    model_weights = []\n",
    "    for weight in raw_model_weights[-2:]:\n",
    "        model_weights.append(np.array(weight))\n",
    "    model_weights = np.array(model_weights)\n",
    "    return model_weights\n",
    "\n",
    "\n",
    "# get the trained dataset and weights of each trained model and also the features of algorithm 1\n",
    "def parse_saved_data(data, base_model_output, true_labels):\n",
    "\n",
    "    all_images_indices = get_all_images_dict(data)\n",
    "    '''\n",
    "    all_images_indices: a dictionary from image_name to index\n",
    "    '''\n",
    "    all_images_size = len(all_images_indices.keys())\n",
    "    '''\n",
    "    all_images_size : total number of images of the (tensorflow) dataset\n",
    "    '''\n",
    "\n",
    "    all_models_train_images = []\n",
    "    all_models_weights = []\n",
    "    X = []\n",
    "    '''\n",
    "    X: A list containing the output of the base model for trainset of each model, which is the features of algorithm 1\n",
    "    '''\n",
    "    Y = []\n",
    "    '''\n",
    "    Y: A list containing the true labels for trainset of each model, which is the labels of algorithm 1\n",
    "    '''\n",
    "    for model_data in data:\n",
    "\n",
    "        base_model_train_images_output, model_train_images, model_labels = get_model_train_images_data(model_data['train_df'], all_images_indices, all_images_size, base_model_output, true_labels)\n",
    "        '''\n",
    "        base_model_train_images_output: the output of the base model for the training dataset of this model\n",
    "        model_train_images: a vector from 0/1 with the size of \"all_images_size\", model_train_images[i] = 1 if \n",
    "                the image with the index i is in the train dataset of this model otherwise model_train_images[i] = 0\n",
    "        model_labels: the true labels for the training dataset of this model\n",
    "        '''\n",
    "\n",
    "        X.append(np.array(base_model_train_images_output))\n",
    "        Y.append(np.array(model_labels))\n",
    "\n",
    "        all_models_train_images.append(model_train_images)\n",
    "\n",
    "        model_weights = get_trained_model_weights(model_data['weights'])\n",
    "        '''\n",
    "        model_weights: the weights of this model for the new model (trainable layers)\n",
    "        '''\n",
    "        all_models_weights.append(model_weights)\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return all_models_train_images, all_models_weights, X, Y\n",
    "\n",
    "\n",
    "# read the trained models data from saved files\n",
    "def read_trained_data_from_saved_files(train_data_dir):\n",
    "    data = []\n",
    "    for filename in sorted(os.listdir(train_data_dir)):\n",
    "        if '.json' not in filename:\n",
    "            continue\n",
    "        num = filename.split('_')[-1].replace('.json', '')\n",
    "        if int(num) >= 50:\n",
    "            continue\n",
    "        with open(os.path.join(train_data_dir, filename), 'r') as f:\n",
    "            data.append(json.load(f))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_trained_data(train_data_dir, base_model_output, true_labels):\n",
    "\n",
    "    # read the trained models data from saved files\n",
    "    data = read_trained_data_from_saved_files(train_data_dir)\n",
    "    '''\n",
    "    data: saved data from the trained models\n",
    "    '''\n",
    "\n",
    "    # get the trained dataset and weights of each trained model and also the features of algorithm 1\n",
    "    all_models_train_images, all_models_weights, X, Y = parse_saved_data(data, base_model_output, true_labels)\n",
    "    '''\n",
    "    all_models_train_images: A list containing the images used for training each model\n",
    "    all_models_weights : A list containing the weight of the new model based on training each model\n",
    "    X : A list containing the output of the base model for trainset of each model, which is the features of algorithm 1\n",
    "    Y : A list containing the true labels for trainset of each model, which is the labels of algorithm 1\n",
    "    '''\n",
    "\n",
    "    return all_models_train_images, all_models_weights, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the distance between the given nodes of the graph\n",
    "def get_dist(first_node, second_node):\n",
    "    all_equals = np.where(first_node == second_node)[0]\n",
    "    equal_train_images = np.where(first_node[all_equals] == 1)[0]\n",
    "    dist = len(equal_train_images) / len(np.where(first_node == 1)[0])\n",
    "    return dist\n",
    "\n",
    "\n",
    "# calculate the adjacency matrix and the weight vector of the empirical graph G\n",
    "def get_B_and_weight_vec(all_models_train_images, neigh_cnt=3):\n",
    "    '''\n",
    "    \n",
    "    :param trained_models_train_images: A list containing the images used for training each model\n",
    "    :param neigh_cnt: number of the neighbors for each node of the empirical graph G\n",
    "\n",
    "    '''\n",
    "    \n",
    "    N = len(all_models_train_images)\n",
    "    E = int(N * (N - 1) / 2)\n",
    "\n",
    "    weight_vec = np.zeros(E)\n",
    "    '''\n",
    "    the weight vector of the edges of the empirical graph G\n",
    "    '''\n",
    "    B = np.zeros((E, N))\n",
    "    '''\n",
    "    the adjacency matrix of the empirical graph G\n",
    "    '''\n",
    "    \n",
    "    cnt = 0\n",
    "    '''\n",
    "    number of edges of the empirical graph G\n",
    "    '''\n",
    "    for i in range(N):\n",
    "        node_dists = []\n",
    "        '''\n",
    "        a list containing the distance between node i and other nodes of the graph\n",
    "        '''\n",
    "        for j in range(N):\n",
    "            if j == i:\n",
    "                continue\n",
    "            node_dists.append(get_dist(all_models_train_images[i], all_models_train_images[j]))\n",
    "        \n",
    "        # sort node_dists in order to pick the nearest nodes to the node i \n",
    "        node_dists.sort(reverse=True)\n",
    "\n",
    "        node_cnt = 0\n",
    "        for j in range(N):\n",
    "            \n",
    "            if node_cnt >= neigh_cnt:\n",
    "                break\n",
    "                \n",
    "            if j == i:\n",
    "                continue\n",
    "                \n",
    "            # calculate the distance between node i and j of the graph\n",
    "            dist = get_dist(all_models_train_images[i], all_models_train_images[j])\n",
    "            if dist == 0 or dist < node_dists[neigh_cnt]:\n",
    "                continue\n",
    "\n",
    "            node_cnt += 1\n",
    "            B[cnt][i] = 1\n",
    "            B[cnt][j] = -1\n",
    "            weight_vec[cnt] = dist\n",
    "            cnt += 1\n",
    "\n",
    "    B = B[:cnt, :]\n",
    "    weight_vec = weight_vec[:cnt]\n",
    "    return B, weight_vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# calculate new model output for all images\n",
    "def get_new_model_output(new_model_weights, base_model_output):\n",
    "    new_model_weights = [np.array(new_model_weights[:-1]).reshape(-1, 1), np.array(new_model_weights[-1:])]\n",
    "\n",
    "    new_model = get_new_model()\n",
    "    new_model.set_weights(new_model_weights)\n",
    "\n",
    "    model_predicts = new_model.predict(base_model_output).flatten()\n",
    "    model_predicts[model_predicts > 0] = 1\n",
    "    model_predicts[model_predicts <= 0] = 0\n",
    "\n",
    "    return model_predicts\n",
    "\n",
    "\n",
    "def save_figures(alg1_estimated_weights, original_weights, lambda_lasso, base_model_output, true_labels):\n",
    "    '''\n",
    "\n",
    "    :param alg1_estimated_weights: A list containing the weights of the models estimated by algorithm 1\n",
    "    :param original_weights: A list containing the weights of the models based on training each model\n",
    "    :param lambda_lasso: lambda_lasso parameter used for algorithm 1\n",
    "    :param base_model_output: A list containing the output of the base model(pre-trained model) for all the images\n",
    "    :param true_labels: A list containing the true label of all the images\n",
    "\n",
    "    '''\n",
    "\n",
    "    N = len(alg1_estimated_weights)\n",
    "\n",
    "    alq1_scores = []  # blue curve\n",
    "    trained_model_scores = []  # orange curve\n",
    "\n",
    "    for i in range(N):\n",
    "\n",
    "        # the trained model output for all images\n",
    "        trained_model_output = get_new_model_output(original_weights[i], base_model_output)\n",
    "        # orange curve\n",
    "        trained_model_score = np.where(true_labels == trained_model_output)[0].shape[0] / len(true_labels)\n",
    "        trained_model_scores.append(trained_model_score)\n",
    "\n",
    "        # alg1 output for all images\n",
    "        alg1_output = get_new_model_output(alg1_estimated_weights[i], base_model_output)\n",
    "        # blue curve\n",
    "        alg1_score = np.where(true_labels == alg1_output)[0].shape[0] / len(alg1_output)\n",
    "        alq1_scores.append(alg1_score)\n",
    "\n",
    "    x_axis = [i for i in range(N)]\n",
    "    plt.close()\n",
    "    plt.plot(x_axis, alq1_scores, label='our')\n",
    "    plt.plot(x_axis, trained_model_scores, label='deep learning')\n",
    "    plt.title('alg1 vs trained accuracy')\n",
    "    plt.xlabel('model')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig('deep_learning_lasso/train_accuracy_%s.png' % lambda_lasso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def optimize(self, idx, hat_w):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearOptimizer(Optimizer):\n",
    "\n",
    "    def __init__(self, samplingset, Gamma_vec, X, Y):\n",
    "        super(Optimizer).__init__()\n",
    "        self.MTX1_INV, self.MTX2 = self.get_preprocessed_matrices(samplingset, Gamma_vec, X, Y)\n",
    "\n",
    "    def get_preprocessed_matrices(self, samplingset, Gamma_vec, X, Y):\n",
    "        MTX1_INV = {}\n",
    "        MTX2 = {}\n",
    "        for i in samplingset:\n",
    "            mtx1 = 2 * Gamma_vec[i] * np.dot(X[i].T, X[i]).astype('float64')\n",
    "            if mtx1.shape:\n",
    "                mtx1 += 1 * np.eye(mtx1.shape[0])\n",
    "                mtx_inv = np.linalg.inv(mtx1)\n",
    "            else:\n",
    "                mtx1 += 1\n",
    "                mtx_inv = 1.0 / mtx1\n",
    "            MTX1_INV[i] = mtx_inv\n",
    "\n",
    "            MTX2[i] = 2 * Gamma_vec[i] * np.dot(X[i].T, Y[i]).T\n",
    "        return MTX1_INV, MTX2\n",
    "\n",
    "\n",
    "    def optimize(self, idx, hat_w):\n",
    "        mtx2 = hat_w[idx] + self.MTX2[idx]\n",
    "        mtx_inv = self.MTX1_INV[idx]\n",
    "\n",
    "        return np.dot(mtx_inv, mtx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticOptimizer(Optimizer):\n",
    "\n",
    "    def __init__(self, tau, X, Y):\n",
    "        super(Optimizer).__init__()\n",
    "        self.tau = tau\n",
    "        self.X = tf.constant(X, dtype=tf.float64)\n",
    "        self.Y = tf.constant(Y, dtype=tf.float64)\n",
    "\n",
    "    def optimize(self, idx, hat_w):\n",
    "        def make_val_and_grad_fn(value_fn):\n",
    "            @functools.wraps(value_fn)\n",
    "            def val_and_grad(x):\n",
    "                return tfp.math.value_and_gradient(value_fn, x)\n",
    "\n",
    "            return val_and_grad\n",
    "\n",
    "        @contextlib.contextmanager\n",
    "        def timed_execution():\n",
    "            t0 = time.time()\n",
    "            yield\n",
    "            dt = time.time() - t0\n",
    "            print('Evaluation took: %f seconds' % dt)\n",
    "\n",
    "        def np_value(tensor):\n",
    "            if isinstance(tensor, tuple):\n",
    "                return type(tensor)(*(np_value(t) for t in tensor))\n",
    "            else:\n",
    "                return tensor.numpy()\n",
    "\n",
    "        def run(optimizer):\n",
    "            optimizer()\n",
    "            # with timed_execution():\n",
    "            result = optimizer()\n",
    "            return np_value(result)\n",
    "\n",
    "        def regression_loss(params):\n",
    "            \n",
    "            # Calculate the logistic loss\n",
    "            labels = Y[idx]\n",
    "            feature = X[idx]\n",
    "            \n",
    "            reshaped_params = tf.expand_dims(params, 1)\n",
    "            logits = tf.matmul(feature, reshaped_params)\n",
    "            labels = tf.expand_dims(labels, 1)\n",
    "            mse_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "            # calculate the penalty loss\n",
    "            w = tf.expand_dims(tf.constant(hat_w[idx], dtype=tf.float64), 1)\n",
    "            penalty_var = tf.math.subtract(w, params)\n",
    "            loss_penalty = regularization_factor * tf.nn.l2_loss(penalty_var)\n",
    "\n",
    "            \n",
    "            total_loss = mse_loss + loss_penalty\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "        @tf.function\n",
    "        def l1_regression_with_lbfgs():\n",
    "            return tfp.optimizer.lbfgs_minimize(\n",
    "                make_val_and_grad_fn(regression_loss),\n",
    "                initial_position=tf.constant(start),\n",
    "                tolerance=1e-8)\n",
    "\n",
    "        dim = len(hat_w[idx])\n",
    "        start = np.random.randn(dim)\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "\n",
    "        regularization_factor = 1/(2*self.tau[idx])\n",
    "\n",
    "        results = run(l1_regression_with_lbfgs)\n",
    "        minimum = results.position\n",
    "        return minimum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y_and_W(X, trained_models_weights):\n",
    "    '''\n",
    "    :param X: A list of the features of algorithm 1\n",
    "    :param trained_models_weights: A list containing the weight of the new model based on training each model\n",
    "    '''\n",
    "\n",
    "    Y = []\n",
    "    W = []\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        # The weights of the trainable layers (the new model) of the i_th trained model\n",
    "        weights = trained_models_weights[i]\n",
    "\n",
    "        # the weights of the dense layer of the model\n",
    "        w1 = np.array(weights[-2]).flatten()\n",
    "\n",
    "        # the bias of the dense layer\n",
    "        w2 = weights[-1]\n",
    "\n",
    "        # combining the weights and the bias of the dense layer of the model, which is the weight of the node for alg1\n",
    "        w = np.concatenate((w1, w2))\n",
    "        W.append(w)\n",
    "\n",
    "        # the label of the i_th node for alg1\n",
    "        Y.append(X[i].dot(w))\n",
    "\n",
    "    Y = np.array(Y)\n",
    "    W = np.array(W)\n",
    "\n",
    "    return Y, W\n",
    "\n",
    "\n",
    "def deep_learning_run(lambda_lasso, K=1000, train_data_dir='deep_learning_lasso/new_deeplarning_data'):\n",
    "\n",
    "    # calculate base model output and true labels for all images\n",
    "    base_model_output, true_labels = get_base_model_output()\n",
    "    '''\n",
    "    base_model_output: A list containing the output of the base(pre-trained) model for all the images\n",
    "    true_labels: A list containing the true label of all the images (which is 0 or 1 for each image)\n",
    "    '''\n",
    "\n",
    "    # load trained data from saved models in train_data_dir\n",
    "    trained_models_train_images, trained_models_weights, X, Y = load_trained_data(train_data_dir, base_model_output, true_labels)\n",
    "    '''\n",
    "    trained_models_train_images: A list containing the images used for training each model\n",
    "    trained_models_weights : A list containing the weight of the new model based on training each model\n",
    "    X : A list containing the output of the base model for trainset of each model, which is the features of algorithm 1\n",
    "    Y : A list containing the true labels for trainset of each model, which is the labels of algorithm 1\n",
    "    '''\n",
    "\n",
    "    # create B and weight_vec for the empirical graph G\n",
    "    B, weight_vec = get_B_and_weight_vec(trained_models_train_images)\n",
    "    E, N = B.shape\n",
    "    '''\n",
    "    B : Incidence matrix of the empirical graph G\n",
    "    weight_vec : Wight of each edge of the empirical graph G\n",
    "    '''\n",
    "\n",
    "    # calculate the weights(W) of the empirical graph G\n",
    "    _, W = get_Y_and_W(X, trained_models_weights)\n",
    "    print (\"hereee\", Y.shape, W.shape, X.shape, true_labels.shape)\n",
    "    '''\n",
    "    W : The weights of the nodes for the algorihtm 1\n",
    "    '''\n",
    "    \n",
    "    # choose sampling set for alg1\n",
    "    M=0.2\n",
    "    samplingset = random.sample([i for i in range(N)], k=int(M * N))\n",
    "    # samplingset = [53, 92, 99, 19, 16, 32, 6, 9, 39, 43, 34, 54, 23, 8, 13, 88, 1, 62, 22, 60]\n",
    "    '''\n",
    "    samplingset : The samplingset selected for algorithm 1\n",
    "    '''\n",
    "\n",
    "    print ('start alg')\n",
    "    # alg1\n",
    "    K=2\n",
    "    _, alg1_estimated_weights = algorithm_1(K, B, weight_vec, X, Y, samplingset, lambda_lasso, loss_func='logistic_reg')    \n",
    "    '''\n",
    "    alg1_estimated_weights : The estimated weights by algorithm 1\n",
    "    '''\n",
    "\n",
    "    # save the orange and blue fig\n",
    "    save_figures(alg1_estimated_weights, W, lambda_lasso, base_model_output, true_labels)\n",
    "\n",
    "    return alg1_estimated_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = deep_learning_run(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f0627516f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNpEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONEncoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow_datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__py2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__init__py3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__py3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow_datasets/__init__py3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# needs to happen before anything else, since the imports below will try to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# import tensorflow, too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_tf_install\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow_datasets/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# import tensorflow, too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtf_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_tf_install\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow_datasets/core/tf_compat.py\u001b[0m in \u001b[0;36mensure_tf_install\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m  \u001b[0;31m# pylint: disable=import-outside-toplevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Print more informative error message, then reraise.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m   _current_module.__path__ = (\n\u001b[1;32m    686\u001b[0m       [_module_util.get_parent_dir(keras)] + _current_module.__path__)\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/python/keras/api/_v1/keras/applications/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m   _sys.modules[__name__] = _module_wrapper.TFModuleWrapper(\n\u001b[1;32m     60\u001b[0m       \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"keras.applications\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpublic_apis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       has_lite=False)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/aalto/lib/python3.7/site-packages/tensorflow/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, wrapped, module_name, public_apis, deprecation, has_lite)\u001b[0m\n\u001b[1;32m    109\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         self._tfmw_wrapped_module.__all__ = [\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mattr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return super(NpEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "# convert pandas.dataset to pandas.dataframe\n",
    "def convert_dataset_to_dataframe(dataset):\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    for obj in dataset:\n",
    "        file_names.append(obj['image/filename'].numpy().decode('utf-8'))\n",
    "        labels.append(obj['label'].numpy())\n",
    "    df = pd.DataFrame(data={'filename': file_names, 'label': labels})\n",
    "    return df\n",
    "\n",
    "\n",
    "# prepare train/validation/test datasets for training the model\n",
    "def split_dataset(dataset, train_ratio, val_ratio, test_ratio):\n",
    "    # ds_size = len(list(ds))\n",
    "    ds_size = 23262\n",
    "\n",
    "    train_size = int(ds_size * train_ratio)\n",
    "    validate_size = int(ds_size * val_ratio)\n",
    "    test_size = int(ds_size * test_ratio)\n",
    "\n",
    "    # split the dataset to train/validation/test datasets based on their ratios\n",
    "    train_ds = dataset.take(train_size)\n",
    "    validation_ds = dataset.skip(train_size).take(validate_size)\n",
    "    test_ds = dataset.skip(train_size + validate_size).take(test_size)\n",
    "\n",
    "    # convert train/validation/test pandas.dataset to pandas.dataframe in order to save them\n",
    "    train_df = convert_dataset_to_dataframe(train_ds)\n",
    "    validate_df = convert_dataset_to_dataframe(validation_ds)\n",
    "    test_df = convert_dataset_to_dataframe(test_ds)\n",
    "\n",
    "    # resize the images of train/validation/test dataset to the standard size\n",
    "    train_ds = train_ds.map(lambda item: (tf.image.resize(item['image'], Image_Size), item['label']))\n",
    "    validation_ds = validation_ds.map(lambda item: (tf.image.resize(item['image'], Image_Size), item['label']))\n",
    "    test_ds = test_ds.map(lambda item: (tf.image.resize(item['image'], Image_Size), item['label']))\n",
    "\n",
    "    # prepare train/validation/test dataset for training the model\n",
    "    train_ds = train_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
    "    validation_ds = validation_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
    "    test_ds = test_ds.cache().batch(BATCH_SIZE).prefetch(buffer_size=10)\n",
    "\n",
    "    return (train_ds, train_df), (validation_ds, validate_df), (test_ds, test_df)\n",
    "\n",
    "\n",
    "n_models = 200\n",
    "\n",
    "# train different models with different train/validation/test datasets and save its data\n",
    "for i in range(n_models):\n",
    "\n",
    "    # load the data from tensorflow dataset\n",
    "    dataset, metadata = tfds.load(\n",
    "        \"cats_vs_dogs\",\n",
    "        shuffle_files=True,\n",
    "        with_info=True,\n",
    "    )\n",
    "    dataset = dataset['train']\n",
    "\n",
    "    # prepare train/validation/test datasets for training the model\n",
    "    (train_ds, train_df), (validation_ds, validate_df), (test_ds, test_df) = split_dataset(dataset, train_ratio=0.75, val_ratio=0.15, test_ratio=0.1)\n",
    "    '''\n",
    "    train_ds: dataset of the training data for the model (we need dataset to train the model)\n",
    "    train_df: dataframe of the training data for the model (we need dataframe to save the model's data) \n",
    "\n",
    "    validation_ds: dataset of the validation data for the model\n",
    "    validate_df: dataframe of the validation data for the model\n",
    "\n",
    "    test_ds: dataset of the test data for the model\n",
    "    test_df: dataframe of the test data for the model\n",
    "    '''\n",
    "    # create the model\n",
    "    model = get_NN_model()\n",
    "\n",
    "    # train the model for the selected train and validation datasets\n",
    "    start = datetime.datetime.now()\n",
    "    model.fit(train_ds, epochs=EPOCHS, validation_data=validation_ds)\n",
    "    print(datetime.datetime.now() - start)\n",
    "\n",
    "    # calculate the predicted labels of the trained model for the test dataset\n",
    "    pred_labels = model.predict(test_ds).flatten()\n",
    "    pred_labels[pred_labels <= 0] = 0\n",
    "    pred_labels[pred_labels > 0] = 1\n",
    "\n",
    "    # obtain the true labels for the test dataset\n",
    "    true_labels = []\n",
    "    for obj in test_ds:\n",
    "        true_labels += list(np.array(obj[1]))\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # calculate the accuracy of the model for the test dataset\n",
    "    accuracy = np.where(true_labels == pred_labels)[0].shape[0] / test_df.shape[0]\n",
    "    print ('\\n\\n\\nthe accuracy is: ', accuracy)\n",
    "\n",
    "    # the weights of the trainable layers of the model\n",
    "    weights = model.get_weights()[-2:]\n",
    "\n",
    "    # useful model's data to save\n",
    "    model_data = {\n",
    "        'score': accuracy,\n",
    "        'train_df': train_df['filename'].values,\n",
    "        'validate_df': validate_df['filename'].values,\n",
    "        'test_df': test_df['filename'].values,\n",
    "        'weights': weights,\n",
    "    }\n",
    "\n",
    "    # save trained data\n",
    "    with open('deep_learning_lasso/deep_learning_data/new_deeplearning_%d.json' % i, 'w') as f:\n",
    "        f.write(json.dumps(model_data, cls=NpEncoder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
