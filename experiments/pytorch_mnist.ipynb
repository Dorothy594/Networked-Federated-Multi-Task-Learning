{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sarchey1/paper/FederatedLearning\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch MNIST Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see in the algorithm picture, the primal update needs a optimizer operator for the sampling set (line 6). We have implemented the optimizers discussed in the paper, both the logistic loss and squared error loss optimizers implementations with pytorch is available, also we have implemented the squared error loss optimizer using the fixed point equation in the `Networked Linear Regression` section of the paper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/optimizer.py \n",
    "import torch\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# The SimpleLinear which is for MNIST experiment\n",
    "class SimpleLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(28*28, h1)\n",
    "        self.linear2 = torch.nn.Linear(h1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "                \n",
    "\n",
    "# The SimpleLinear which is for MNIST experiment\n",
    "class CombinedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.models = []\n",
    "        for i in range(40):\n",
    "            model = SimpleLinear(h1)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        preds = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            x = xs[i]\n",
    "            pred = np.argmax(model.forward(x))\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "    def parameters(self, recurse: bool = True):\n",
    "        for model in self.models:\n",
    "            for name, param in model.named_parameters(recurse=recurse):\n",
    "                yield param\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also proposed an experiment based on the MNIST dataset. Here we have two different clusters $C_1, C_2$. In order to simulate an environment where the data on each node is generated from a different distribution, for each cluster two different numbers (out of 10 available classes) is selected (\n",
    "$C_1 \\in \\{0, 1\\}$ and $C_2 \\in \\{2, 3\\}$).\n",
    "Each node of the empirical graph stands for a simple neural network with two Linear layers in which the dimension of the first layer's input is 28*28 and its output is 1 (with Relu activation function), and also the last layer has the output with dimension 2 (and Softmax activation function).\n",
    "\n",
    "\n",
    "The training images for each cluster are splitted equally to the nodes in that cluster. Thus, the only difference between the nodes in the clusters is their training datasets. First, by using TSNE the dimension of each image ($d=28*28$) is reduced to $d^\\prime=2$. Suppose that the training features for the node $i$ is $[img_i^1 ,\\cdots, img_i^P] \\in R^{784*P}$\n",
    ", so the embedded features can be expressed by $embedded_i = [embedded_i^1,\\cdots,embedded_i^P] \\in R^{2*P}$. For calculating the similarities between the nodes, in order to calculate a consistent distance between the nodes, we consider the sorted embedded features, so $embedded_i = sorted([embedded_i^1,\\cdots,embedded_i^P])$. Then\n",
    "$\\forall i \\in N $ and $\\forall j \\in N$ \n",
    " \\begin{equation*}\n",
    "     dist(i, j) = \\frac{1}{P} \\sum_{k=1}^{P} (embedded_i^k - embedded_j^k)^2.\n",
    " \\end{equation*}\n",
    "\n",
    "For each node the 6 nearest neighbours (the ones with the lowest distances) is selected and the edge weight between nodes i and j is defined by $w_{ij} = e^{-dist(i,j)}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import random\n",
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def get_images():\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                     ])\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "\n",
    "    dl = DataLoader(mnist_dataset)\n",
    "\n",
    "    X = dl.dataset.data # (60000,28, 28)\n",
    "    y = dl.dataset.targets #(60000)\n",
    "\n",
    "    # normalize to have 0 ~ 1 range in each pixel\n",
    "\n",
    "    X = X / 255.0\n",
    "    \n",
    "    cluster_nums = [[0, 1], [2, 3]]\n",
    "    \n",
    "    cluster_data0 = np.concatenate((np.where(y==cluster_nums[0][0])[0], np.where(y==cluster_nums[0][1])[0]))\n",
    "    random.shuffle(cluster_data0)\n",
    "    \n",
    "    cluster_data1 = np.concatenate((np.where(y==cluster_nums[1][0])[0], np.where(y==cluster_nums[1][1])[0]))\n",
    "    random.shuffle(cluster_data1)\n",
    "    \n",
    "    cluster_data = [\n",
    "        cluster_data0,\n",
    "        cluster_data1,\n",
    "    ]\n",
    "    \n",
    "    all_cluster_data = np.concatenate((cluster_data0 , cluster_data1))\n",
    "    selected_X = X[all_cluster_data].reshape(-1, 28*28)\n",
    "    selected_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                            init='random').fit_transform(selected_X)\n",
    "    \n",
    "    all_embeded = np.zeros((len(X), 2))\n",
    "    all_embeded[all_cluster_data] = selected_embedded\n",
    "    return X, y, cluster_data, cluster_nums, all_embeded\n",
    "\n",
    "\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "    \n",
    "    \n",
    "def save_data():\n",
    "    X, y, cluster_data, cluster_nums, all_embeded = get_images()\n",
    "\n",
    "    saved_data = {\n",
    "        'X': X.tolist(), \n",
    "        'y': y.tolist(),\n",
    "        'cluster_data': cluster_data,\n",
    "        'cluster_nums': cluster_nums,\n",
    "        'all_embeded': all_embeded,\n",
    "    }\n",
    "    with open('./experiments/data/MNIST/mnist_data.json', 'w') as f:\n",
    "        f.write(json.dumps(saved_data, cls=NpEncoder))\n",
    "    \n",
    "def loead_data():\n",
    "    with open('./experiments/data/MNIST/mnist_data.json', 'r') as f:\n",
    "        saved_data = json.load(f)\n",
    "    \n",
    "    X = torch.tensor(saved_data['X'])\n",
    "    y = torch.tensor(saved_data['y'])\n",
    "    cluster_data = np.array(saved_data['cluster_data'])\n",
    "    cluster_nums = np.array(saved_data['cluster_nums'])\n",
    "    all_embeded = np.array(saved_data['all_embeded'])\n",
    "    \n",
    "    return X, y, cluster_data, cluster_nums, all_embeded\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from algorithm.optimizer import *\n",
    "\n",
    "def build_graph(cluster_sizes, X, y, cluster_data, all_embeded, num_neig=3, h1=200):\n",
    "    '''\n",
    "    :param W: a list containing the weight vectors for each cluster\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "\n",
    "    N = np.sum(cluster_sizes)\n",
    "    E = N-1\n",
    "    '''\n",
    "    N: total number of nodes\n",
    "    E: total number of edges\n",
    "    '''\n",
    "    \n",
    "    feature_size = int(10000//cluster_sizes[0])\n",
    "    train_size = int(feature_size*0.8)\n",
    "    \n",
    "    # create the data of each node needed for the algorithm 1 \n",
    "    node_degrees = np.full(N, 1.0/num_neig)\n",
    "#     np.array((1.0 / (np.sum(abs(B), 0)))).ravel()\n",
    "    '''\n",
    "    node_degrees: a list containing the nodes degree for the alg1 (1/N_i)\n",
    "    '''\n",
    "    \n",
    "    datapoints = {}\n",
    "    '''\n",
    "    datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1,\n",
    "    which are features, label, degree, and also the optimizer model for each node\n",
    "    '''\n",
    "    true_labels = []\n",
    "    '''\n",
    "    true_labels: the true labels for the nodes of the graph\n",
    "    '''\n",
    "    \n",
    "    embeded_features = []\n",
    "    cnt = 0\n",
    "    for i, cluster_size in enumerate(cluster_sizes):\n",
    "        for j in range(cluster_size):\n",
    "            features = X[cluster_data[i][j*feature_size:(j+1)*feature_size]][:train_size]\n",
    "            res = all_embeded[cluster_data[i][j*feature_size:(j+1)*feature_size]][:train_size]\n",
    "            res = np.sort(res, axis=0)\n",
    "            embeded_features.append(res)\n",
    "            test_features = X[cluster_data[i][j*feature_size:(j+1)*feature_size]][train_size:]\n",
    "            '''\n",
    "            features: the feature vector of node i \n",
    "            '''\n",
    "            label = y[cluster_data[i][j*feature_size:(j+1)*feature_size]][:train_size]\n",
    "            test_label = y[cluster_data[i][j*feature_size:(j+1)*feature_size]][train_size:]\n",
    "            \n",
    "            label[label == cluster_nums[i][0]] = 0\n",
    "            label[label == cluster_nums[i][1]] = 1\n",
    "            \n",
    "            test_label[test_label == cluster_nums[i][0]] = 0\n",
    "            test_label[test_label == cluster_nums[i][1]] = 1\n",
    "            '''\n",
    "            label: the label of the node i\n",
    "            '''\n",
    "            \n",
    "            true_labels.append(label)\n",
    "            \n",
    "\n",
    "            '''\n",
    "            model : the linear model for the node i \n",
    "            optimizer : the optimizer model for the node i \n",
    "            ''' \n",
    "            \n",
    "            datapoints[cnt] = {\n",
    "                'features': features,\n",
    "                'test_features': test_features,\n",
    "                'degree': node_degrees[cnt],\n",
    "                'label': label,\n",
    "                'test_label': test_label,\n",
    "            }\n",
    "            cnt += 1\n",
    "            \n",
    "    \n",
    "    # create B(adjacency matrix) and edges's weights vector(weight_vec) based on the graph G\n",
    "    E = N*num_neig\n",
    "    B = np.zeros((E, N))\n",
    "    '''\n",
    "    B: adjacency matrix of the graph with the shape of E*N\n",
    "    '''\n",
    "    weight_vec = np.zeros(E)\n",
    "    '''\n",
    "    weight_vec: a list containing the edges's weights of the graph with the shape of E\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for i in range(N):\n",
    "        dists = []\n",
    "        for j in range(N):\n",
    "            dists.append(mean_squared_error(embeded_features[i], embeded_features[j]))\n",
    "        node_dists = np.argsort(dists)\n",
    "        for k in range(num_neig):\n",
    "            neig = node_dists[k+1]\n",
    "            B[cnt, i] = 1\n",
    "            B[cnt, neig] = -1\n",
    "            weight_vec[cnt] = np.exp(-dists[neig])\n",
    "            cnt += 1\n",
    "\n",
    "    weight_vec = (weight_vec - np.min(weight_vec)) / (np.max(weight_vec) - np.min(weight_vec))\n",
    "    \n",
    "\n",
    "    return B, weight_vec, np.array(true_labels), datapoints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity graph with Two Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# from sparsebm import generate_SBM_dataset\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_graph_data(X, y, cluster_data, all_embeded, n_neig=3, h1=200):\n",
    "    '''\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "    cluster_sizes = [100, 100]\n",
    "    cluster_sizes = [20, 20]\n",
    "    \n",
    "    return build_graph(cluster_sizes, X, y, cluster_data, all_embeded, n_neig, h1)\n",
    "\n",
    "\n",
    "# save_data()\n",
    "X, y, cluster_data, cluster_nums, all_embeded = loead_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the test accuracy vs iterations for algorithm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "n_epochs = 1500\n",
    "\n",
    "batch_size_train = 2000\n",
    "user_batch_size = batch_size_train//40\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_graph_data(X, y, cluster_data, all_embeded, n_neig=6, h1=1)\n",
    "weight_vec = torch.tensor(weight_vec)\n",
    "\n",
    "# _cluster_data = np.concatenate((cluster_data[0], cluster_data[1]))\n",
    "_cluster_data = cluster_data[0]\n",
    "features = X[_cluster_data]\n",
    "labels = y[_cluster_data]\n",
    "print(np.unique(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnt = len(_cluster_data)\n",
    "train_data = []\n",
    "test_data = []\n",
    "for i in range(all_cnt):\n",
    "    if i < all_cnt*0.8:\n",
    "        train_data.append([features[i], labels[i]])\n",
    "    else:\n",
    "        test_data.append([features[i], labels[i]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size_train)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [0/10132 (0%)]\tLoss: 43.949371\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1471/2533 (58%)\n",
      "\n",
      "Train Epoch: 200 [0/10132 (0%)]\tLoss: 40.536152\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1579/2533 (62%)\n",
      "\n",
      "Train Epoch: 300 [0/10132 (0%)]\tLoss: 38.800671\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1654/2533 (65%)\n",
      "\n",
      "Train Epoch: 400 [0/10132 (0%)]\tLoss: 37.084824\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1656/2533 (65%)\n",
      "\n",
      "Train Epoch: 500 [0/10132 (0%)]\tLoss: 35.297527\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1761/2533 (70%)\n",
      "\n",
      "Train Epoch: 600 [0/10132 (0%)]\tLoss: 34.261070\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1828/2533 (72%)\n",
      "\n",
      "Train Epoch: 700 [0/10132 (0%)]\tLoss: 33.272690\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1844/2533 (73%)\n",
      "\n",
      "Train Epoch: 800 [0/10132 (0%)]\tLoss: 32.474487\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1863/2533 (74%)\n",
      "\n",
      "Train Epoch: 900 [0/10132 (0%)]\tLoss: 31.716272\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1869/2533 (74%)\n",
      "\n",
      "Train Epoch: 1000 [0/10132 (0%)]\tLoss: 30.824854\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1931/2533 (76%)\n",
      "\n",
      "Train Epoch: 1100 [0/10132 (0%)]\tLoss: 30.343861\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1952/2533 (77%)\n",
      "\n",
      "Train Epoch: 1200 [0/10132 (0%)]\tLoss: 30.140450\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1934/2533 (76%)\n",
      "\n",
      "Train Epoch: 1300 [0/10132 (0%)]\tLoss: 28.838547\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 1975/2533 (78%)\n",
      "\n",
      "Train Epoch: 1400 [0/10132 (0%)]\tLoss: 28.953966\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 2024/2533 (80%)\n",
      "\n",
      "Train Epoch: 1500 [0/10132 (0%)]\tLoss: 28.696260\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 2020/2533 (80%)\n",
      "\n",
      "\n",
      "Test set: Avg. loss: nan, Accuracy: 2008/2533 (79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "network = CombinedModel(1)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def my_loss(output, target, network, B, weight_vec):\n",
    "    loss = 0\n",
    "    E, N = B.shape\n",
    "    for idx in range(N):\n",
    "        loss += F.nll_loss(output[idx], target[idx])\n",
    "             \n",
    "        neighs = np.where(B[:, idx] == -1)[0]\n",
    "        for e in neighs:\n",
    "            j = np.where(B[e]==1)[0][0]\n",
    "            loss += weight_vec[e] * torch.mean((\n",
    "                network.models[j].linear1.weight.data - network.models[idx].linear1.weight.data)**2)\n",
    "            loss += weight_vec[e] * torch.mean((\n",
    "                network.models[j].linear2.weight.data - network.models[idx].linear2.weight.data)**2)\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train(epoch, is_print=False):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        splited_data = [data[i*user_batch_size:(i+1)*user_batch_size] for i in range(40)]\n",
    "        splited_target = [target[i*user_batch_size:(i+1)*user_batch_size] for i in range(40)]\n",
    "        output = network(splited_data)\n",
    "        loss = my_loss(output, splited_target, network, B, weight_vec)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and is_print:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(is_print=False):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            splited_data = [data[i*user_batch_size:(i+1)*user_batch_size] for i in range(40)]\n",
    "            splited_target = [target[i*user_batch_size:(i+1)*user_batch_size] for i in range(40)]\n",
    "            output = network(splited_data)\n",
    "            test_loss = my_loss(output, splited_target, network, B, weight_vec)\n",
    "            preds = [item.data.max(1, keepdim=True)[1] for item in output]\n",
    "            for i, pred in enumerate(preds):\n",
    "                correct += pred.eq(splited_target[i].data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    if is_print:\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    if epoch % 100 == 0:\n",
    "        train(epoch, True)\n",
    "        test(True)\n",
    "    else:\n",
    "        test(False)\n",
    "test(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
