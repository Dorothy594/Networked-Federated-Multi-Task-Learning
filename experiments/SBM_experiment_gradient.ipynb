{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sarchey1/paper\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Block Model Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before geting into the experiment details, let's review algorithm 1 and the primal and dual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../gradient_federated.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/main.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from algorithm.penalty import *\n",
    "\n",
    "\n",
    "def algorithm_1(K, D, weight_vec, datapoints, true_labels, samplingset, lambda_lasso, calculate_score=False):\n",
    "    '''\n",
    "    :param K: the number of iterations\n",
    "    :param D: the block incidence matrix\n",
    "    :param weight_vec: a list containing the edges's weights of the graph\n",
    "    :param datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param true_labels: a list containing the true labels of the nodes\n",
    "    :param samplingset: the sampling set\n",
    "    :param lambda_lasso: the parameter lambda\n",
    "    :param penalty_func_name: the name of the penalty function used in the algorithm\n",
    "\n",
    "    :return iteration_scores: the mean squared error of the predicted weight vectors in each iteration\n",
    "    :return new_w: the predicted weigh vectors for each node\n",
    "    '''\n",
    "\n",
    "    alpha = 0.12\n",
    "    E, N = D.shape\n",
    "    m, n = datapoints[0]['features'].shape\n",
    "\n",
    "    # starting algorithm 1\n",
    "\n",
    "    new_w = np.array([np.zeros(n) for i in range(N)])\n",
    "    '''\n",
    "    new_w: the primal variable of the algorithm 1\n",
    "    '''\n",
    "\n",
    "    iteration_scores = []\n",
    "    for iterk in range(K):\n",
    "        prev_w = np.copy(new_w)\n",
    "        # line 2 of Gradient descent for networked FL\n",
    "        new_w = np.array([np.zeros(n) for i in range(N)])\n",
    "\n",
    "        # line 3 of Gradient descent for networked FL\n",
    "        for i in range(N):\n",
    "            if i in samplingset:  \n",
    "                # line 4 of Gradient descent for networked FL\n",
    "#                 optimizer = datapoints[i]['optimizer']\n",
    "#                 new_w[i] = 1/m * optimizer.optimize(datapoints[i]['features'], datapoints[i]['label'], prev_w[i], datapoints[i]['degree'])\n",
    "                \n",
    "                # line 4 of Gradient descent for networked FL\n",
    "                var = np.dot(datapoints[i]['features'], prev_w[i]) - datapoints[i]['label']\n",
    "                new_w[i] = 1/m * np.dot(datapoints[i]['features'].T, var)\n",
    "\n",
    "\n",
    "        # line 6 of Gradient descent for networked FL\n",
    "        for e in range(E):\n",
    "            i = np.where(B[e]==1)[0]\n",
    "            j = np.where(B[e]==-1)[0]\n",
    "            \n",
    "            # line 7 of Gradient descent for networked FL\n",
    "            tilde_w = lambda_lasso * weight_vec[e] * (prev_w[j] - prev_w[i])\n",
    "            \n",
    "            # line 8 of Gradient descent for networked FL\n",
    "            new_w[i] = new_w[i] - tilde_w\n",
    "            \n",
    "            # line 9 of Gradient descent for networked FL\n",
    "            new_w[j] = new_w[j] + tilde_w\n",
    "            \n",
    "\n",
    "        # line 11 of Gradient descent for networked FL\n",
    "        new_w = prev_w - alpha * new_w\n",
    "\n",
    "        # calculate the MSE of the predicted weight vectors\n",
    "        if calculate_score:\n",
    "            Y_pred = []\n",
    "            for i in range(N):\n",
    "                Y_pred.append(np.dot(datapoints[i]['features'], new_w[i]))\n",
    "\n",
    "            iteration_scores.append(mean_squared_error(true_labels.reshape(N, m), Y_pred))\n",
    "\n",
    "    # print (np.max(abs(new_w - prev_w)))\n",
    "\n",
    "    return iteration_scores, new_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/main.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def joonas_algorithm_1(K, D, weight_vec, datapoints, true_labels, samplingset, lambda_lasso, calculate_score=False):\n",
    "    '''\n",
    "    :param K: the number of iterations\n",
    "    :param D: the block incidence matrix\n",
    "    :param weight_vec: a list containing the edges's weights of the graph\n",
    "    :param datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param true_labels: a list containing the true labels of the nodes\n",
    "    :param samplingset: the sampling set\n",
    "    :param lambda_lasso: the parameter lambda\n",
    "    :param penalty_func_name: the name of the penalty function used in the algorithm\n",
    "\n",
    "    :return iteration_scores: the mean squared error of the predicted weight vectors in each iteration\n",
    "    :return new_w: the predicted weigh vectors for each node\n",
    "    '''\n",
    "\n",
    "    E, N = D.shape\n",
    "    m, n = datapoints[0]['features'].shape\n",
    "\n",
    "    # starting algorithm 1\n",
    "\n",
    "    # step size, 0.0001\n",
    "    # last 0.05\n",
    "    a = 0.02\n",
    "    \n",
    "    D = torch.Tensor(D)\n",
    "    weight_vec = torch.Tensor(weight_vec)\n",
    "    \n",
    "    iteration_scores = []\n",
    "    \n",
    "    model_params = list()\n",
    "    for i in range(N):\n",
    "        datapoints[i]['model'] = TorchLinearModel(n)\n",
    "        model_params += list(datapoints[i]['model'].parameters())\n",
    "\n",
    "    optim = torch.optim.SGD(model_params, lr=a)\n",
    "    prev_loss = 0\n",
    "    iterations = K\n",
    "    \n",
    "    for iterk in range(K):\n",
    "        optim.zero_grad()\n",
    "        # Shared loss over all nodes\n",
    "        loss = 0\n",
    "        # f(w) (training error/loss)\n",
    "        for i in samplingset:\n",
    "            x_i = torch.Tensor(datapoints[i]['features'])\n",
    "            y_i = torch.Tensor(datapoints[i]['label']).reshape(-1, 1)\n",
    "            y_pred = datapoints[i]['model'](x_i)\n",
    "            loss1 = F.mse_loss(y_pred, y_i)\n",
    "            loss += loss1\n",
    "                \n",
    "        # GTV\n",
    "        # weight_vec is A (edge weights)\n",
    "        # D is sort of adjacency matrix? +1 on j and -1 on i\n",
    "        # new_w has the current weights\n",
    "        new_w = torch.cat([datapoints[i]['model'].linear.weight for i in range(N)])\n",
    "        # w_j - w_i = D @ new_w\n",
    "        norm_squared = torch.square(torch.linalg.norm(D @ new_w, dim=1))\n",
    "        loss2 = lambda_lasso * torch.dot(weight_vec, norm_squared)\n",
    "        loss += loss2\n",
    "        \n",
    "#         print(\"epoch:\", iterk)\n",
    "#         print(\"loss\", loss.item())\n",
    "        if prev_loss - loss < 0.0005 and prev_loss - loss >= 0:\n",
    "            iterations = iterk\n",
    "            break\n",
    "        else:\n",
    "            prev_loss = loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "\n",
    "        # calculate the MSE of the predicted weight vectors\n",
    "        if calculate_score:\n",
    "            Y_pred = []\n",
    "            for i in range(N):\n",
    "                Y_pred.append(datapoints[i]['model'](datapoints[i]['features']))\n",
    "            iteration_scores.append(mean_squared_error(true_labels.reshape(N, m), Y_pred))\n",
    "    \n",
    "    new_w = np.array([datapoints[i]['model'].linear.weight.data.numpy() for i in range(N)]).reshape((N, -1))\n",
    "\n",
    "    return iteration_scores, new_w, iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## primal dual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/main.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from algorithm.penalty import *\n",
    "\n",
    "\n",
    "def primal_dual_algorithm_1(K, D, weight_vec, datapoints, true_labels, samplingset, lambda_lasso, penalty_func_name='norm1', calculate_score=False):\n",
    "    '''\n",
    "    :param K: the number of iterations\n",
    "    :param D: the block incidence matrix\n",
    "    :param weight_vec: a list containing the edges's weights of the graph\n",
    "    :param datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param true_labels: a list containing the true labels of the nodes\n",
    "    :param samplingset: the sampling set\n",
    "    :param lambda_lasso: the parameter lambda\n",
    "    :param penalty_func_name: the name of the penalty function used in the algorithm\n",
    "\n",
    "    :return iteration_scores: the mean squared error of the predicted weight vectors in each iteration\n",
    "    :return new_w: the predicted weigh vectors for each node\n",
    "    '''\n",
    "\n",
    "    Sigma = np.diag(np.full(weight_vec.shape, 0.9 / 2))\n",
    "    '''\n",
    "    Sigma: the block diagonal matrix Sigma\n",
    "    '''\n",
    "    T_matrix = np.diag(np.array((1.0 / (np.sum(abs(D), 0)))).ravel())\n",
    "    '''\n",
    "    T_matrix: the block diagonal matrix T\n",
    "    '''\n",
    "\n",
    "    if np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(T_matrix ** 0.5), 2) > 1:\n",
    "        print ('product norm', np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(T_matrix ** 0.5), 2))\n",
    "\n",
    "    E, N = D.shape\n",
    "    m, n = datapoints[0]['features'].shape\n",
    "\n",
    "    # define the penalty function\n",
    "    if penalty_func_name == 'norm1':\n",
    "        penalty_func = Norm1Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    elif penalty_func_name == 'norm2':\n",
    "        penalty_func = Norm2Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    elif penalty_func_name == 'mocha':\n",
    "        penalty_func = MOCHAPelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid penalty name')\n",
    "\n",
    "    # starting algorithm 1\n",
    "\n",
    "    new_w = np.array([np.zeros(n) for i in range(N)])\n",
    "    '''\n",
    "    new_w: the primal variable of the algorithm 1\n",
    "    '''\n",
    "    new_u = np.array([np.zeros(n) for i in range(E)])\n",
    "    '''\n",
    "    new_u: the dual variable of the algorithm 1\n",
    "    '''\n",
    "\n",
    "    iteration_scores = []\n",
    "    for iterk in range(K):\n",
    "        # if iterk % 100 == 0:\n",
    "        #     print ('iter:', iterk)\n",
    "        prev_w = np.copy(new_w)\n",
    "\n",
    "        # algorithm 1, line 2\n",
    "        hat_w = new_w - np.dot(T_matrix, np.dot(D.T, new_u))\n",
    "\n",
    "        for i in range(N):\n",
    "            if i in samplingset:  # algorithm 1, line 6\n",
    "\n",
    "                optimizer = datapoints[i]['optimizer']\n",
    "                new_w[i] = optimizer.optimize(datapoints[i]['features'], datapoints[i]['label'], hat_w[i], datapoints[i]['degree'])\n",
    "\n",
    "            else:\n",
    "                new_w[i] = hat_w[i]\n",
    "\n",
    "        # algorithm 1, line 9\n",
    "        tilde_w = 2 * new_w - prev_w\n",
    "        new_u = new_u + np.dot(Sigma, np.dot(D, tilde_w))\n",
    "\n",
    "        # algorithm 1, line 10\n",
    "        new_u = penalty_func.update(new_u)\n",
    "\n",
    "        # calculate the MSE of the predicted weight vectors\n",
    "        if calculate_score:\n",
    "            Y_pred = []\n",
    "            for i in range(N):\n",
    "                Y_pred.append(np.dot(datapoints[i]['features'], new_w[i]))\n",
    "\n",
    "            iteration_scores.append(mean_squared_error(true_labels.reshape(N, m), Y_pred))\n",
    "\n",
    "    # print (np.max(abs(new_w - prev_w)))\n",
    "\n",
    "    return iteration_scores, new_w\n",
    "\n",
    "\n",
    "\n",
    "# %load algorithm/optimizer.py \n",
    "import torch\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "# The linear model which is implemented by pytorch\n",
    "class TorchLinearModel(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(TorchLinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# The abstract optimizer model which should have model, optimizer, and criterion as the input\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        torch_old_weight = torch.from_numpy(np.array(old_weight, dtype=np.float32))\n",
    "        self.model.linear.weight.data = torch_old_weight\n",
    "        for iterinner in range(40):\n",
    "            self.optimizer.zero_grad()\n",
    "            y_pred = self.model(x_data)\n",
    "            loss1 = self.criterion(y_pred, y_data)\n",
    "            loss2 = 1 / (2 * regularizer_term) * torch.mean((self.model.linear.weight - torch_old_weight) ** 2)  # + 10000*torch.mean((model.linear.bias+0.5)**2)#model.linear.weight.norm(2)\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return self.model.linear.weight.data.numpy()\n",
    "\n",
    "\n",
    "# The linear model in Networked Linear Regression section of the paper\n",
    "class LinearModel:\n",
    "    def __init__(self, degree, features, label):\n",
    "        mtx1 = 2 * degree * np.dot(features.T, features).astype('float64')\n",
    "        mtx1 += 1 * np.eye(mtx1.shape[0])\n",
    "        mtx1_inv = np.linalg.inv(mtx1)\n",
    "\n",
    "        mtx2 = 2 * degree * np.dot(features.T, label).T\n",
    "\n",
    "        self.mtx1_inv = mtx1_inv\n",
    "        self.mtx2 = mtx2\n",
    "\n",
    "    def forward(self, x):\n",
    "        mtx2 = x + self.mtx2\n",
    "        mtx_inv = self.mtx1_inv\n",
    "\n",
    "        return np.dot(mtx_inv, mtx2)\n",
    "\n",
    "\n",
    "# The Linear optimizer in Networked Linear Regression section of the paper\n",
    "class LinearOptimizer(Optimizer):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(LinearOptimizer, self).__init__(model, None, None)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return self.model.forward(old_weight)\n",
    "\n",
    "\n",
    "# The Linear optimizer model which is implemented by pytorch\n",
    "class TorchLinearOptimizer(Optimizer):\n",
    "    def __init__(self, model):\n",
    "        criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters())\n",
    "        super(TorchLinearOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchLinearOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n",
    "\n",
    "\n",
    "# The Logistic optimizer model which is implemented by pytorch\n",
    "class TorchLogisticOptimizer(Optimizer):\n",
    "    def __init__(self, model):\n",
    "        criterion = torch.nn.BCELoss(reduction='mean')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters())\n",
    "        super(TorchLogisticOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchLogisticOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n",
    "\n",
    "\n",
    "\n",
    "# %load algorithm/penalty.py\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "# The abstract penalty function which has a function update\n",
    "class Penalty(ABC):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        self.lambda_lasso = lambda_lasso\n",
    "        self.weight_vec = weight_vec\n",
    "        self.Sigma = Sigma\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, new_u):\n",
    "        pass\n",
    "\n",
    "\n",
    "# The norm2 penalty function\n",
    "class Norm2Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(Norm2Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.limit = np.array(lambda_lasso * weight_vec)\n",
    "\n",
    "    def update(self, new_u):\n",
    "        normalized_u = np.where(np.linalg.norm(new_u, axis=1) >= self.limit)\n",
    "        new_u[normalized_u] = (new_u[normalized_u].T * self.limit[normalized_u] / np.linalg.norm(new_u[normalized_u], axis=1)).T\n",
    "        return new_u\n",
    "\n",
    "\n",
    "# The MOCHA penalty function\n",
    "class MOCHAPelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(MOCHAPelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.normalize_factor = 1 + np.dot(2 * self.Sigma, 1/(self.lambda_lasso * self.weight_vec))\n",
    "\n",
    "    def update(self, new_u):\n",
    "        for i in range(new_u.shape[1]):\n",
    "            new_u[:, i] /= self.normalize_factor\n",
    "\n",
    "        return new_u\n",
    "\n",
    "\n",
    "# The norm1 penalty function\n",
    "class Norm1Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(Norm1Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.limit = np.array([np.zeros(n) for i in range(len(weight_vec))])\n",
    "        for i in range(n):\n",
    "            self.limit[:, i] = lambda_lasso * weight_vec\n",
    "\n",
    "    def update(self, new_u):\n",
    "        normalized_u = np.where(abs(new_u) >= self.limit)\n",
    "        new_u[normalized_u] = self.limit[normalized_u] * new_u[normalized_u] / abs(new_u[normalized_u])\n",
    "        return new_u\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from algorithm.penalty import *\n",
    "\n",
    "\n",
    "def fedAvg(K, datapoints, true_labels, samplingset, calculate_score=False):\n",
    "    '''\n",
    "    :param K: the number of iterations\n",
    "    :param datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param true_labels: a list containing the true labels of the nodes\n",
    "    :param penalty_func_name: the name of the penalty function used in the algorithm\n",
    "\n",
    "    :return iteration_scores: the mean squared error of the predicted weight vectors in each iteration\n",
    "    :return new_w: the predicted weigh vectors for each node\n",
    "    '''\n",
    "\n",
    "    N = len(datapoints)\n",
    "    m, n = datapoints[0]['features'].shape\n",
    "\n",
    "    new_w = np.array([np.zeros(n) for i in range(N)])\n",
    "    '''\n",
    "    new_w: the weights\n",
    "    '''\n",
    "\n",
    "    iteration_scores = []\n",
    "    for iterk in range(K):\n",
    "        for i in samplingset:\n",
    "            optimizer = datapoints[i]['optimizer']\n",
    "            new_w[i] = optimizer.optimize(\n",
    "                datapoints[i]['features'], \n",
    "                datapoints[i]['label'], \n",
    "                new_w[i], \n",
    "                datapoints[i]['degree']\n",
    "            )\n",
    "        new_w[:, :] = np.mean(new_w[samplingset], axis=0)\n",
    "\n",
    "        # calculate the MSE of the predicted weight vectors\n",
    "        if calculate_score:\n",
    "            Y_pred = []\n",
    "            for i in range(N):\n",
    "                Y_pred.append(np.dot(datapoints[i]['features'], new_w[i]))\n",
    "\n",
    "            iteration_scores.append(mean_squared_error(true_labels.reshape(N, m), Y_pred))\n",
    "\n",
    "    # print (np.max(abs(new_w - prev_w)))\n",
    "\n",
    "    return iteration_scores, new_w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SBM Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic block model is a generative model for random graphs with some clusters structure. Two nodes within the same cluster of the empirical graph are connected by an edge with probability pin, two nodes from different clusters are connected by an edge with probability pout. Each node $i \\in V$ represents a local dataset consisting of $m$ feature vectors $x^{(i,1)}, ... , x^{(i,m)} \\in R^n$. The feature vectors are i.i.d. realizations of a standard Gaussian random vector x ∼ N(0,I). The labels $y_1^{(i)}, . . . , y_m^{(i)} \\in R$ of the nodes $i \\in V$ are generated according to the linear model $y_r^{(i)} = (x^{(i, r)})^T w^{(i)} + \\epsilon$, with $\\epsilon ∼ N(0,\\sigma)$. To learn the weight $w^{(i)}$ ,we apply Algorithm 1 to a training set M obtained by randomly selecting 40% of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithm.optimizer import *\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def get_sbm_data(cluster_sizes, G, W, m=5, n=2, noise_sd=0):\n",
    "    '''\n",
    "    :param G: generated SBM graph with defined clusters using sparsebm.generate_SBM_dataset\n",
    "    :param W: a list containing the weight vectors for each cluster\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "\n",
    "    N = len(G.nodes)\n",
    "    E = len(G.edges)\n",
    "    '''\n",
    "    N: total number of nodes\n",
    "    E: total number of edges\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # create B(adjacency matrix) and edges's weights vector(weight_vec) based on the graph G\n",
    "    B = np.zeros((E, N))\n",
    "    '''\n",
    "    B: adjacency matrix of the graph with the shape of E*N\n",
    "    '''\n",
    "    weight_vec = np.zeros(E)\n",
    "    '''\n",
    "    weight_vec: a list containing the edges's weights of the graph with the shape of E\n",
    "    '''\n",
    "    \n",
    "    cnt = 0\n",
    "    for i, j in G.edges:\n",
    "        if i > j:\n",
    "            continue\n",
    "        B[cnt, i] = 1\n",
    "        B[cnt, j] = -1\n",
    "\n",
    "        weight_vec[cnt] = 1\n",
    "        cnt += 1\n",
    "    \n",
    "    weight_vec = weight_vec[:cnt]\n",
    "    B = B[:cnt, :]\n",
    "    \n",
    "    # create the data of each node needed for the algorithm 1 \n",
    "    \n",
    "    node_degrees = np.array((1.0 / (np.sum(abs(B), 0)))).ravel()\n",
    "    '''\n",
    "    node_degrees: a list containing the nodes degree for the alg1 (1/N_i)\n",
    "    '''\n",
    "    \n",
    "    datapoints = {}\n",
    "    '''\n",
    "    datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1,\n",
    "    which are features, label, degree, and also the optimizer model for each node\n",
    "    '''\n",
    "    true_labels = []\n",
    "    '''\n",
    "    true_labels: the true labels for the nodes of the graph\n",
    "    '''\n",
    "    \n",
    "    cnt = 0\n",
    "    for i, cluster_size in enumerate(cluster_sizes):\n",
    "        for j in range(cluster_size):\n",
    "            features = np.random.normal(loc=0.0, scale=1.0, size=(m, n))\n",
    "            '''\n",
    "            features: the feature vector of node i which are i.i.d. realizations of a standard Gaussian random vector x~N(0,I)\n",
    "            '''\n",
    "            label = np.dot(features, W[i]) + np.random.normal(0,noise_sd)\n",
    "            '''\n",
    "            label: the label of the node i that is generated according to the linear model y = x^T w + e\n",
    "            '''\n",
    "            \n",
    "            true_labels.append(label)\n",
    "\n",
    "            model = LinearModel(node_degrees[i], features, label)\n",
    "            optimizer = LinearOptimizer(model)            \n",
    "            '''\n",
    "            model : the linear model for the node i \n",
    "            optimizer : the optimizer model for the node i \n",
    "            ''' \n",
    "            \n",
    "            datapoints[cnt] = {\n",
    "                'features': features,\n",
    "                'degree': node_degrees[i],\n",
    "                'label': label,\n",
    "                'optimizer': optimizer\n",
    "            }\n",
    "            cnt += 1\n",
    "\n",
    "    return B, weight_vec, np.array(true_labels), datapoints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result we compare the MSE of Algorithm 1 with plain linear regression \n",
    "and decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load results/compare_results.py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def get_algorithm1_MSE(datapoints, predicted_w, samplingset):\n",
    "    '''\n",
    "    :param datapoints:  a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param predicted_w: the predicted weigh vectors for each node\n",
    "    :param samplingset: the sampling set for the algorithm 1\n",
    "\n",
    "    :return alg1_MSE: the MSE of the algorithm 1 for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "    not_samplingset = [i for i in range(len(datapoints)) if i not in samplingset]\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for i in range(len(datapoints)):\n",
    "        features = np.array(datapoints[i]['features'])\n",
    "        label = np.array(datapoints[i]['label'])\n",
    "        true_labels.append(label)\n",
    "\n",
    "        pred_labels.append(np.dot(features, predicted_w[i]))\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    alg1_MSE = {'total': mean_squared_error(true_labels, pred_labels),\n",
    "                'train': mean_squared_error(true_labels[samplingset], pred_labels[samplingset]),\n",
    "                'test': mean_squared_error(true_labels[not_samplingset], pred_labels[not_samplingset])}\n",
    "\n",
    "    return alg1_MSE\n",
    "\n",
    "\n",
    "def get_linear_regression_MSE(x, y, samplingset, not_samplingset):\n",
    "    '''\n",
    "    :param x: a list containing the features of the nodes\n",
    "    :param y: a list containing the labels of the nodes\n",
    "    :param samplingset: the training dataset\n",
    "    :param not_samplingset: the test dataset\n",
    "    :return linear_regression_MSE : the MSE of linear regression for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "\n",
    "    model = LinearRegression().fit(x[samplingset], y[samplingset])\n",
    "    pred_y = model.predict(x)\n",
    "\n",
    "    linear_regression_MSE = {'total': mean_squared_error(y, pred_y),\n",
    "                             'train': mean_squared_error(y[samplingset],\n",
    "                                                         pred_y[samplingset]),\n",
    "                             'test': mean_squared_error(y[not_samplingset],\n",
    "                                                        pred_y[not_samplingset])}\n",
    "\n",
    "    return linear_regression_MSE\n",
    "\n",
    "\n",
    "def get_decision_tree_MSE(x, y, samplingset, not_samplingset):\n",
    "    '''\n",
    "    :param x: a list containing the features of the nodes\n",
    "    :param y: a list containing the labels of the nodes\n",
    "    :param samplingset: the training dataset\n",
    "    :param not_samplingset: the test dataset\n",
    "    :return decision_tree_MSE : the MSE of decision tree for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "\n",
    "    max_depth = 2\n",
    "\n",
    "    regressor = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    regressor.fit(x[samplingset], y[samplingset])\n",
    "    pred_y = regressor.predict(x)\n",
    "\n",
    "    decision_tree_MSE = {'total': mean_squared_error(y, pred_y),\n",
    "                         'train': mean_squared_error(y[samplingset],\n",
    "                                                     pred_y[samplingset]),\n",
    "                         'test': mean_squared_error(y[not_samplingset],\n",
    "                                                    pred_y[not_samplingset])}\n",
    "    return decision_tree_MSE\n",
    "\n",
    "\n",
    "def get_scores(datapoints, primal_dual_w, predicted_w, joona_w, fl_w, samplingset, others=True):\n",
    "    N = len(datapoints)\n",
    "    '''\n",
    "    N : the total number of nodes\n",
    "    '''\n",
    "\n",
    "    # calculate algorithm1 MSE\n",
    "    alg_1_score = get_algorithm1_MSE(datapoints, predicted_w, samplingset)\n",
    "    primal_dual_score = get_algorithm1_MSE(datapoints, primal_dual_w, samplingset)\n",
    "    joonas_score = get_algorithm1_MSE(datapoints, joona_w, samplingset)\n",
    "    fl_score = get_algorithm1_MSE(datapoints, fl_w, samplingset)\n",
    "    \n",
    "    linear_regression_score = None\n",
    "    decision_tree_score = None\n",
    "\n",
    "    if others:\n",
    "        # prepare the data for calculating the linear regression and decision tree regression MSEs\n",
    "        X = []\n",
    "        '''\n",
    "        X: an array containing the features of all the nodes\n",
    "        '''\n",
    "        true_labels = []\n",
    "        '''\n",
    "        true_labels: an array containing the labels of all the nodes\n",
    "        '''\n",
    "        for i in range(len(datapoints)):\n",
    "            X.append(np.array(datapoints[i]['features']))\n",
    "            true_labels.append(np.array(datapoints[i]['label']))\n",
    "\n",
    "        X = np.array(X)\n",
    "        true_labels = np.array(true_labels)\n",
    "        m, n = X[0].shape\n",
    "\n",
    "        x = X.reshape(-1, n)\n",
    "        y = true_labels.reshape(-1, 1)\n",
    "\n",
    "        reformated_samplingset = []\n",
    "        for item in samplingset:\n",
    "            for i in range(m):\n",
    "                reformated_samplingset.append(m * item + i)\n",
    "        reformated_not_samplingset = [i for i in range(m * N) if i not in reformated_samplingset]\n",
    "        \n",
    "        # calculate linear regression MSE\n",
    "        linear_regression_score = get_linear_regression_MSE(x, y, reformated_samplingset, reformated_not_samplingset)\n",
    "\n",
    "        # calculate decision tree MSE\n",
    "        decision_tree_score = get_decision_tree_MSE(x, y, reformated_samplingset, reformated_not_samplingset)\n",
    "\n",
    "    return alg_1_score, joonas_score, primal_dual_score, fl_score, linear_regression_score, decision_tree_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBM with Two Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SBM has two clusters $|C_1| = |C_2| = 100$.\n",
    "Two nodes within the same cluster are connected by an edge with probability `pin=0.5`, \n",
    "and two nodes from different clusters are connected by an edge with probability `pout=0.01`. \n",
    "Each node $i \\in V$ represents a local dataset consisting of feature vectors $x^{(i,1)}, ... , x^{(i,5)} \\in R^2$.\n",
    "The feature vectors are i.i.d. realizations of a standard Gaussian random vector x ~ N(0,I).\n",
    "The labels $y_1^{(i)}, . . . , y_5^{(i)} \\in R$ for each node $i \\in V$\n",
    "are generated according to the linear model $y_r^{(i)} = (x^{(i, r)})^T w^{(i)} + \\epsilon$, with $\\epsilon = 0$. \n",
    "The tuning parameter $\\lambda$ in algorithm1 \n",
    "is manually chosen, guided by the resulting MSE, as $\\lambda=0.01$ for norm1 and norm2 and also $\\lambda=0.05$ for mocha penalty function. \n",
    "To learn the weight $w^{(i)}$ ,we apply Algorithm 1 to a training set M obtained by randomly selecting 40% of the nodes and use the rest as test set. As the result we compare the mean MSE of Algorithm 1 with plain linear regression and decision tree regression with respect to the different random sampling sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# from sparsebm import generate_SBM_dataset\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_sbm_2blocks_data(m=5, n=2, pin=0.5, pout=0.01, noise_sd=0):\n",
    "    '''\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "    cluster_sizes = [50, 50]\n",
    "    probs = np.array([[pin, pout],[pout, pin]])\n",
    "\n",
    "    G = nx.stochastic_block_model(cluster_sizes, probs, seed=0)\n",
    "    '''\n",
    "    G: generated SBM graph with 2 clusters\n",
    "    ''' \n",
    "    \n",
    "    # define weight vectors for each cluster of the graph\n",
    "    \n",
    "    W1 = np.array([2 for i in range(n)])\n",
    "    '''\n",
    "    W1: the weigh vector for the first cluster\n",
    "    '''\n",
    "    W2 = np.array([-2 for i in range(n)])\n",
    "    '''\n",
    "    W2: the weigh vector for the second cluster\n",
    "    '''\n",
    "    \n",
    "    W = [W1, W2]\n",
    "    \n",
    "    \n",
    "    return get_sbm_data(cluster_sizes, G, W, m, n, noise_sd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the MSE with respect to the different random sampling sets for each penalty function, the plots are in the log scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### Number of features: 2   \n",
    "### Number of datapoints per node: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "K = 1000\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_sbm_2blocks_data(pin=1.0, pout=0.01)\n",
    "E, N = B.shape\n",
    "\n",
    "alg1_scores = []\n",
    "joonas_scores = []\n",
    "primal_dual_scores = []\n",
    "fl_scores = []\n",
    "linear_regression_scores = []\n",
    "decision_tree_scores = []\n",
    "\n",
    "num_tries = 5\n",
    "num_tries = 1\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "def fun():\n",
    "    samplingset = random.sample([j for j in range(N)], k=int(0.4* N)) \n",
    "    lambda_lasso = 0.1\n",
    "    _, primal_dual_w = primal_dual_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, predicted_w = algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, joona_w, _ = joonas_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, fl_w = fedAvg(K, datapoints, true_labels, samplingset)\n",
    "\n",
    "    return get_scores(datapoints, primal_dual_w, predicted_w, joona_w, fl_w, samplingset)\n",
    "\n",
    "\n",
    "# results = Parallel(n_jobs=4)(delayed(fun)() \n",
    "#                                          for i in range(num_tries))\n",
    "\n",
    "results = [fun()]\n",
    "\n",
    "for scores in results:\n",
    "    alg1_score, joonas_score, primal_dual_score, fl_score, linear_regression_score, decision_tree_score = scores\n",
    "    alg1_scores.append(alg1_score)\n",
    "    joonas_scores.append(joonas_score)\n",
    "    primal_dual_scores.append(primal_dual_score)\n",
    "    fl_scores.append(fl_score)\n",
    "    linear_regression_scores.append(linear_regression_score)\n",
    "    decision_tree_scores.append(decision_tree_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm 1: \n",
      " mean train MSE: 0.27358479307876105 \n",
      " mean test MSE: 0.5009840399027731\n",
      "joonas algorithm 1: \n",
      " mean train MSE: 0.2895299718567297 \n",
      " mean test MSE: 0.5291886202057027\n",
      "primal dual algorithm 1: \n",
      " mean train MSE: 0.00030906913658545045 \n",
      " mean test MSE: 0.0003428283579007561\n",
      "federated learning: \n",
      " mean train MSE: 7.460654147701409 \n",
      " mean test MSE: 7.997366675229729\n",
      "linear regression: \n",
      " mean train MSE: 7.4480010500735805 \n",
      " mean test MSE: 8.021341761796618\n",
      "decision tree: \n",
      " mean train MSE: 6.51615089584878 \n",
      " mean test MSE: 8.809757420954954\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['alg1', 'joonas', 'primal-dual', 'FedAvg', 'lin_reg', 'd_tree']\n",
    "x_pos = np.arange(len(labels))\n",
    "    \n",
    "    \n",
    "print('algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in alg1_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in alg1_scores]))\n",
    "\n",
    "print('joonas algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in joonas_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in joonas_scores]))\n",
    "\n",
    "print('primal dual algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in primal_dual_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in primal_dual_scores]))\n",
    "\n",
    "print('federated learning:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in fl_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in fl_scores])) \n",
    "                                                                         \n",
    "print('linear regression:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in linear_regression_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in linear_regression_scores]))\n",
    "\n",
    "print('decision tree:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in decision_tree_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in decision_tree_scores]))\n",
    "    \n",
    "alg1_norm1_score = [item['total'] for item in alg1_scores]\n",
    "joonas_score = [item['total'] for item in joonas_scores]\n",
    "primal_dual_score = [item['total'] for item in primal_dual_scores]\n",
    "fl_score = [item['total'] for item in fl_scores]  \n",
    "linear_regression_score = [item['total'] for item in linear_regression_scores]\n",
    "decision_tree_score = [item['total'] for item in decision_tree_scores]\n",
    "\n",
    "mean_MSEs = [\n",
    "    np.mean(alg1_norm1_score), \n",
    "    np.mean(joonas_score), \n",
    "    np.mean(primal_dual_score), \n",
    "    np.mean(fl_score), \n",
    "    np.mean(linear_regression_score), \n",
    "    np.mean(decision_tree_score)\n",
    "]\n",
    "\n",
    "std_MSEs = [\n",
    "    np.std(alg1_norm1_score), \n",
    "    np.std(joonas_score), \n",
    "    np.std(primal_dual_score), \n",
    "    np.std(fl_score),\n",
    "    np.std(linear_regression_score), \n",
    "    np.std(decision_tree_score)\n",
    "]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, mean_MSEs,\n",
    "   yerr=std_MSEs,\n",
    "   align='center',\n",
    "   alpha=0.5,\n",
    "   ecolor='black',\n",
    "   capsize=20)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_title('error bars plot')\n",
    "plt.show()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### Number of features: 1\n",
    "### Number of datapoints per node: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "PENALTY_FUNCS = ['norm1', 'norm2', 'mocha']\n",
    "\n",
    "LAMBDA_LASSO = {'norm1': 0.1, 'norm2': 0.01, 'mocha': 0.05}\n",
    "\n",
    "K = 1000\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_sbm_2blocks_data(m=100, n=1, pin=1.0, pout=0.01)\n",
    "E, N = B.shape\n",
    "\n",
    "alg1_scores = []\n",
    "fl_scores = []\n",
    "linear_regression_scores = []\n",
    "decision_tree_scores = []\n",
    "\n",
    "num_tries = 5\n",
    "num_tries = 1\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "def fun():\n",
    "    samplingset = random.sample([j for j in range(N)], k=int(0.8* N)) \n",
    "    lambda_lasso = 0.1\n",
    "    _, primal_dual_w = primal_dual_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, predicted_w = algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, joona_w, _ = joonas_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, fl_w = fedAvg(K, datapoints, true_labels, samplingset)\n",
    "\n",
    "    return get_scores(datapoints, primal_dual_w, predicted_w, joona_w, fl_w, samplingset)\n",
    "\n",
    "\n",
    "# results = Parallel(n_jobs=4)(delayed(fun)() \n",
    "#                                          for i in range(num_tries))\n",
    "\n",
    "results = [fun()]\n",
    "\n",
    "for scores in results:\n",
    "    alg1_score, joonas_score, primal_dual_score, fl_score, linear_regression_score, decision_tree_score = scores\n",
    "    alg1_scores.append(alg1_score)\n",
    "    joonas_scores.append(joonas_score)\n",
    "    primal_dual_scores.append(primal_dual_score)\n",
    "    fl_scores.append(fl_score)\n",
    "    linear_regression_scores.append(linear_regression_score)\n",
    "    decision_tree_scores.append(decision_tree_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm 1: \n",
      " mean train MSE: 0.04395700239520238 \n",
      " mean test MSE: 0.060076266104091704\n",
      "joonas algorithm 1: \n",
      " mean train MSE: 0.16847108232140157 \n",
      " mean test MSE: 0.297288609475197\n",
      "primal dual algorithm 1: \n",
      " mean train MSE: 0.00015457804622491644 \n",
      " mean test MSE: 0.00017145645857788067\n",
      "federated learning: \n",
      " mean train MSE: 4.06018352781002 \n",
      " mean test MSE: 3.95717183283453\n",
      "linear regression: \n",
      " mean train MSE: 4.058787084056135 \n",
      " mean test MSE: 3.9527159864433825\n",
      "decision tree: \n",
      " mean train MSE: 4.027237947049597 \n",
      " mean test MSE: 4.029176184866145\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['alg1', 'joonas', 'primal-dual', 'FedAvg', 'lin_reg', 'd_tree']\n",
    "x_pos = np.arange(len(labels))\n",
    "    \n",
    "    \n",
    "print('algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in alg1_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in alg1_scores]))\n",
    "\n",
    "print('joonas algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in joonas_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in joonas_scores]))\n",
    "\n",
    "print('primal dual algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in primal_dual_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in primal_dual_scores]))\n",
    "\n",
    "print('federated learning:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in fl_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in fl_scores])) \n",
    "                                                                         \n",
    "print('linear regression:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in linear_regression_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in linear_regression_scores]))\n",
    "\n",
    "print('decision tree:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in decision_tree_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in decision_tree_scores]))\n",
    "    \n",
    "alg1_norm1_score = [item['total'] for item in alg1_scores]\n",
    "joonas_score = [item['total'] for item in joonas_scores]\n",
    "primal_dual_score = [item['total'] for item in primal_dual_scores]\n",
    "fl_score = [item['total'] for item in fl_scores]  \n",
    "linear_regression_score = [item['total'] for item in linear_regression_scores]\n",
    "decision_tree_score = [item['total'] for item in decision_tree_scores]\n",
    "\n",
    "mean_MSEs = [\n",
    "    np.mean(alg1_norm1_score), \n",
    "    np.mean(joonas_score), \n",
    "    np.mean(primal_dual_score), \n",
    "    np.mean(fl_score), \n",
    "    np.mean(linear_regression_score), \n",
    "    np.mean(decision_tree_score)\n",
    "]\n",
    "\n",
    "std_MSEs = [\n",
    "    np.std(alg1_norm1_score), \n",
    "    np.std(joonas_score), \n",
    "    np.std(primal_dual_score), \n",
    "    np.std(fl_score),\n",
    "    np.std(linear_regression_score), \n",
    "    np.std(decision_tree_score)\n",
    "]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, mean_MSEs,\n",
    "   yerr=std_MSEs,\n",
    "   align='center',\n",
    "   alpha=0.5,\n",
    "   ecolor='black',\n",
    "   capsize=20)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_title('error bars plot')\n",
    "plt.show()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### Number of features: 10\n",
    "### Number of datapoints per node: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "PENALTY_FUNCS = ['norm1', 'norm2', 'mocha']\n",
    "\n",
    "LAMBDA_LASSO = {'norm1': 0.1, 'norm2': 0.01, 'mocha': 0.05}\n",
    "\n",
    "K = 1000\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_sbm_2blocks_data(m=100, n=10, pin=1.0, pout=0.01)\n",
    "E, N = B.shape\n",
    "\n",
    "alg1_scores = []\n",
    "fl_scores = []\n",
    "linear_regression_scores = []\n",
    "decision_tree_scores = []\n",
    "\n",
    "num_tries = 5\n",
    "num_tries = 1\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "def fun():\n",
    "    samplingset = random.sample([j for j in range(N)], k=int(0.8* N)) \n",
    "    lambda_lasso = 0.1\n",
    "    _, primal_dual_w = primal_dual_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, predicted_w = algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, joona_w, _ = joonas_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, fl_w = fedAvg(K, datapoints, true_labels, samplingset)\n",
    "\n",
    "    return get_scores(datapoints, primal_dual_w, predicted_w, joona_w, fl_w, samplingset)\n",
    "\n",
    "\n",
    "# results = Parallel(n_jobs=4)(delayed(fun)() \n",
    "#                                          for i in range(num_tries))\n",
    "\n",
    "results = [fun()]\n",
    "\n",
    "for scores in results:\n",
    "    alg1_score, joonas_score, primal_dual_score, fl_score, linear_regression_score, decision_tree_score = scores\n",
    "    alg1_scores.append(alg1_score)\n",
    "    joonas_scores.append(joonas_score)\n",
    "    primal_dual_scores.append(primal_dual_score)\n",
    "    fl_scores.append(fl_score)\n",
    "    linear_regression_scores.append(linear_regression_score)\n",
    "    decision_tree_scores.append(decision_tree_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm 1: \n",
      " mean train MSE: 0.4540710184745062 \n",
      " mean test MSE: 0.5993696588085993\n",
      "joonas algorithm 1: \n",
      " mean train MSE: 0.26732173222918265 \n",
      " mean test MSE: 0.4043232729035058\n",
      "primal dual algorithm 1: \n",
      " mean train MSE: 0.00010335127894694271 \n",
      " mean test MSE: 0.00011461391786946419\n",
      "federated learning: \n",
      " mean train MSE: 39.35299212723412 \n",
      " mean test MSE: 41.272657241539\n",
      "linear regression: \n",
      " mean train MSE: 39.305657641370885 \n",
      " mean test MSE: 40.545847605997054\n",
      "decision tree: \n",
      " mean train MSE: 39.289615726164165 \n",
      " mean test MSE: 39.41200140924834\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['alg1', 'joonas', 'primal-dual', 'FedAvg', 'lin_reg', 'd_tree']\n",
    "x_pos = np.arange(len(labels))\n",
    "    \n",
    "    \n",
    "print('algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in alg1_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in alg1_scores]))\n",
    "\n",
    "print('joonas algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in joonas_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in joonas_scores]))\n",
    "\n",
    "print('primal dual algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in primal_dual_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in primal_dual_scores]))\n",
    "\n",
    "print('federated learning:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in fl_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in fl_scores])) \n",
    "                                                                         \n",
    "print('linear regression:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in linear_regression_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in linear_regression_scores]))\n",
    "\n",
    "print('decision tree:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in decision_tree_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in decision_tree_scores]))\n",
    "    \n",
    "alg1_norm1_score = [item['total'] for item in alg1_scores]\n",
    "joonas_score = [item['total'] for item in joonas_scores]\n",
    "primal_dual_score = [item['total'] for item in primal_dual_scores]\n",
    "fl_score = [item['total'] for item in fl_scores]  \n",
    "linear_regression_score = [item['total'] for item in linear_regression_scores]\n",
    "decision_tree_score = [item['total'] for item in decision_tree_scores]\n",
    "\n",
    "mean_MSEs = [\n",
    "    np.mean(alg1_norm1_score), \n",
    "    np.mean(joonas_score), \n",
    "    np.mean(primal_dual_score), \n",
    "    np.mean(fl_score), \n",
    "    np.mean(linear_regression_score), \n",
    "    np.mean(decision_tree_score)\n",
    "]\n",
    "\n",
    "std_MSEs = [\n",
    "    np.std(alg1_norm1_score), \n",
    "    np.std(joonas_score), \n",
    "    np.std(primal_dual_score), \n",
    "    np.std(fl_score),\n",
    "    np.std(linear_regression_score), \n",
    "    np.std(decision_tree_score)\n",
    "]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, mean_MSEs,\n",
    "   yerr=std_MSEs,\n",
    "   align='center',\n",
    "   alpha=0.5,\n",
    "   ecolor='black',\n",
    "   capsize=20)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_title('error bars plot')\n",
    "plt.show()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### Number of features: 50\n",
    "### Number of datapoints per node: 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "PENALTY_FUNCS = ['norm1', 'norm2', 'mocha']\n",
    "\n",
    "LAMBDA_LASSO = {'norm1': 0.1, 'norm2': 0.01, 'mocha': 0.05}\n",
    "\n",
    "K = 1000\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_sbm_2blocks_data(m=100, n=50, pin=1.0, pout=0.01)\n",
    "E, N = B.shape\n",
    "\n",
    "alg1_scores = []\n",
    "fl_scores = []\n",
    "linear_regression_scores = []\n",
    "decision_tree_scores = []\n",
    "\n",
    "num_tries = 5\n",
    "num_tries = 1\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "def fun():\n",
    "    samplingset = random.sample([j for j in range(N)], k=int(0.8* N)) \n",
    "    lambda_lasso = 0.1\n",
    "    _, primal_dual_w = primal_dual_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, predicted_w = algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, joona_w, _ = joonas_algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso)\n",
    "    _, fl_w = fedAvg(K, datapoints, true_labels, samplingset)\n",
    "\n",
    "    return get_scores(datapoints, primal_dual_w, predicted_w, joona_w, fl_w, samplingset)\n",
    "\n",
    "\n",
    "# results = Parallel(n_jobs=4)(delayed(fun)() \n",
    "#                                          for i in range(num_tries))\n",
    "\n",
    "results = [fun()]\n",
    "\n",
    "for scores in results:\n",
    "    alg1_score, joonas_score, primal_dual_score, fl_score, linear_regression_score, decision_tree_score = scores\n",
    "    alg1_scores.append(alg1_score)\n",
    "    joonas_scores.append(joonas_score)\n",
    "    primal_dual_scores.append(primal_dual_score)\n",
    "    fl_scores.append(fl_score)\n",
    "    linear_regression_scores.append(linear_regression_score)\n",
    "    decision_tree_scores.append(decision_tree_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm 1: \n",
      " mean train MSE: 2.2294739477134455 \n",
      " mean test MSE: 3.6534639928829735\n",
      "joonas algorithm 1: \n",
      " mean train MSE: 0.7640497192624996 \n",
      " mean test MSE: 1.228000950066652\n",
      "primal dual algorithm 1: \n",
      " mean train MSE: 7.864499588870881e-05 \n",
      " mean test MSE: 8.714189834428209e-05\n",
      "federated learning: \n",
      " mean train MSE: 196.89476993445453 \n",
      " mean test MSE: 200.06071928888906\n",
      "linear regression: \n",
      " mean train MSE: 195.94745447845594 \n",
      " mean test MSE: 201.61328466499492\n",
      "decision tree: \n",
      " mean train MSE: 196.3109574766272 \n",
      " mean test MSE: 205.48067463197128\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['alg1', 'joonas', 'primal-dual', 'FedAvg', 'lin_reg', 'd_tree']\n",
    "x_pos = np.arange(len(labels))\n",
    "    \n",
    "    \n",
    "print('algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in alg1_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in alg1_scores]))\n",
    "\n",
    "print('joonas algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in joonas_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in joonas_scores]))\n",
    "\n",
    "print('primal dual algorithm 1:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in primal_dual_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in primal_dual_scores]))\n",
    "\n",
    "print('federated learning:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in fl_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in fl_scores])) \n",
    "                                                                         \n",
    "print('linear regression:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in linear_regression_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in linear_regression_scores]))\n",
    "\n",
    "print('decision tree:', \n",
    "      '\\n mean train MSE:', np.mean([item['train'] for item in decision_tree_scores]),\n",
    "      '\\n mean test MSE:', np.mean([item['test'] for item in decision_tree_scores]))\n",
    "    \n",
    "alg1_norm1_score = [item['total'] for item in alg1_scores]\n",
    "joonas_score = [item['total'] for item in joonas_scores]\n",
    "primal_dual_score = [item['total'] for item in primal_dual_scores]\n",
    "fl_score = [item['total'] for item in fl_scores]  \n",
    "linear_regression_score = [item['total'] for item in linear_regression_scores]\n",
    "decision_tree_score = [item['total'] for item in decision_tree_scores]\n",
    "\n",
    "mean_MSEs = [\n",
    "    np.mean(alg1_norm1_score), \n",
    "    np.mean(joonas_score), \n",
    "    np.mean(primal_dual_score), \n",
    "    np.mean(fl_score), \n",
    "    np.mean(linear_regression_score), \n",
    "    np.mean(decision_tree_score)\n",
    "]\n",
    "\n",
    "std_MSEs = [\n",
    "    np.std(alg1_norm1_score), \n",
    "    np.std(joonas_score), \n",
    "    np.std(primal_dual_score), \n",
    "    np.std(fl_score),\n",
    "    np.std(linear_regression_score), \n",
    "    np.std(decision_tree_score)\n",
    "]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x_pos, mean_MSEs,\n",
    "   yerr=std_MSEs,\n",
    "   align='center',\n",
    "   alpha=0.5,\n",
    "   ecolor='black',\n",
    "   capsize=20)\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(labels)\n",
    "# ax.set_yscale('log')\n",
    "ax.set_title('error bars plot')\n",
    "plt.show()\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
