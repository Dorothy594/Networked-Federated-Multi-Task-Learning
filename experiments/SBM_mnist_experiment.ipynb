{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sahel/papers/FederatedLearning\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Block Model Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before geting into the experiment details, let's review algorithm 1 and the primal and dual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../algorithm1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/main.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from algorithm.penalty import *\n",
    "\n",
    "\n",
    "def algorithm_1(K, D, weight_vec, datapoints, true_labels, samplingset, lambda_lasso, penalty_func_name='norm1', calculate_score=False):\n",
    "    '''\n",
    "    :param K: the number of iterations\n",
    "    :param D: the block incidence matrix\n",
    "    :param weight_vec: a list containing the edges's weights of the graph\n",
    "    :param datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param true_labels: a list containing the true labels of the nodes\n",
    "    :param samplingset: the sampling set\n",
    "    :param lambda_lasso: the parameter lambda\n",
    "    :param penalty_func_name: the name of the penalty function used in the algorithm\n",
    "\n",
    "    :return iteration_scores: the mean squared error of the predicted weight vectors in each iteration\n",
    "    :return new_w: the predicted weigh vectors for each node\n",
    "    '''\n",
    "\n",
    "    Sigma = np.diag(np.full(weight_vec.shape, 0.9 / 2))\n",
    "    '''\n",
    "    Sigma: the block diagonal matrix Sigma\n",
    "    '''\n",
    "    T_matrix = np.diag(np.array((1.0 / (np.sum(abs(D), 0)))).ravel())\n",
    "    '''\n",
    "    T_matrix: the block diagonal matrix T\n",
    "    '''\n",
    "\n",
    "    if np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(T_matrix ** 0.5), 2) > 1:\n",
    "        print ('product norm', np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(T_matrix ** 0.5), 2))\n",
    "\n",
    "    E, N = D.shape\n",
    "#     m = datapoints[0]['features'].shape[0]\n",
    "    n = datapoints[0]['optimizer'].model.linear.weight.data.flatten().shape[0]\n",
    "\n",
    "    # define the penalty function\n",
    "#     if penalty_func_name == 'norm1':\n",
    "#         penalty_func = Norm1Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    if penalty_func_name == 'norm2':\n",
    "        penalty_func = Norm2Pelanty(lambda_lasso, weight_vec, Sigma)\n",
    "\n",
    "    elif penalty_func_name == 'mocha':\n",
    "        penalty_func = MOCHAPelanty(lambda_lasso, weight_vec, Sigma)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid penalty name')\n",
    "\n",
    "    # starting algorithm 1\n",
    "\n",
    "    new_w = np.array([np.zeros(n) for i in range(N)])\n",
    "    '''\n",
    "    new_w: the primal variable of the algorithm 1\n",
    "    '''\n",
    "    new_u = np.array([np.zeros(n) for i in range(E)])\n",
    "    '''\n",
    "    new_u: the dual variable of the algorithm 1\n",
    "    '''\n",
    "\n",
    "    iteration_scores = []\n",
    "    for iterk in range(K):\n",
    "        if iterk % 2 == 0:\n",
    "            print ('iter:', iterk)\n",
    "        prev_w = np.copy(new_w)\n",
    "\n",
    "        # algorithm 1, line 2\n",
    "        hat_w = new_w - np.dot(T_matrix, np.dot(D.T, new_u))\n",
    "\n",
    "        for i in range(N):\n",
    "            if i in samplingset:  # algorithm 1, line 6\n",
    "\n",
    "                optimizer = datapoints[i]['optimizer']\n",
    "                new_w[i] = optimizer.optimize(datapoints[i]['features'], datapoints[i]['label'], hat_w[i], datapoints[i]['degree'])\n",
    "\n",
    "            else:\n",
    "                new_w[i] = hat_w[i]\n",
    "\n",
    "        # algorithm 1, line 9\n",
    "        tilde_w = 2 * new_w - prev_w\n",
    "        new_u = new_u + np.dot(Sigma, np.dot(D, tilde_w))\n",
    "\n",
    "        # algorithm 1, line 10\n",
    "        new_u = penalty_func.update(new_u)\n",
    "\n",
    "        # calculate the MSE of the predicted weight vectors\n",
    "        if calculate_score:\n",
    "            Y_pred = []\n",
    "            for i in range(N):\n",
    "                test_output = optimizer.model.forward(x_data)\n",
    "                pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "                Y_pred.append(pred_y)\n",
    "                \n",
    "            Y_pred = np.array(Y_pred)\n",
    "\n",
    "            iteration_scores.append(mean_squared_error(true_labels.flatten(), Y_pred.flattein()))\n",
    "\n",
    "    # print (np.max(abs(new_w - prev_w)))\n",
    "\n",
    "    return iteration_scores, new_w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see in the algorithm picture, the primal update needs a optimizer operator for the sampling set (line 6). We have implemented the optimizers discussed in the paper, both the logistic loss and squared error loss optimizers implementations with pytorch is available, also we have implemented the squared error loss optimizer using the fixed point equation in the `Networked Linear Regression` section of the paper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/optimizer.py \n",
    "import torch\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "# The linear model which is implemented by pytorch\n",
    "class TorchLinearModel(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(TorchLinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "class TorchMnistModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TorchMnistModel, self).__init__()\n",
    "        self.conv1 = torch.nn.Sequential(         \n",
    "            torch.nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=5,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            torch.nn.ReLU(),                      \n",
    "            torch.nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = torch.nn.Sequential(         \n",
    "            torch.nn.Conv2d(16, 32, 5, 1, 2),     \n",
    "            torch.nn.ReLU(),                      \n",
    "            torch.nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.linear = torch.nn.Linear(32 * 7 * 7, 2)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1) \n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "\n",
    "# The abstract optimizer model which should have model, optimizer, and criterion as the input\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        old_weight = old_weight.reshape(self.model.linear.weight.data.shape)\n",
    "        torch_old_weight = torch.from_numpy(np.array(old_weight, dtype=np.float32))\n",
    "        self.model.linear.weight.data = torch_old_weight\n",
    "        for iterinner in range(40):\n",
    "            self.optimizer.zero_grad()\n",
    "            y_pred = self.model(x_data)\n",
    "            loss1 = self.criterion(y_pred, y_data)\n",
    "            loss2 = 1 / (2 * regularizer_term) * torch.mean((self.model.linear.weight - torch_old_weight) ** 2)  # + 10000*torch.mean((model.linear.bias+0.5)**2)#model.linear.weight.norm(2)\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        return self.model.linear.weight.data.numpy().flatten()\n",
    "\n",
    "\n",
    "# The linear model in Networked Linear Regression section of the paper\n",
    "class LinearModel:\n",
    "    def __init__(self, degree, features, label):\n",
    "        mtx1 = 2 * degree * np.dot(features.T, features).astype('float64')\n",
    "        mtx1 += 1 * np.eye(mtx1.shape[0])\n",
    "        mtx1_inv = np.linalg.inv(mtx1)\n",
    "\n",
    "        mtx2 = 2 * degree * np.dot(features.T, label).T\n",
    "\n",
    "        self.mtx1_inv = mtx1_inv\n",
    "        self.mtx2 = mtx2\n",
    "\n",
    "    def forward(self, x):\n",
    "        mtx2 = x + self.mtx2\n",
    "        mtx_inv = self.mtx1_inv\n",
    "\n",
    "        return np.dot(mtx_inv, mtx2)\n",
    "\n",
    "\n",
    "# The Linear optimizer in Networked Linear Regression section of the paper\n",
    "class LinearOptimizer(Optimizer):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(LinearOptimizer, self).__init__(model, None, None)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return self.model.forward(old_weight)\n",
    "\n",
    "\n",
    "# The Linear optimizer model which is implemented by pytorch\n",
    "class TorchLinearOptimizer(Optimizer):\n",
    "    def __init__(self, model):\n",
    "        criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters())\n",
    "        super(TorchLinearOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchLinearOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n",
    "\n",
    "\n",
    "# The Logistic optimizer model which is implemented by pytorch\n",
    "class TorchLogisticOptimizer(Optimizer):\n",
    "    def __init__(self, model):\n",
    "        criterion = torch.nn.BCELoss(reduction='mean')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters())\n",
    "        super(TorchLogisticOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchLogisticOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n",
    "\n",
    "    \n",
    "    \n",
    "class TorchMnistOptimizer(Optimizer):\n",
    "    def __init__(self, model):\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)   \n",
    "        super(TorchMnistOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchMnistOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the paper, the dual update has a penalty function(line 10) which is either norm1, norm2, or mocha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/penalty.py\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "# The abstract penalty function which has a function update\n",
    "class Penalty(ABC):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma):\n",
    "        self.lambda_lasso = lambda_lasso\n",
    "        self.weight_vec = weight_vec\n",
    "        self.Sigma = Sigma\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, new_u):\n",
    "        pass\n",
    "\n",
    "\n",
    "# The norm2 penalty function\n",
    "class Norm2Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma):\n",
    "        super(Norm2Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma)\n",
    "        self.limit = np.array(lambda_lasso * weight_vec)\n",
    "\n",
    "    def update(self, new_u):\n",
    "        normalized_u = np.where(np.linalg.norm(new_u, axis=1) >= self.limit)\n",
    "        new_u[normalized_u] = (new_u[normalized_u].T * self.limit[normalized_u] / np.linalg.norm(new_u[normalized_u], axis=1)).T\n",
    "        return new_u\n",
    "\n",
    "\n",
    "# The MOCHA penalty function\n",
    "class MOCHAPelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma):\n",
    "        super(MOCHAPelanty, self).__init__(lambda_lasso, weight_vec, Sigma)\n",
    "        self.normalize_factor = 1 + np.dot(2 * self.Sigma, 1/(self.lambda_lasso * self.weight_vec))\n",
    "\n",
    "    def update(self, new_u):\n",
    "        for i in range(new_u.shape[1]):\n",
    "            new_u[:, i] /= self.normalize_factor\n",
    "\n",
    "        return new_u\n",
    "\n",
    "\n",
    "# The norm1 penalty function\n",
    "class Norm1Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(Norm1Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma)\n",
    "        self.limit = np.array([np.zeros(n) for i in range(len(weight_vec))])\n",
    "        for i in range(n):\n",
    "            self.limit[:, i] = lambda_lasso * weight_vec\n",
    "\n",
    "    def update(self, new_u):\n",
    "        normalized_u = np.where(abs(new_u) >= self.limit)\n",
    "        new_u[normalized_u] = self.limit[normalized_u] * new_u[normalized_u] / abs(new_u[normalized_u])\n",
    "        return new_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SBM Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic block model is a generative model for random graphs with some clusters structure. Two nodes within the same cluster of the empirical graph are connected by an edge with probability pin, two nodes from different clusters are connected by an edge with probability pout. Each node $i \\in V$ represents a local dataset consisting of $m$ feature vectors $x^{(i,1)}, ... , x^{(i,m)} \\in R^n$. The feature vectors are i.i.d. realizations of a standard Gaussian random vector x ∼ N(0,I). The labels $y_1^{(i)}, . . . , y_m^{(i)} \\in R$ of the nodes $i \\in V$ are generated according to the linear model $y_r^{(i)} = (x^{(i, r)})^T w^{(i)} + \\epsilon$, with $\\epsilon ∼ N(0,\\sigma)$. To learn the weight $w^{(i)}$ ,we apply Algorithm 1 to a training set M obtained by randomly selecting 40% of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithm.optimizer import *\n",
    "from torch.autograd import Variable\n",
    "from graspy.simulations import sbm\n",
    "\n",
    "\n",
    "def get_sbm_data(cluster_sizes, G, train_loader):\n",
    "    '''\n",
    "    :param cluster_sizes: a list containing the size of each cluster\n",
    "    :param G: generated SBM graph with defined clusters using graspy.simulations\n",
    "    :param W: a list containing the weight vectors for each cluster\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "\n",
    "    N = len(G)\n",
    "    E = int(len(np.argwhere(G > 0))/2)\n",
    "    '''\n",
    "    N: total number of nodes\n",
    "    E: total number of edges\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # create B(adjacency matrix) and edges's weights vector(weight_vec) based on the graph G\n",
    "    B = np.zeros((E, N))\n",
    "    '''\n",
    "    B: adjacency matrix of the graph with the shape of E*N\n",
    "    '''\n",
    "    weight_vec = np.zeros(E)\n",
    "    '''\n",
    "    weight_vec: a list containing the edges's weights of the graph with the shape of E\n",
    "    '''\n",
    "    \n",
    "    cnt = 0\n",
    "    for i, j in np.argwhere(G > 0):\n",
    "        if i > j:\n",
    "            continue\n",
    "        B[cnt, i] = 1\n",
    "        B[cnt, j] = -1\n",
    "\n",
    "        weight_vec[cnt] = 1\n",
    "        cnt += 1\n",
    "    \n",
    "    \n",
    "    # create the data of each node needed for the algorithm 1 \n",
    "    \n",
    "    node_degrees = np.array((1.0 / (np.sum(abs(B), 0)))).ravel()\n",
    "    '''\n",
    "    node_degrees: a list containing the nodes degree for the alg1 (1/N_i)\n",
    "    '''\n",
    "    \n",
    "    datapoints = {}\n",
    "    '''\n",
    "    datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1,\n",
    "    which are features, label, degree, and also the optimizer model for each node\n",
    "    '''\n",
    "    true_labels = []\n",
    "    '''\n",
    "    true_labels: the true labels for the nodes of the graph\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for i, cluster_size in enumerate(cluster_sizes):\n",
    "#         for j in range(cluster_size):\n",
    "        train_size = 0\n",
    "        for j, (images, img_labels) in enumerate(train_loader[i]):\n",
    "            if j == 0:\n",
    "                train_size = len(img_labels)\n",
    "            if len(img_labels) != train_size:\n",
    "                continue\n",
    "            features = images\n",
    "            '''\n",
    "            features: the feature vector of node i which is a mnist image\n",
    "            '''\n",
    "            label = img_labels\n",
    "            '''\n",
    "            label: the label of the node i that is the mnist image cluster\n",
    "            '''\n",
    "            \n",
    "            true_labels.append(np.array(label))\n",
    "\n",
    "            model = TorchMnistModel()\n",
    "            optimizer = TorchMnistOptimizer(model)\n",
    "            features = Variable(features)\n",
    "            label = Variable(label)\n",
    "          \n",
    "            '''\n",
    "            model : the linear model for the node i \n",
    "            optimizer : the optimizer model for the node i \n",
    "            ''' \n",
    "            \n",
    "            datapoints[cnt] = {\n",
    "                'features': features,\n",
    "                'degree': node_degrees[i],\n",
    "                'label': label,\n",
    "                'optimizer': optimizer\n",
    "            }\n",
    "            cnt += 1\n",
    "        \n",
    "\n",
    "    return B, weight_vec, np.array(true_labels), datapoints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result we compare the MSE of Algorithm 1 with plain linear regression \n",
    "and decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load results/compare_results.py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def get_algorithm1_MSE(datapoints, predicted_w, samplingset):\n",
    "    '''\n",
    "    :param datapoints:  a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param predicted_w: the predicted weigh vectors for each node\n",
    "    :param samplingset: the sampling set for the algorithm 1\n",
    "\n",
    "    :return alg1_MSE: the MSE of the algorithm 1 for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "    not_samplingset = [i for i in range(len(datapoints)) if i not in samplingset]\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for i in range(len(datapoints)):\n",
    "        features = np.array(datapoints[i]['features'])\n",
    "        label = np.array(datapoints[i]['label'])\n",
    "        true_labels.append(label)\n",
    "\n",
    "        pred_labels.append(np.dot(features, predicted_w[i]))\n",
    "\n",
    "    pred_labels = np.array(pred_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    alg1_MSE = {'total': mean_squared_error(true_labels, pred_labels),\n",
    "                'train': mean_squared_error(true_labels[samplingset], pred_labels[samplingset]),\n",
    "                'test': mean_squared_error(true_labels[not_samplingset], pred_labels[not_samplingset])}\n",
    "\n",
    "    return alg1_MSE\n",
    "\n",
    "\n",
    "def get_linear_regression_MSE(x, y, samplingset, not_samplingset):\n",
    "    '''\n",
    "    :param x: a list containing the features of the nodes\n",
    "    :param y: a list containing the labels of the nodes\n",
    "    :param samplingset: the training dataset\n",
    "    :param not_samplingset: the test dataset\n",
    "    :return linear_regression_MSE : the MSE of linear regression for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "\n",
    "    model = LinearRegression().fit(x[samplingset], y[samplingset])\n",
    "    pred_y = model.predict(x)\n",
    "\n",
    "    linear_regression_MSE = {'total': mean_squared_error(y, pred_y),\n",
    "                             'train': mean_squared_error(y[samplingset],\n",
    "                                                         pred_y[samplingset]),\n",
    "                             'test': mean_squared_error(y[not_samplingset],\n",
    "                                                        pred_y[not_samplingset])}\n",
    "\n",
    "    return linear_regression_MSE\n",
    "\n",
    "\n",
    "def get_decision_tree_MSE(x, y, samplingset, not_samplingset):\n",
    "    '''\n",
    "    :param x: a list containing the features of the nodes\n",
    "    :param y: a list containing the labels of the nodes\n",
    "    :param samplingset: the training dataset\n",
    "    :param not_samplingset: the test dataset\n",
    "    :return decision_tree_MSE : the MSE of decision tree for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "\n",
    "    max_depth = 2\n",
    "\n",
    "    regressor = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    regressor.fit(x[samplingset], y[samplingset])\n",
    "    pred_y = regressor.predict(x)\n",
    "\n",
    "    decision_tree_MSE = {'total': mean_squared_error(y, pred_y),\n",
    "                         'train': mean_squared_error(y[samplingset],\n",
    "                                                     pred_y[samplingset]),\n",
    "                         'test': mean_squared_error(y[not_samplingset],\n",
    "                                                    pred_y[not_samplingset])}\n",
    "    return decision_tree_MSE\n",
    "\n",
    "\n",
    "def get_scores(datapoints, predicted_w, samplingset):\n",
    "    N = len(datapoints)\n",
    "    '''\n",
    "    N : the total number of nodes\n",
    "    '''\n",
    "\n",
    "    # calculate algorithm1 MSE\n",
    "    alg_1_score = get_algorithm1_MSE(datapoints, predicted_w, samplingset)\n",
    "\n",
    "    # prepare the data for calculating the linear regression and decision tree regression MSEs\n",
    "    X = []\n",
    "    '''\n",
    "    X: an array containing the features of all the nodes\n",
    "    '''\n",
    "    true_labels = []\n",
    "    '''\n",
    "    true_labels: an array containing the labels of all the nodes\n",
    "    '''\n",
    "    for i in range(len(datapoints)):\n",
    "        X.append(np.array(datapoints[i]['features']))\n",
    "        true_labels.append(np.array(datapoints[i]['label']))\n",
    "\n",
    "    X = np.array(X)\n",
    "    true_labels = np.array(true_labels)\n",
    "    m, n = X[0].shape\n",
    "\n",
    "    x = X.reshape(-1, n)\n",
    "    y = true_labels.reshape(-1, 1)\n",
    "\n",
    "    reformated_samplingset = []\n",
    "    for item in samplingset:\n",
    "        for i in range(m):\n",
    "            reformated_samplingset.append(m * item + i)\n",
    "    reformated_not_samplingset = [i for i in range(m * N) if i not in reformated_samplingset]\n",
    "\n",
    "    # calculate linear regression MSE\n",
    "    linear_regression_score = get_linear_regression_MSE(x, y, reformated_samplingset, reformated_not_samplingset)\n",
    "\n",
    "    # calculate decision tree MSE\n",
    "    decision_tree_score = get_decision_tree_MSE(x, y, reformated_samplingset, reformated_not_samplingset)\n",
    "\n",
    "    return alg_1_score, linear_regression_score, decision_tree_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBM with Two Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This SBM has two clusters $|C_1| = |C_2| = 100$.\n",
    "Two nodes within the same cluster are connected by an edge with probability `pin=0.5`, \n",
    "and two nodes from different clusters are connected by an edge with probability `pout=0.01`. \n",
    "Each node $i \\in V$ represents a local dataset consisting of feature vectors $x^{(i,1)}, ... , x^{(i,5)} \\in R^2$.\n",
    "The feature vectors are i.i.d. realizations of a standard Gaussian random vector x ~ N(0,I).\n",
    "The labels $y_1^{(i)}, . . . , y_5^{(i)} \\in R$ for each node $i \\in V$\n",
    "are generated according to the linear model $y_r^{(i)} = (x^{(i, r)})^T w^{(i)} + \\epsilon$, with $\\epsilon = 0$. \n",
    "The tuning parameter $\\lambda$ in algorithm1 \n",
    "is manually chosen, guided by the resulting MSE, as $\\lambda=0.01$ for norm1 and norm2 and also $\\lambda=0.05$ for mocha penalty function. \n",
    "To learn the weight $w^{(i)}$ ,we apply Algorithm 1 to a training set M obtained by randomly selecting 40% of the nodes and use the rest as test set. As the result we compare the mean MSE of Algorithm 1 with plain linear regression and decision tree regression with respect to the different random sampling sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graspy.simulations import sbm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "class YourSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, mask, data_source):\n",
    "        self.mask = mask\n",
    "        self.data_source = data_source\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([i.item() for i in torch.nonzero(self.mask)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "\n",
    "\n",
    "def get_sbm_2blocks_data(m=5, n=2, pin=0.5, pout=0.01, noise_sd=0, is_torch_model=True):\n",
    "    '''\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "    N1, N2 = 100, 100\n",
    "    cluster_sizes = [N1, N2]\n",
    "\n",
    "    # generate graph G which is a SBM wich 2 clusters\n",
    "    G = sbm(n=cluster_sizes, p=[[pin, pout],[pout, pin]])\n",
    "    '''\n",
    "    G: generated SBM graph with 2 clusters\n",
    "    ''' \n",
    "    \n",
    "    mnist = datasets.MNIST(\n",
    "        root = 'data',\n",
    "        train = True,                         \n",
    "        transform = ToTensor(), \n",
    "        download = True,            \n",
    "    )\n",
    "       \n",
    "    mask = [1 if mnist[i][1] == 0 else 0 for i in range(len(mnist))]\n",
    "    mask = torch.tensor(mask)   \n",
    "    sampler = YourSampler(mask, mnist)\n",
    "    batch_zie = int(len(np.where(mask==1)[0])/N1)\n",
    "    trainloader_zero = torch.utils.data.DataLoader(mnist, batch_size=batch_zie,sampler = sampler, shuffle=False)\n",
    "\n",
    "    mask = [1 if mnist[i][1] == 1 else 0 for i in range(len(mnist))]\n",
    "    mask = torch.tensor(mask)   \n",
    "    sampler = YourSampler(mask, mnist)\n",
    "    batch_zie = int(len(np.where(mask==1)[0])/N2)\n",
    "    trainloader_one = torch.utils.data.DataLoader(mnist, batch_size=batch_zie,sampler = sampler, shuffle=False)\n",
    "    \n",
    "    \n",
    "    return get_sbm_data(cluster_sizes, G, (trainloader_zero, trainloader_one))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the MSE with respect to the different random sampling sets for each penalty function, the plots are in the log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sahel/anaconda3/envs/aalto/lib/python3.7/site-packages/ipykernel_launcher.py:106: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0\n",
      "iter: 2\n",
      "iter: 4\n",
      "iter: 6\n",
      "iter: 8\n",
      "iter: 10\n",
      "iter: 12\n",
      "iter: 14\n",
      "iter: 16\n",
      "iter: 18\n",
      "iter: 20\n",
      "iter: 22\n",
      "iter: 24\n",
      "iter: 26\n",
      "iter: 28\n",
      "iter: 30\n",
      "iter: 32\n",
      "iter: 34\n",
      "iter: 36\n",
      "iter: 38\n",
      "iter: 40\n",
      "iter: 42\n",
      "iter: 44\n",
      "iter: 46\n",
      "iter: 48\n",
      "iter: 50\n",
      "iter: 52\n",
      "iter: 54\n",
      "iter: 56\n",
      "iter: 58\n",
      "iter: 60\n",
      "iter: 62\n",
      "iter: 64\n",
      "iter: 66\n",
      "iter: 68\n",
      "iter: 70\n",
      "iter: 72\n",
      "iter: 74\n",
      "iter: 76\n",
      "iter: 78\n",
      "iter: 80\n",
      "iter: 82\n",
      "iter: 84\n",
      "iter: 86\n",
      "iter: 88\n",
      "iter: 90\n",
      "iter: 92\n",
      "iter: 94\n",
      "iter: 96\n",
      "iter: 98\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "PENALTY_FUNCS = ['norm1', 'norm2', 'mocha']\n",
    "PENALTY_FUNCS = ['norm2', 'mocha']\n",
    "\n",
    "LAMBDA_LASSO = {'norm1': 0.01, 'norm2': 0.01, 'mocha': 0.05}\n",
    "\n",
    "K = 1000\n",
    "K = 100\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_sbm_2blocks_data(pin=0.5, pout=0.01, is_torch_model=False)\n",
    "E, N = B.shape\n",
    "\n",
    "alg1_scores = defaultdict(list)\n",
    "linear_regression_scores = defaultdict(list)\n",
    "decision_tree_scores = defaultdict(list)\n",
    "\n",
    "num_tries = 5\n",
    "num_tries = 1\n",
    "for i in range(num_tries):\n",
    "    samplingset = random.sample([i for i in range(N)], k=int(0.4* N))\n",
    "\n",
    "    for penalty_func in PENALTY_FUNCS:\n",
    "\n",
    "        lambda_lasso = LAMBDA_LASSO[penalty_func]\n",
    "        _, predicted_w = algorithm_1(K, B, weight_vec, datapoints, true_labels, samplingset, lambda_lasso, penalty_func)\n",
    "\n",
    "#         alg1_score, linear_regression_score, decision_tree_score = get_scores(datapoints, predicted_w, samplingset)\n",
    "        \n",
    "#         alg1_scores[penalty_func].append(alg1_score)\n",
    "#         linear_regression_scores[penalty_func].append(linear_regression_score)\n",
    "#         decision_tree_scores[penalty_func].append(decision_tree_score)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for node 0 is: 0.03389830508474576\n",
      "accuracy for node 1 is: 1.0\n",
      "accuracy for node 2 is: 1.0\n",
      "accuracy for node 3 is: 1.0\n",
      "accuracy for node 4 is: 0.0\n",
      "accuracy for node 5 is: 1.0\n",
      "accuracy for node 6 is: 0.3389830508474576\n",
      "accuracy for node 7 is: 1.0\n",
      "accuracy for node 8 is: 0.0\n",
      "accuracy for node 9 is: 1.0\n",
      "accuracy for node 10 is: 1.0\n",
      "accuracy for node 11 is: 1.0\n",
      "accuracy for node 12 is: 1.0\n",
      "accuracy for node 13 is: 1.0\n",
      "accuracy for node 14 is: 1.0\n",
      "accuracy for node 15 is: 1.0\n",
      "accuracy for node 16 is: 0.01694915254237288\n",
      "accuracy for node 17 is: 0.9322033898305084\n",
      "accuracy for node 18 is: 0.288135593220339\n",
      "accuracy for node 19 is: 0.3050847457627119\n",
      "accuracy for node 20 is: 0.5254237288135594\n",
      "accuracy for node 21 is: 1.0\n",
      "accuracy for node 22 is: 0.0\n",
      "accuracy for node 23 is: 1.0\n",
      "accuracy for node 24 is: 0.3898305084745763\n",
      "accuracy for node 25 is: 1.0\n",
      "accuracy for node 26 is: 0.15254237288135594\n",
      "accuracy for node 27 is: 0.03389830508474576\n",
      "accuracy for node 28 is: 1.0\n",
      "accuracy for node 29 is: 1.0\n",
      "accuracy for node 30 is: 0.7627118644067796\n",
      "accuracy for node 31 is: 0.1016949152542373\n",
      "accuracy for node 32 is: 1.0\n",
      "accuracy for node 33 is: 0.3559322033898305\n",
      "accuracy for node 34 is: 0.11864406779661017\n",
      "accuracy for node 35 is: 0.9322033898305084\n",
      "accuracy for node 36 is: 0.0\n",
      "accuracy for node 37 is: 1.0\n",
      "accuracy for node 38 is: 0.01694915254237288\n",
      "accuracy for node 39 is: 1.0\n",
      "accuracy for node 40 is: 1.0\n",
      "accuracy for node 41 is: 0.0\n",
      "accuracy for node 42 is: 1.0\n",
      "accuracy for node 43 is: 1.0\n",
      "accuracy for node 44 is: 1.0\n",
      "accuracy for node 45 is: 0.0\n",
      "accuracy for node 46 is: 1.0\n",
      "accuracy for node 47 is: 1.0\n",
      "accuracy for node 48 is: 1.0\n",
      "accuracy for node 49 is: 0.13559322033898305\n",
      "accuracy for node 50 is: 0.05084745762711865\n",
      "accuracy for node 51 is: 0.0\n",
      "accuracy for node 52 is: 1.0\n",
      "accuracy for node 53 is: 0.0\n",
      "accuracy for node 54 is: 1.0\n",
      "accuracy for node 55 is: 0.06779661016949153\n",
      "accuracy for node 56 is: 1.0\n",
      "accuracy for node 57 is: 0.8305084745762712\n",
      "accuracy for node 58 is: 1.0\n",
      "accuracy for node 59 is: 1.0\n",
      "accuracy for node 60 is: 0.0\n",
      "accuracy for node 61 is: 0.1864406779661017\n",
      "accuracy for node 62 is: 0.4576271186440678\n",
      "accuracy for node 63 is: 1.0\n",
      "accuracy for node 64 is: 0.01694915254237288\n",
      "accuracy for node 65 is: 1.0\n",
      "accuracy for node 66 is: 1.0\n",
      "accuracy for node 67 is: 0.8305084745762712\n",
      "accuracy for node 68 is: 1.0\n",
      "accuracy for node 69 is: 0.9491525423728814\n",
      "accuracy for node 70 is: 1.0\n",
      "accuracy for node 71 is: 0.6779661016949152\n",
      "accuracy for node 72 is: 1.0\n",
      "accuracy for node 73 is: 1.0\n",
      "accuracy for node 74 is: 0.0\n",
      "accuracy for node 75 is: 0.5084745762711864\n",
      "accuracy for node 76 is: 0.9830508474576272\n",
      "accuracy for node 77 is: 1.0\n",
      "accuracy for node 78 is: 0.9152542372881356\n",
      "accuracy for node 79 is: 1.0\n",
      "accuracy for node 80 is: 1.0\n",
      "accuracy for node 81 is: 0.9322033898305084\n",
      "accuracy for node 82 is: 0.3389830508474576\n",
      "accuracy for node 83 is: 0.847457627118644\n",
      "accuracy for node 84 is: 1.0\n",
      "accuracy for node 85 is: 1.0\n",
      "accuracy for node 86 is: 0.6271186440677966\n",
      "accuracy for node 87 is: 0.03389830508474576\n",
      "accuracy for node 88 is: 0.2033898305084746\n",
      "accuracy for node 89 is: 1.0\n",
      "accuracy for node 90 is: 1.0\n",
      "accuracy for node 91 is: 0.9322033898305084\n",
      "accuracy for node 92 is: 1.0\n",
      "accuracy for node 93 is: 0.0\n",
      "accuracy for node 94 is: 0.6610169491525424\n",
      "accuracy for node 95 is: 0.05084745762711865\n",
      "accuracy for node 96 is: 0.9830508474576272\n",
      "accuracy for node 97 is: 0.9661016949152542\n",
      "accuracy for node 98 is: 1.0\n",
      "accuracy for node 99 is: 1.0\n",
      "accuracy for node 100 is: 0.582089552238806\n",
      "accuracy for node 101 is: 1.0\n",
      "accuracy for node 102 is: 1.0\n",
      "accuracy for node 103 is: 0.0\n",
      "accuracy for node 104 is: 0.0\n",
      "accuracy for node 105 is: 0.5671641791044776\n",
      "accuracy for node 106 is: 1.0\n",
      "accuracy for node 107 is: 1.0\n",
      "accuracy for node 108 is: 0.9253731343283582\n",
      "accuracy for node 109 is: 1.0\n",
      "accuracy for node 110 is: 1.0\n",
      "accuracy for node 111 is: 1.0\n",
      "accuracy for node 112 is: 0.9552238805970149\n",
      "accuracy for node 113 is: 0.0\n",
      "accuracy for node 114 is: 1.0\n",
      "accuracy for node 115 is: 0.0\n",
      "accuracy for node 116 is: 0.0\n",
      "accuracy for node 117 is: 0.3582089552238806\n",
      "accuracy for node 118 is: 0.0\n",
      "accuracy for node 119 is: 1.0\n",
      "accuracy for node 120 is: 1.0\n",
      "accuracy for node 121 is: 1.0\n",
      "accuracy for node 122 is: 0.0\n",
      "accuracy for node 123 is: 0.9552238805970149\n",
      "accuracy for node 124 is: 1.0\n",
      "accuracy for node 125 is: 0.23880597014925373\n",
      "accuracy for node 126 is: 0.029850746268656716\n",
      "accuracy for node 127 is: 0.05970149253731343\n",
      "accuracy for node 128 is: 1.0\n",
      "accuracy for node 129 is: 1.0\n",
      "accuracy for node 130 is: 1.0\n",
      "accuracy for node 131 is: 0.0\n",
      "accuracy for node 132 is: 1.0\n",
      "accuracy for node 133 is: 0.16417910447761194\n",
      "accuracy for node 134 is: 1.0\n",
      "accuracy for node 135 is: 0.8208955223880597\n",
      "accuracy for node 136 is: 1.0\n",
      "accuracy for node 137 is: 0.0\n",
      "accuracy for node 138 is: 0.0\n",
      "accuracy for node 139 is: 1.0\n",
      "accuracy for node 140 is: 1.0\n",
      "accuracy for node 141 is: 1.0\n",
      "accuracy for node 142 is: 0.0\n",
      "accuracy for node 143 is: 0.0\n",
      "accuracy for node 144 is: 0.7611940298507462\n",
      "accuracy for node 145 is: 1.0\n",
      "accuracy for node 146 is: 1.0\n",
      "accuracy for node 147 is: 0.0\n",
      "accuracy for node 148 is: 1.0\n",
      "accuracy for node 149 is: 1.0\n",
      "accuracy for node 150 is: 1.0\n",
      "accuracy for node 151 is: 0.5373134328358209\n",
      "accuracy for node 152 is: 0.3880597014925373\n",
      "accuracy for node 153 is: 1.0\n",
      "accuracy for node 154 is: 1.0\n",
      "accuracy for node 155 is: 0.0\n",
      "accuracy for node 156 is: 0.4626865671641791\n",
      "accuracy for node 157 is: 1.0\n",
      "accuracy for node 158 is: 0.0\n",
      "accuracy for node 159 is: 1.0\n",
      "accuracy for node 160 is: 0.8656716417910447\n",
      "accuracy for node 161 is: 0.8208955223880597\n",
      "accuracy for node 162 is: 1.0\n",
      "accuracy for node 163 is: 1.0\n",
      "accuracy for node 164 is: 0.08955223880597014\n",
      "accuracy for node 165 is: 0.8805970149253731\n",
      "accuracy for node 166 is: 1.0\n",
      "accuracy for node 167 is: 0.47761194029850745\n",
      "accuracy for node 168 is: 0.0\n",
      "accuracy for node 169 is: 1.0\n",
      "accuracy for node 170 is: 0.0\n",
      "accuracy for node 171 is: 1.0\n",
      "accuracy for node 172 is: 0.014925373134328358\n",
      "accuracy for node 173 is: 1.0\n",
      "accuracy for node 174 is: 1.0\n",
      "accuracy for node 175 is: 1.0\n",
      "accuracy for node 176 is: 1.0\n",
      "accuracy for node 177 is: 1.0\n",
      "accuracy for node 178 is: 1.0\n",
      "accuracy for node 179 is: 0.8208955223880597\n",
      "accuracy for node 180 is: 1.0\n",
      "accuracy for node 181 is: 1.0\n",
      "accuracy for node 182 is: 1.0\n",
      "accuracy for node 183 is: 1.0\n",
      "accuracy for node 184 is: 0.0\n",
      "accuracy for node 185 is: 1.0\n",
      "accuracy for node 186 is: 0.5373134328358209\n",
      "accuracy for node 187 is: 1.0\n",
      "accuracy for node 188 is: 1.0\n",
      "accuracy for node 189 is: 0.8208955223880597\n",
      "accuracy for node 190 is: 1.0\n",
      "accuracy for node 191 is: 1.0\n",
      "accuracy for node 192 is: 1.0\n",
      "accuracy for node 193 is: 1.0\n",
      "accuracy for node 194 is: 1.0\n",
      "accuracy for node 195 is: 1.0\n",
      "accuracy for node 196 is: 0.2537313432835821\n",
      "accuracy for node 197 is: 0.0\n",
      "accuracy for node 198 is: 0.9552238805970149\n",
      "accuracy for node 199 is: 0.0\n"
     ]
    }
   ],
   "source": [
    "Y_pred = []\n",
    "for i in range(N):\n",
    "    optimizer = datapoints[i]['optimizer']\n",
    "    test_output = optimizer.model.forward(datapoints[i]['features'])\n",
    "    pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "    print('accuracy for node %d is:' % i, len(np.where(pred_y == datapoints[i]['label'])[0])/len(pred_y))\n",
    "    Y_pred.append(pred_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
