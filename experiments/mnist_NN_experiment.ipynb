{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sarchey1/paper/FederatedLearning\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain graph Neural Network Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before geting into the experiment details, let's review algorithm 1 and the primal and dual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../algorithm1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/main.py\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from algorithm.penalty import *\n",
    "\n",
    "\n",
    "from abc import ABC\n",
    "from joblib import Parallel, delayed\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "class OptVar(ABC):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = vectors\n",
    "        \n",
    "    def get_node_vectors(self, node):\n",
    "        node_vectors = []\n",
    "        for vec in self.vectors:\n",
    "            node_vectors.append(vec[node])\n",
    "        return node_vectors\n",
    "    \n",
    "    def set_node_vectors(self, node, node_vectors):\n",
    "        for i, node_vec in enumerate(node_vectors):\n",
    "            self.vectors[i][node] = node_vec\n",
    "            \n",
    "    def diff(self, var2):\n",
    "        diff_vec = []\n",
    "        for i, sub_w in enumerate(self.vectors):\n",
    "            diff_vec.append(sub_w - var2.vectors[i])\n",
    "        return OptVar(diff_vec)\n",
    "    \n",
    "    def mult(self, num):\n",
    "        for i, sub_vec in enumerate(self.vectors):\n",
    "            self.vectors[i] = num * sub_vec\n",
    "    \n",
    "\n",
    "class PrimalVar(OptVar):\n",
    "    def __init__(self, primal_vectors, T_matrix, D):\n",
    "        super(PrimalVar, self).__init__(primal_vectors)\n",
    "        self.T_matrix = T_matrix\n",
    "        self.D = D\n",
    "            \n",
    "    def update(self, new_u):\n",
    "        hat_w = []\n",
    "        for i, sub_w in enumerate(self.vectors):\n",
    "            hat_w.append(sub_w - np.dot(self.T_matrix, np.dot(self.D.T, new_u.vectors[i])))\n",
    "        return OptVar(hat_w)\n",
    "    \n",
    "class DualVar(OptVar):\n",
    "    def __init__(self, dual_vectors, Sigma, D):\n",
    "        super(DualVar, self).__init__(dual_vectors)\n",
    "        self.Sigma = Sigma\n",
    "        self.D = D\n",
    "            \n",
    "    def update(self, tilde_w):\n",
    "        for i, sub_vec in enumerate(self.vectors):\n",
    "            self.vectors[i] = sub_vec + np.dot(self.Sigma, np.dot(self.D, tilde_w.vectors[i]))\n",
    "\n",
    "            \n",
    "def primal_update(hat_w_i, datapoints, i):\n",
    "    optimizer = datapoints[i]['optimizer']\n",
    "    res = optimizer.optimize(\n",
    "        datapoints[i]['features'], \n",
    "        datapoints[i]['label'], \n",
    "        hat_w_i, \n",
    "        datapoints[i]['degree'],\n",
    "    )\n",
    "    return res, i\n",
    "    \n",
    "\n",
    "            \n",
    "def algorithm_1(K, D, weight_vec, datapoints, true_labels, samplingset, lambda_lasso, penalty_func_name='norm1', calculate_score=False):\n",
    "    '''\n",
    "    :param K: the number of iterations\n",
    "    :param D: the block incidence matrix\n",
    "    :param weight_vec: a list containing the edges's weights of the graph\n",
    "    :param datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param true_labels: a list containing the true labels of the nodes\n",
    "    :param samplingset: the sampling set\n",
    "    :param lambda_lasso: the parameter lambda\n",
    "    :param penalty_func_name: the name of the penalty function used in the algorithm\n",
    "\n",
    "    :return iteration_scores: the mean squared error of the predicted weight vectors in each iteration\n",
    "    :return new_w: the predicted weigh vectors for each node\n",
    "    '''\n",
    "\n",
    "    Sigma = np.diag(np.full(weight_vec.shape, 0.9 / 2))\n",
    "    '''\n",
    "    Sigma: the block diagonal matrix Sigma\n",
    "    '''\n",
    "    T_matrix = np.diag(np.array((1.0 / (np.sum(abs(D), 0)))).ravel())\n",
    "    '''\n",
    "    T_matrix: the block diagonal matrix T\n",
    "    '''\n",
    "\n",
    "    if np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(T_matrix ** 0.5), 2) > 1:\n",
    "        print ('product norm', np.linalg.norm(np.dot(Sigma ** 0.5, D).dot(T_matrix ** 0.5), 2))\n",
    "\n",
    "    E, N = D.shape\n",
    "#     m, n = datapoints[0]['features'].shape\n",
    "    m = len(datapoints[0]['features'])\n",
    "    n = np.sum(datapoints[0]['features'][0].shape)\n",
    "\n",
    "    # define the penalty function\n",
    "    if penalty_func_name == 'norm1':\n",
    "#         penalty_func = Norm1Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "        penalty_func = Norm2Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    elif penalty_func_name == 'norm2':\n",
    "        penalty_func = Norm2Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    elif penalty_func_name == 'mocha':\n",
    "        penalty_func = MOCHAPelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "   \n",
    "    elif penalty_func_name == 'sq_norm2':\n",
    "        penalty_func = SquaredNorm2Pelanty(lambda_lasso, weight_vec, Sigma, n)\n",
    "\n",
    "    else:\n",
    "        raise Exception('Invalid penalty name')\n",
    "\n",
    "    # starting algorithm 1\n",
    "    model = datapoints[0]['optimizer'].model\n",
    "    out1, in1 = model.linear1.in_features, model.linear1.out_features\n",
    "    out2, in2 = model.linear2.in_features, model.linear2.out_features\n",
    "    \n",
    "    l1 = np.random.random(in1*out1)\n",
    "    l2 = np.random.random(in2*out2)\n",
    "    new_w = PrimalVar([\n",
    "        np.array([l1 for i in range(N)]), \n",
    "        np.array([l2 for i in range(N)]),\n",
    "    ], T_matrix, D)\n",
    "    hat_w = np.copy(new_w)\n",
    "    '''\n",
    "    new_w: the primal variable of the algorithm 1\n",
    "    '''\n",
    "    new_u = DualVar([\n",
    "        np.array([l1 for i in range(E)]),\n",
    "        np.array([l2 for i in range(E)]),\n",
    "    ], Sigma, D)\n",
    "    '''\n",
    "    new_u: the dual variable of the algorithm 1\n",
    "    '''\n",
    "\n",
    "    iteration_scores = []\n",
    "    for iterk in range(K):\n",
    "#         if iterk % 2 == 0:\n",
    "#             alg1_score = get_algorithm1_error(datapoints, new_w, samplingset)\n",
    "#             print ('iter:', iterk, alg1_score)\n",
    "        prev_w = copy.deepcopy(new_w)\n",
    "\n",
    "        # algorithm 1, line 2\n",
    "        hat_w = new_w.update(new_u)\n",
    "        \n",
    "#         results = Parallel(n_jobs=10)(delayed(primal_update)(hat_w.get_node_vectors(i), datapoints, i)\n",
    "#                                              for i in samplingset)  \n",
    "        \n",
    "#         for res, i in results:\n",
    "#             new_w.set_node_vectors(i, res)\n",
    "\n",
    "        for i in samplingset:\n",
    "            res, i = primal_update(hat_w.get_node_vectors(i), datapoints, i)\n",
    "            new_w.set_node_vectors(i, res)\n",
    " \n",
    "        for i in range(N):\n",
    "            if i in samplingset:  # algorithm 1, line 6\n",
    "#                 optimizer = datapoints[i]['optimizer']\n",
    "#                 new_w.set_node_vectors(i, optimizer.optimize(\n",
    "#                     datapoints[i]['features'], \n",
    "#                     datapoints[i]['label'], \n",
    "#                     hat_w.get_node_vectors(i), \n",
    "#                     datapoints[i]['degree'],\n",
    "#                 ))\n",
    "                pass\n",
    "            else:\n",
    "                new_w.vectors[0][i] = hat_w.vectors[0][i]\n",
    "                new_w.vectors[1][i] = hat_w.vectors[1][i]\n",
    "\n",
    "        # algorithm 1, line 9\n",
    "        tilde_w = new_w.diff(prev_w)\n",
    "        tilde_w.mult(2)\n",
    "        new_u.update(tilde_w)\n",
    "        \n",
    "        # algorithm 1, line 10\n",
    "        if lambda_lasso != 0:\n",
    "            new_u.vectors[0] = penalty_func.update(new_u.vectors[0])\n",
    "            new_u.vectors[1] = penalty_func.update(new_u.vectors[1])\n",
    "        \n",
    "\n",
    "        # calculate the MSE of the predicted weight vectors\n",
    "        if calculate_score:\n",
    "            alg1_score = get_algorithm1_error(datapoints, new_w, samplingset)\n",
    "            iteration_scores.append(alg1_score)\n",
    "\n",
    "    # print (np.max(abs(new_w - prev_w)))\n",
    "    \n",
    "    for i in range(N):\n",
    "        optimizer = datapoints[i]['optimizer']\n",
    "        \n",
    "        optimizer.model.linear1.weight.data = torch.from_numpy(\n",
    "            np.array(new_w.vectors[0][i].reshape(in1, out1), dtype=np.float32))\n",
    "        optimizer.model.linear2.weight.data = torch.from_numpy(\n",
    "            np.array(new_w.vectors[1][i].reshape(in2, out2), dtype=np.float32))\n",
    "\n",
    "\n",
    "    return iteration_scores, new_w, new_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primal Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see in the algorithm picture, the primal update needs a optimizer operator for the sampling set (line 6). We have implemented the optimizers discussed in the paper, both the logistic loss and squared error loss optimizers implementations with pytorch is available, also we have implemented the squared error loss optimizer using the fixed point equation in the `Networked Linear Regression` section of the paper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/optimizer.py \n",
    "import torch\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# The linear model which is implemented by pytorch\n",
    "class TorchLinearModel(torch.nn.Module):\n",
    "    def __init__(self, n):\n",
    "        super(TorchLinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# The SimpleLinear which is for MNIST experiment\n",
    "class SimpleLinear(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(28*28, h1)\n",
    "        self.linear2 = torch.nn.Linear(h1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# The abstract optimizer model which should have model, optimizer, and criterion as the input\n",
    "class Optimizer(ABC):\n",
    "    def __init__(self, model, optimizer, criterion):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        out1, in1 = self.model.linear1.in_features, self.model.linear1.out_features\n",
    "        out2, in2 = self.model.linear2.in_features, self.model.linear2.out_features\n",
    "        \n",
    "        torch_old_weight0 = torch.from_numpy(np.array(old_weight[0], dtype=np.float32).reshape(in1, out1))\n",
    "        torch_old_weight1 = torch.from_numpy(np.array(old_weight[1], dtype=np.float32).reshape(in2, out2))\n",
    "        \n",
    "        if np.sum(old_weight[0]) == 0:\n",
    "#             print('hereee ', np.sum(old_weight[0]))\n",
    "            self.model.linear1.weight.data = torch.tensor(np.array(np.random.rand(in1*out1), dtype=np.float32).reshape(in1, out1))\n",
    "            self.model.linear2.weight.data = torch.tensor(np.array(np.random.rand(in2*out2), dtype=np.float32).reshape(in2, out2))\n",
    "        else:\n",
    "            self.model.linear1.weight.data = torch.tensor(np.array(old_weight[0], dtype=np.float32).reshape(in1, out1))\n",
    "            self.model.linear2.weight.data = torch.tensor(np.array(old_weight[1], dtype=np.float32).reshape(in2, out2))\n",
    "            \n",
    "        for iterinner in range(40):\n",
    "#         for iterinner in range(30):\n",
    "            y_pred = self.model(x_data)\n",
    "#             y_pred = torch.argmax(y_pred, axis=1)\n",
    "#             loss1 = self.criterion(y_pred.ravel(), y_data.ravel())\n",
    "            loss1 = self.criterion(y_pred, y_data)\n",
    "            loss2 = 1 / (2 * regularizer_term) * torch.mean((self.model.linear1.weight.data - torch_old_weight0) ** 2)  # + 10000*torch.mean((model.linear.bias+0.5)**2)#model.linear.weight.norm(2)\n",
    "            loss3 = 1 / (2 * regularizer_term) * torch.mean((self.model.linear2.weight.data - torch_old_weight1) ** 2)\n",
    "            loss = loss1 + loss2 + loss3\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "#         print('old_weight', old_weight)\n",
    "#         print('model', self.model.linear1.weight.data.numpy().ravel(), self.model.linear2.weight.data.numpy().ravel())\n",
    "        return [self.model.linear1.weight.data.numpy().ravel(), \n",
    "                self.model.linear2.weight.data.numpy().ravel()]\n",
    "\n",
    "\n",
    "# The linear model in Networked Linear Regression section of the paper\n",
    "class LinearModel:\n",
    "    def __init__(self, degree, features, label):\n",
    "        mtx1 = 2 * degree * np.dot(features.T, features).astype('float64')\n",
    "        mtx1 += 1 * np.eye(mtx1.shape[0])\n",
    "        mtx1_inv = np.linalg.inv(mtx1)\n",
    "\n",
    "        mtx2 = 2 * degree * np.dot(features.T, label).T\n",
    "\n",
    "        self.mtx1_inv = mtx1_inv\n",
    "        self.mtx2 = mtx2\n",
    "\n",
    "    def forward(self, x):\n",
    "        mtx2 = x + self.mtx2\n",
    "        mtx_inv = self.mtx1_inv\n",
    "\n",
    "        return np.dot(mtx_inv, mtx2)\n",
    "\n",
    "\n",
    "# The Linear optimizer in Networked Linear Regression section of the paper\n",
    "class LinearOptimizer(Optimizer):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super(LinearOptimizer, self).__init__(model, None, None)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return self.model.forward(old_weight)\n",
    "\n",
    "\n",
    "# The Linear optimizer model which is implemented by pytorch\n",
    "class TorchLinearOptimizer(Optimizer):\n",
    "    def __init__(self, model, criterion):\n",
    "#         criterion = torch.nn.MSELoss(reduction='mean')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters())\n",
    "        super(TorchLinearOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchLinearOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n",
    "\n",
    "\n",
    "# The Logistic optimizer model which is implemented by pytorch\n",
    "class TorchLogisticOptimizer(Optimizer):\n",
    "    def __init__(self, model):\n",
    "        criterion = torch.nn.BCELoss(reduction='mean')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters())\n",
    "        super(TorchLogisticOptimizer, self).__init__(model, optimizer, criterion)\n",
    "\n",
    "    def optimize(self, x_data, y_data, old_weight, regularizer_term):\n",
    "        return super(TorchLogisticOptimizer, self).optimize(x_data, y_data, old_weight, regularizer_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the paper, the dual update has a penalty function(line 10) which is either norm1, norm2, or mocha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load algorithm/penalty.py\n",
    "import abc\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC\n",
    "\n",
    "\n",
    "# The abstract penalty function which has a function update\n",
    "class Penalty(ABC):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        self.lambda_lasso = lambda_lasso\n",
    "        self.weight_vec = weight_vec\n",
    "        self.Sigma = Sigma\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def update(self, new_u):\n",
    "        pass\n",
    "\n",
    "\n",
    "# The norm2 penalty function\n",
    "class Norm2Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(Norm2Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.limit = np.array(lambda_lasso * weight_vec)\n",
    "\n",
    "    def update(self, new_u):\n",
    "        normalized_u = np.where(np.linalg.norm(new_u, axis=1) >= self.limit)\n",
    "        new_u[normalized_u] = (new_u[normalized_u].T * self.limit[normalized_u] / np.linalg.norm(new_u[normalized_u], axis=1)).T\n",
    "        return new_u\n",
    "\n",
    "\n",
    "# The squared norm2 penalty function\n",
    "class SquaredNorm2Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(SquaredNorm2Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.normalize_factor = 1 + np.dot(2 * self.Sigma, 1/(self.lambda_lasso * self.weight_vec))\n",
    "\n",
    "    def update(self, new_u):\n",
    "        for i in range(new_u.shape[1]):\n",
    "            new_u[:, i] /= self.normalize_factor\n",
    "\n",
    "        return new_u\n",
    "    \n",
    "    \n",
    "\n",
    "# The MOCHA penalty function\n",
    "class MOCHAPelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(MOCHAPelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.normalize_factor = 1 + np.dot(self.Sigma, 1/(self.lambda_lasso * self.weight_vec))\n",
    "\n",
    "    def update(self, new_u):\n",
    "        for i in range(new_u.shape[1]):\n",
    "            new_u[:, i] /= self.normalize_factor\n",
    "\n",
    "        return new_u\n",
    "\n",
    "\n",
    "# The norm1 penalty function\n",
    "class Norm1Pelanty(Penalty):\n",
    "    def __init__(self, lambda_lasso, weight_vec, Sigma, n):\n",
    "        super(Norm1Pelanty, self).__init__(lambda_lasso, weight_vec, Sigma, n)\n",
    "        self.limit = np.array([np.zeros(n) for i in range(len(weight_vec))])\n",
    "        for i in range(n):\n",
    "            self.limit[:, i] = lambda_lasso * weight_vec\n",
    "\n",
    "    def update(self, new_u):\n",
    "        normalized_u = np.where(abs(new_u) >= self.limit)\n",
    "        new_u[normalized_u] = self.limit[normalized_u] * new_u[normalized_u] / abs(new_u[normalized_u])\n",
    "        return new_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Chain Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node $i \\in V$ represents a local dataset consisting of $m$ feature vectors $x^{(i,1)}, ... , x^{(i,m)} \\in R^n$. The feature vectors are i.i.d. realizations of a standard Gaussian random vector x ∼ N(0,I). The labels $y_1^{(i)}, . . . , y_m^{(i)} \\in R$ of the nodes $i \\in V$ are generated according to the linear model $y_r^{(i)} = (x^{(i, r)})^T w^{(i)} + \\epsilon$, with $\\epsilon ∼ N(0,\\sigma)$. To learn the weight $w^{(i)}$ ,we apply Algorithm 1 to a training set M obtained by randomly selecting 40% of the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from algorithm.optimizer import *\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import random\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "def KLdivergence(x, y):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two multivariate samples.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 2D array (n,d)\n",
    "    Samples from distribution P, which typically represents the true\n",
    "    distribution.\n",
    "    y : 2D array (m,d)\n",
    "    Samples from distribution Q, which typically represents the approximate\n",
    "    distribution.\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "    The estimated Kullback-Leibler divergence D(P||Q).\n",
    "    References\n",
    "    ----------\n",
    "    Pérez-Cruz, F. Kullback-Leibler divergence estimation of\n",
    "    continuous distributions IEEE International Symposium on Information\n",
    "    Theory, 2008.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the dimensions are consistent\n",
    "    x = np.atleast_2d(x)\n",
    "    y = np.atleast_2d(y)\n",
    "\n",
    "    n,d = x.shape\n",
    "    m,dy = y.shape\n",
    "\n",
    "    assert(d == dy)\n",
    "\n",
    "\n",
    "    # Build a KD tree representation of the samples and find the nearest neighbour\n",
    "    # of each point in x.\n",
    "    xtree = KDTree(x)\n",
    "    ytree = KDTree(y)\n",
    "\n",
    "    # Get the first two nearest neighbours for x, since the closest one is the\n",
    "    # sample itself.\n",
    "    r = xtree.query(x, k=2, eps=.01, p=2)[0][:,1]\n",
    "    s = ytree.query(x, k=1, eps=.01, p=2)[0]\n",
    "\n",
    "    # There is a mistake in the paper. In Eq. 14, the right side misses a negative sign\n",
    "    # on the first term of the right hand side.\n",
    "    return -np.log(r/s).sum() * d / n + np.log(m / (n - 1.))\n",
    "\n",
    "\n",
    "\n",
    "def get_images():\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "                       torchvision.transforms.ToTensor(),\n",
    "                     ])\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "\n",
    "    dl = DataLoader(mnist_dataset)\n",
    "\n",
    "    X = dl.dataset.data # (60000,28, 28)\n",
    "    y = dl.dataset.targets #(60000)\n",
    "\n",
    "    # normalize to have 0 ~ 1 range in each pixel\n",
    "\n",
    "    X = X / 255.0\n",
    "    \n",
    "    cluster_nums = [[0, 1], [2, 3]]\n",
    "    \n",
    "    cluster_data0 = np.concatenate((np.where(y==cluster_nums[0][0])[0], np.where(y==cluster_nums[0][1])[0]))\n",
    "    random.shuffle(cluster_data0)\n",
    "    \n",
    "    cluster_data1 = np.concatenate((np.where(y==cluster_nums[1][0])[0], np.where(y==cluster_nums[1][1])[0]))\n",
    "    random.shuffle(cluster_data1)\n",
    "    \n",
    "    cluster_data = [\n",
    "        cluster_data0,\n",
    "        cluster_data1,\n",
    "    ]\n",
    "    \n",
    "    all_cluster_data = np.concatenate((cluster_data0 , cluster_data1))\n",
    "    selected_X = X[all_cluster_data].reshape(-1, 28*28)\n",
    "    selected_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                            init='random').fit_transform(selected_X)\n",
    "    \n",
    "    all_embeded = np.zeros((len(X), 2))\n",
    "    all_embeded[all_cluster_data] = selected_embedded\n",
    "    return X, y, cluster_data, all_embeded\n",
    "\n",
    "\n",
    "from scipy.spatial import cKDTree as KDTree\n",
    "\n",
    "def KLdivergence(x, y):\n",
    "    \"\"\"Compute the Kullback-Leibler divergence between two multivariate samples.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 2D array (n,d)\n",
    "    Samples from distribution P, which typically represents the true\n",
    "    distribution.\n",
    "    y : 2D array (m,d)\n",
    "    Samples from distribution Q, which typically represents the approximate\n",
    "    distribution.\n",
    "    Returns\n",
    "    -------\n",
    "    out : float\n",
    "    The estimated Kullback-Leibler divergence D(P||Q).\n",
    "    References\n",
    "    ----------\n",
    "    Pérez-Cruz, F. Kullback-Leibler divergence estimation of\n",
    "    continuous distributions IEEE International Symposium on Information\n",
    "    Theory, 2008.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check the dimensions are consistent\n",
    "    x = np.atleast_2d(x)\n",
    "    y = np.atleast_2d(y)\n",
    "\n",
    "    n,d = x.shape\n",
    "    m,dy = y.shape\n",
    "\n",
    "    assert(d == dy)\n",
    "\n",
    "\n",
    "    # Build a KD tree representation of the samples and find the nearest neighbour\n",
    "    # of each point in x.\n",
    "    xtree = KDTree(x)\n",
    "    ytree = KDTree(y)\n",
    "\n",
    "    # Get the first two nearest neighbours for x, since the closest one is the\n",
    "    # sample itself.\n",
    "    r = xtree.query(x, k=2, eps=.01, p=2)[0][:,1]\n",
    "    s = ytree.query(x, k=1, eps=.01, p=2)[0]\n",
    "\n",
    "    # There is a mistake in the paper. In Eq. 14, the right side misses a negative sign\n",
    "    # on the first term of the right hand side.\n",
    "    return -np.log(r/s).sum() * d / n + np.log(m / (n - 1.))\n",
    "\n",
    "\n",
    "def get_chain_data(cluster_sizes, eps, X, y, cluster_data, all_embeded, h1=200, num_neig=3):\n",
    "    '''\n",
    "    :param W: a list containing the weight vectors for each cluster\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "    cluster_nums = [[0, 1], [2, 3]]\n",
    "\n",
    "    N = np.sum(cluster_sizes)\n",
    "    E = N-1\n",
    "    '''\n",
    "    N: total number of nodes\n",
    "    E: total number of edges\n",
    "    '''\n",
    "    \n",
    "    feature_size = int(10000//cluster_sizes[0])\n",
    "    train_size = int(feature_size*0.8)\n",
    "    \n",
    "    # create the data of each node needed for the algorithm 1 \n",
    "    node_degrees = np.full(N, 1.0/num_neig)\n",
    "#     np.array((1.0 / (np.sum(abs(B), 0)))).ravel()\n",
    "    '''\n",
    "    node_degrees: a list containing the nodes degree for the alg1 (1/N_i)\n",
    "    '''\n",
    "    \n",
    "    datapoints = {}\n",
    "    '''\n",
    "    datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1,\n",
    "    which are features, label, degree, and also the optimizer model for each node\n",
    "    '''\n",
    "    true_labels = []\n",
    "    '''\n",
    "    true_labels: the true labels for the nodes of the graph\n",
    "    '''\n",
    "    \n",
    "    embeded_features = []\n",
    "    cnt = 0\n",
    "    for i, cluster_size in enumerate(cluster_sizes):\n",
    "        for j in range(cluster_size):\n",
    "            features = X[cluster_data[i][j*feature_size:(j+1)*feature_size]][:train_size]\n",
    "            res = all_embeded[cluster_data[i][j*feature_size:(j+1)*feature_size]][:train_size]\n",
    "            res = np.sort(res, axis=0)\n",
    "            embeded_features.append(res)\n",
    "            test_features = X[cluster_data[i][j*feature_size:(j+1)*feature_size]][train_size:]\n",
    "            '''\n",
    "            features: the feature vector of node i \n",
    "            '''\n",
    "            label = y[cluster_data[i][j*feature_size:(j+1)*feature_size]][:train_size]\n",
    "            test_label = y[cluster_data[i][j*feature_size:(j+1)*feature_size]][train_size:]\n",
    "            \n",
    "            label[label == cluster_nums[i][0]] = 0\n",
    "            label[label == cluster_nums[i][1]] = 1\n",
    "            \n",
    "            test_label[test_label == cluster_nums[i][0]] = 0\n",
    "            test_label[test_label == cluster_nums[i][1]] = 1\n",
    "            '''\n",
    "            label: the label of the node i\n",
    "            '''\n",
    "            \n",
    "            true_labels.append(label)\n",
    "\n",
    "            model = SimpleLinear(h1=h1) \n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "            optimizer = TorchLinearOptimizer(model, criterion)\n",
    "#             optimizer = TorchLogisticOptimizer(model)\n",
    "#             features = Variable(torch.from_numpy(features)).to(torch.float32)\n",
    "#             label = Variable(torch.from_numpy(label)).to(torch.float32) \n",
    "      \n",
    "            '''\n",
    "            model : the linear model for the node i \n",
    "            optimizer : the optimizer model for the node i \n",
    "            ''' \n",
    "            \n",
    "            datapoints[cnt] = {\n",
    "                'features': features,\n",
    "                'test_features': test_features,\n",
    "                'degree': node_degrees[cnt],\n",
    "                'label': label,\n",
    "                'test_label': test_label,\n",
    "                'optimizer': optimizer\n",
    "            }\n",
    "            cnt += 1\n",
    "            \n",
    "    \n",
    "    # create B(adjacency matrix) and edges's weights vector(weight_vec) based on the graph G\n",
    "    E = N*num_neig\n",
    "    B = np.zeros((E, N))\n",
    "    '''\n",
    "    B: adjacency matrix of the graph with the shape of E*N\n",
    "    '''\n",
    "    weight_vec = np.zeros(E)\n",
    "    '''\n",
    "    weight_vec: a list containing the edges's weights of the graph with the shape of E\n",
    "    '''\n",
    "    cnt = 0\n",
    "    for i in range(N):\n",
    "        dists = []\n",
    "        for j in range(N):\n",
    "            dists.append(KLdivergence(embeded_features[i], embeded_features[j]))\n",
    "        node_dists = np.argsort(dists)\n",
    "        for k in range(num_neig):\n",
    "            neig = node_dists[k+1]\n",
    "            B[cnt, i] = 1\n",
    "            B[cnt, neig] = -1\n",
    "            weight_vec[cnt] = np.exp(-dists[neig])\n",
    "            cnt += 1\n",
    "\n",
    "    weight_vec = (weight_vec - np.min(weight_vec)) / (np.max(weight_vec) - np.min(weight_vec))\n",
    "    \n",
    "\n",
    "    return B, weight_vec, np.array(true_labels), datapoints\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the result we compare the MSE of Algorithm 1 with plain linear regression \n",
    "and decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load results/compare_results.py\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def get_algorithm1_error(datapoints, predicted_w, samplingset):\n",
    "    '''\n",
    "    :param datapoints:  a dictionary containing the data of each node in the graph needed for the algorithm 1\n",
    "    :param predicted_w: the predicted weigh vectors for each node\n",
    "    :param samplingset: the sampling set for the algorithm 1\n",
    "\n",
    "    :return alg1_MSE: the MSE of the algorithm 1 for all the nodes, the samplingset and other nodes (test set)\n",
    "    '''\n",
    "    \n",
    "    true_labels = []\n",
    "    test_true_labels = []\n",
    "    pred_labels = []\n",
    "    test_pred_labels = []\n",
    "    for i in range(len(datapoints)):\n",
    "        features = datapoints[i]['features']\n",
    "        label = datapoints[i]['label'].ravel().detach().numpy()\n",
    "        true_labels.append(label)\n",
    "        \n",
    "        pred_label = datapoints[i]['optimizer'].model(features)\n",
    "        pred_label = torch.argmax(pred_label, axis=1).ravel().detach().numpy() \n",
    "        pred_labels.append(pred_label)\n",
    "        \n",
    "        \n",
    "        test_features = datapoints[i]['test_features']\n",
    "        test_label = datapoints[i]['test_label'].ravel().detach().numpy()\n",
    "        test_true_labels.append(test_label)\n",
    "        \n",
    "        test_pred_label = datapoints[i]['optimizer'].model(test_features)\n",
    "        test_pred_label = torch.argmax(test_pred_label, axis=1).ravel().detach().numpy() \n",
    "        test_pred_labels.append(test_pred_label)\n",
    "\n",
    "    pred_labels = np.array(pred_labels).ravel()\n",
    "    true_labels = np.array(true_labels).ravel()\n",
    "    \n",
    "    test_pred_labels = np.array(test_pred_labels).ravel()\n",
    "    test_true_labels = np.array(test_true_labels).ravel()\n",
    "    \n",
    "    all_labels = np.concatenate((true_labels, test_true_labels))\n",
    "    all_pred_labels = np.concatenate((pred_labels, test_pred_labels))\n",
    "    \n",
    "    alg1_MSE = {'total': len(np.where(all_labels == all_pred_labels)[0]) / len(all_pred_labels),\n",
    "                'train': len(np.where(true_labels == pred_labels)[0]) / len(pred_labels),\n",
    "                'test': len(np.where(test_true_labels == test_pred_labels)[0]) / len(test_pred_labels),\n",
    "               }\n",
    "\n",
    "    return alg1_MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain graph with Two Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chain graph has two clusters $|C_1| = |C_2| = 100$.\n",
    "Each node $i \\in V$ represents a local dataset consisting of feature vectors $x^{(i,1)}, ... , x^{(i,5)} \\in R^2$.\n",
    "The feature vectors are i.i.d. realizations of a standard Gaussian random vector x ~ N(0,I).\n",
    "The labels $y_1^{(i)}, . . . , y_5^{(i)} \\in R$ for each node $i \\in V$\n",
    "are selected from the MNIST dataset in away that we have two clusters $C_1$ and $C_2$ and for the first cluster we selected the images accosiated with number 0 and 1 and for the other cluster we selected the images accosiated with number 2 and 3 and we splited the selected dataset for each cluster randomly to each node within that cluster, and the model for each node is a simple 2 layers NN.\n",
    " \n",
    "The tuning parameter $\\lambda$ in algorithm1 \n",
    "is manually chosen, guided by the resulting MSE, as $\\lambda=0.01$ for norm1 and norm2 and also $\\lambda=0.05$ for mocha penalty function. \n",
    "To learn the weight $w^{(i)}$ ,we apply Algorithm 1 to a training set M obtained by randomly selecting 90% of the nodes and use the rest as test set. As the result we ploted the mean accuracy achived by each choise of $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# from sparsebm import generate_SBM_dataset\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_chain_graph_data(X, y, cluster_data, all_embeded, eps=0.1, h1=200, num_neig=3):\n",
    "    '''\n",
    "    :param m, n: shape of features vector for each node\n",
    "    :param pin: the probability of edges inside each cluster\n",
    "    :param pout: the probability of edges between the clusters\n",
    "    :param noise_sd: the standard deviation of the noise for calculating the labels\n",
    "    \n",
    "    :return B: adjacency matrix of the graph\n",
    "    :return weight_vec: a list containing the edges's weights of the graph\n",
    "    :return true_labels: a list containing the true labels of the nodes\n",
    "    :return datapoints: a dictionary containing the data of each node in the graph needed for the algorithm 1 \n",
    "    '''\n",
    "    cluster_sizes = [100, 100]\n",
    "    cluster_sizes = [20, 20]\n",
    "    \n",
    "    return get_chain_data(cluster_sizes, eps, X, y, cluster_data, all_embeded, h1, num_neig)\n",
    "\n",
    "\n",
    "X, y, cluster_data, all_embeded = get_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the MSE with respect to the different epsilons (0.1, 0.5, 0.8) for each penalty function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_lasso: 0.0 , {'total': 0.90305, 'train': 0.906375, 'test': 0.88975}\n",
      "\n",
      "lambda_lasso: 0.1 , {'total': 0.9937, 'train': 0.99925, 'test': 0.9715}\n",
      "\n",
      "lambda_lasso: 0.5 , {'total': 0.9938, 'train': 0.9994375, 'test': 0.97125}\n",
      "\n",
      "lambda_lasso: 1.0 , {'total': 0.9941, 'train': 0.999375, 'test': 0.973}\n",
      "\n",
      "lambda_lasso: 10000.0 , {'total': 0.9251, 'train': 0.9278125, 'test': 0.91425}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penalty_func = 'norm2'\n",
    "lambda_scores = {}\n",
    "for lambda_lasso in [0.0, 0.1, 0.5, 1.0, 10000.0]:\n",
    "    B, weight_vec, true_labels, datapoints = get_chain_graph_data(X, y, cluster_data, all_embeded, \n",
    "                                                                  h1=1, num_neig=4)\n",
    "    E, N = B.shape\n",
    "    h1 = 1\n",
    "    samplingset = [i for i in range(N)]\n",
    "    iter_scores, predicted_w, predicted_u = algorithm_1(70, B, weight_vec, datapoints, true_labels, samplingset, \n",
    "                             lambda_lasso, penalty_func, calculate_score=True)\n",
    "    alg1_score = get_algorithm1_error(datapoints, predicted_w, samplingset)\n",
    "    lambda_scores[lambda_lasso] = iter_scores\n",
    "    print('lambda_lasso:', lambda_lasso, ',', alg1_score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x  = [i for i in range(70)]\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure()\n",
    "\n",
    "for treshold in [0.0, 1.0, 10000.0]:\n",
    "    scores = [item['test'] for item in lambda_scores[treshold]]\n",
    "    x = [i for i in range(len(scores))]\n",
    "    plt.plot(x, scores, label='lambda=' + str(treshold))\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# f.savefig(\"experiments/data/MNIST/treshhhold_graph.pdf\", bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 500\n",
    "n_epochs = 1000\n",
    "\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "\n",
    "# The SimpleLinear which is for MNIST experiment\n",
    "class SimpleLinear4(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(28*28, h1)\n",
    "        self.linear2 = torch.nn.Linear(h1, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "# The SimpleLinear which is for MNIST experiment\n",
    "class Simple4(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(28*28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.linear1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "transforms = torchvision.transforms.Compose([\n",
    "                   torchvision.transforms.ToTensor(),\n",
    "                 ])\n",
    "mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "\n",
    "dl = DataLoader(mnist_dataset)\n",
    "\n",
    "X = dl.dataset.data # (60000,28, 28)\n",
    "y = dl.dataset.targets #(60000)\n",
    "\n",
    "# normalize to have 0 ~ 1 range in each pixel\n",
    "\n",
    "X = X / 255.0\n",
    "\n",
    "cluster_nums = [[0, 1], [2, 3]]\n",
    "\n",
    "cluster_data0 = np.array([])\n",
    "for cluster_num in cluster_nums[0]:\n",
    "    cluster_data0 = np.concatenate((cluster_data0, np.where(y==cluster_num)[0]))\n",
    "cluster_data0 = np.array(cluster_data0)\n",
    "random.shuffle(cluster_data0)\n",
    "\n",
    "cluster_data1 = []\n",
    "for cluster_num in cluster_nums[1]:\n",
    "    cluster_data1 = np.concatenate((cluster_data1, np.where(y==cluster_num)[0]))\n",
    "cluster_data1 = np.array(cluster_data1)\n",
    "random.shuffle(cluster_data1)\n",
    "cluster_data = np.concatenate((cluster_data0, cluster_data1))\n",
    "# cluster_data = cluster_data0\n",
    "random.shuffle(cluster_data)\n",
    "\n",
    "features = X[cluster_data]\n",
    "labels = y[cluster_data]\n",
    "print(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnt = len(cluster_data)\n",
    "train_data = []\n",
    "test_data = []\n",
    "for i in range(all_cnt):\n",
    "    if i < all_cnt*0.8:\n",
    "        train_data.append([features[i], labels[i]])\n",
    "    else:\n",
    "        test_data.append([features[i], labels[i]])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size_train)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 100 [0/19804 (0%)]\tLoss: 1.503993\n",
      "Train Epoch: 100 [640/19804 (3%)]\tLoss: 1.491372\n",
      "Train Epoch: 100 [1280/19804 (6%)]\tLoss: 1.312555\n",
      "Train Epoch: 100 [1920/19804 (10%)]\tLoss: 1.376291\n",
      "Train Epoch: 100 [2560/19804 (13%)]\tLoss: 1.222917\n",
      "Train Epoch: 100 [3200/19804 (16%)]\tLoss: 1.268267\n",
      "Train Epoch: 100 [3840/19804 (19%)]\tLoss: 1.120801\n",
      "Train Epoch: 100 [4480/19804 (23%)]\tLoss: 1.192205\n",
      "Train Epoch: 100 [5120/19804 (26%)]\tLoss: 1.301904\n",
      "Train Epoch: 100 [5760/19804 (29%)]\tLoss: 1.162326\n",
      "Train Epoch: 100 [6400/19804 (32%)]\tLoss: 1.188491\n",
      "Train Epoch: 100 [7040/19804 (35%)]\tLoss: 1.165867\n",
      "Train Epoch: 100 [7680/19804 (39%)]\tLoss: 1.151610\n",
      "Train Epoch: 100 [8320/19804 (42%)]\tLoss: 1.098512\n",
      "Train Epoch: 100 [8960/19804 (45%)]\tLoss: 1.074687\n",
      "Train Epoch: 100 [9600/19804 (48%)]\tLoss: 1.122294\n",
      "Train Epoch: 100 [10240/19804 (52%)]\tLoss: 1.215856\n",
      "Train Epoch: 100 [10880/19804 (55%)]\tLoss: 1.060493\n",
      "Train Epoch: 100 [11520/19804 (58%)]\tLoss: 1.163561\n",
      "Train Epoch: 100 [12160/19804 (61%)]\tLoss: 1.223127\n",
      "Train Epoch: 100 [12800/19804 (65%)]\tLoss: 1.117330\n",
      "Train Epoch: 100 [13440/19804 (68%)]\tLoss: 0.884223\n",
      "Train Epoch: 100 [14080/19804 (71%)]\tLoss: 1.159018\n",
      "Train Epoch: 100 [14720/19804 (74%)]\tLoss: 1.042334\n",
      "Train Epoch: 100 [15360/19804 (77%)]\tLoss: 1.109804\n",
      "Train Epoch: 100 [16000/19804 (81%)]\tLoss: 1.078976\n",
      "Train Epoch: 100 [16640/19804 (84%)]\tLoss: 1.072471\n",
      "Train Epoch: 100 [17280/19804 (87%)]\tLoss: 1.112558\n",
      "Train Epoch: 100 [17920/19804 (90%)]\tLoss: 1.110315\n",
      "Train Epoch: 100 [18560/19804 (94%)]\tLoss: 1.110665\n",
      "Train Epoch: 100 [19200/19804 (97%)]\tLoss: 1.062134\n",
      "\n",
      "Test set: Avg. loss: 1.0355, Accuracy: 2288/4950 (46%)\n",
      "\n",
      "Train Epoch: 200 [0/19804 (0%)]\tLoss: 1.073457\n",
      "Train Epoch: 200 [640/19804 (3%)]\tLoss: 1.073390\n",
      "Train Epoch: 200 [1280/19804 (6%)]\tLoss: 1.039471\n",
      "Train Epoch: 200 [1920/19804 (10%)]\tLoss: 1.121021\n",
      "Train Epoch: 200 [2560/19804 (13%)]\tLoss: 0.945016\n",
      "Train Epoch: 200 [3200/19804 (16%)]\tLoss: 0.970653\n",
      "Train Epoch: 200 [3840/19804 (19%)]\tLoss: 0.917459\n",
      "Train Epoch: 200 [4480/19804 (23%)]\tLoss: 1.082778\n",
      "Train Epoch: 200 [5120/19804 (26%)]\tLoss: 0.995578\n",
      "Train Epoch: 200 [5760/19804 (29%)]\tLoss: 1.087781\n",
      "Train Epoch: 200 [6400/19804 (32%)]\tLoss: 1.027923\n",
      "Train Epoch: 200 [7040/19804 (35%)]\tLoss: 1.063043\n",
      "Train Epoch: 200 [7680/19804 (39%)]\tLoss: 0.935804\n",
      "Train Epoch: 200 [8320/19804 (42%)]\tLoss: 0.888125\n",
      "Train Epoch: 200 [8960/19804 (45%)]\tLoss: 1.022428\n",
      "Train Epoch: 200 [9600/19804 (48%)]\tLoss: 1.106391\n",
      "Train Epoch: 200 [10240/19804 (52%)]\tLoss: 0.892487\n",
      "Train Epoch: 200 [10880/19804 (55%)]\tLoss: 0.958864\n",
      "Train Epoch: 200 [11520/19804 (58%)]\tLoss: 1.050884\n",
      "Train Epoch: 200 [12160/19804 (61%)]\tLoss: 1.007241\n",
      "Train Epoch: 200 [12800/19804 (65%)]\tLoss: 0.876795\n",
      "Train Epoch: 200 [13440/19804 (68%)]\tLoss: 0.930618\n",
      "Train Epoch: 200 [14080/19804 (71%)]\tLoss: 0.907177\n",
      "Train Epoch: 200 [14720/19804 (74%)]\tLoss: 1.012500\n",
      "Train Epoch: 200 [15360/19804 (77%)]\tLoss: 1.038696\n",
      "Train Epoch: 200 [16000/19804 (81%)]\tLoss: 0.847826\n",
      "Train Epoch: 200 [16640/19804 (84%)]\tLoss: 1.074547\n",
      "Train Epoch: 200 [17280/19804 (87%)]\tLoss: 1.005789\n",
      "Train Epoch: 200 [17920/19804 (90%)]\tLoss: 1.026287\n",
      "Train Epoch: 200 [18560/19804 (94%)]\tLoss: 0.937797\n",
      "Train Epoch: 200 [19200/19804 (97%)]\tLoss: 0.953369\n",
      "\n",
      "Test set: Avg. loss: 0.9719, Accuracy: 2482/4950 (50%)\n",
      "\n",
      "Train Epoch: 300 [0/19804 (0%)]\tLoss: 0.969876\n",
      "Train Epoch: 300 [640/19804 (3%)]\tLoss: 0.943490\n",
      "Train Epoch: 300 [1280/19804 (6%)]\tLoss: 1.033068\n",
      "Train Epoch: 300 [1920/19804 (10%)]\tLoss: 1.009954\n",
      "Train Epoch: 300 [2560/19804 (13%)]\tLoss: 0.996262\n",
      "Train Epoch: 300 [3200/19804 (16%)]\tLoss: 0.943727\n",
      "Train Epoch: 300 [3840/19804 (19%)]\tLoss: 0.884238\n",
      "Train Epoch: 300 [4480/19804 (23%)]\tLoss: 1.027211\n",
      "Train Epoch: 300 [5120/19804 (26%)]\tLoss: 0.910490\n",
      "Train Epoch: 300 [5760/19804 (29%)]\tLoss: 0.962446\n",
      "Train Epoch: 300 [6400/19804 (32%)]\tLoss: 0.916252\n",
      "Train Epoch: 300 [7040/19804 (35%)]\tLoss: 0.957032\n",
      "Train Epoch: 300 [7680/19804 (39%)]\tLoss: 1.048021\n",
      "Train Epoch: 300 [8320/19804 (42%)]\tLoss: 0.947678\n",
      "Train Epoch: 300 [8960/19804 (45%)]\tLoss: 0.963588\n",
      "Train Epoch: 300 [9600/19804 (48%)]\tLoss: 0.949050\n",
      "Train Epoch: 300 [10240/19804 (52%)]\tLoss: 0.986272\n",
      "Train Epoch: 300 [10880/19804 (55%)]\tLoss: 1.051980\n",
      "Train Epoch: 300 [11520/19804 (58%)]\tLoss: 1.005498\n",
      "Train Epoch: 300 [12160/19804 (61%)]\tLoss: 0.911032\n",
      "Train Epoch: 300 [12800/19804 (65%)]\tLoss: 0.883041\n",
      "Train Epoch: 300 [13440/19804 (68%)]\tLoss: 0.929981\n",
      "Train Epoch: 300 [14080/19804 (71%)]\tLoss: 0.989636\n",
      "Train Epoch: 300 [14720/19804 (74%)]\tLoss: 1.046085\n",
      "Train Epoch: 300 [15360/19804 (77%)]\tLoss: 1.106891\n",
      "Train Epoch: 300 [16000/19804 (81%)]\tLoss: 0.979510\n",
      "Train Epoch: 300 [16640/19804 (84%)]\tLoss: 0.993112\n",
      "Train Epoch: 300 [17280/19804 (87%)]\tLoss: 1.016214\n",
      "Train Epoch: 300 [17920/19804 (90%)]\tLoss: 0.978933\n",
      "Train Epoch: 300 [18560/19804 (94%)]\tLoss: 0.934565\n",
      "Train Epoch: 300 [19200/19804 (97%)]\tLoss: 1.014845\n",
      "\n",
      "Test set: Avg. loss: 0.9491, Accuracy: 2483/4950 (50%)\n",
      "\n",
      "Train Epoch: 400 [0/19804 (0%)]\tLoss: 0.868722\n",
      "Train Epoch: 400 [640/19804 (3%)]\tLoss: 0.976636\n",
      "Train Epoch: 400 [1280/19804 (6%)]\tLoss: 1.006460\n",
      "Train Epoch: 400 [1920/19804 (10%)]\tLoss: 0.926272\n",
      "Train Epoch: 400 [2560/19804 (13%)]\tLoss: 0.975587\n",
      "Train Epoch: 400 [3200/19804 (16%)]\tLoss: 1.014653\n",
      "Train Epoch: 400 [3840/19804 (19%)]\tLoss: 0.884348\n",
      "Train Epoch: 400 [4480/19804 (23%)]\tLoss: 0.939390\n",
      "Train Epoch: 400 [5120/19804 (26%)]\tLoss: 0.885193\n",
      "Train Epoch: 400 [5760/19804 (29%)]\tLoss: 0.852143\n",
      "Train Epoch: 400 [6400/19804 (32%)]\tLoss: 1.062338\n",
      "Train Epoch: 400 [7040/19804 (35%)]\tLoss: 0.900204\n",
      "Train Epoch: 400 [7680/19804 (39%)]\tLoss: 0.950514\n",
      "Train Epoch: 400 [8320/19804 (42%)]\tLoss: 0.978352\n",
      "Train Epoch: 400 [8960/19804 (45%)]\tLoss: 0.970648\n",
      "Train Epoch: 400 [9600/19804 (48%)]\tLoss: 0.868378\n",
      "Train Epoch: 400 [10240/19804 (52%)]\tLoss: 0.833815\n",
      "Train Epoch: 400 [10880/19804 (55%)]\tLoss: 0.990449\n",
      "Train Epoch: 400 [11520/19804 (58%)]\tLoss: 0.948463\n",
      "Train Epoch: 400 [12160/19804 (61%)]\tLoss: 0.884114\n",
      "Train Epoch: 400 [12800/19804 (65%)]\tLoss: 1.050454\n",
      "Train Epoch: 400 [13440/19804 (68%)]\tLoss: 0.993636\n",
      "Train Epoch: 400 [14080/19804 (71%)]\tLoss: 0.939449\n",
      "Train Epoch: 400 [14720/19804 (74%)]\tLoss: 0.874044\n",
      "Train Epoch: 400 [15360/19804 (77%)]\tLoss: 0.869861\n",
      "Train Epoch: 400 [16000/19804 (81%)]\tLoss: 1.027457\n",
      "Train Epoch: 400 [16640/19804 (84%)]\tLoss: 0.955378\n",
      "Train Epoch: 400 [17280/19804 (87%)]\tLoss: 0.982672\n",
      "Train Epoch: 400 [17920/19804 (90%)]\tLoss: 0.859183\n",
      "Train Epoch: 400 [18560/19804 (94%)]\tLoss: 0.885658\n",
      "Train Epoch: 400 [19200/19804 (97%)]\tLoss: 0.952806\n",
      "\n",
      "Test set: Avg. loss: 0.9354, Accuracy: 2518/4950 (51%)\n",
      "\n",
      "Train Epoch: 500 [0/19804 (0%)]\tLoss: 0.939088\n",
      "Train Epoch: 500 [640/19804 (3%)]\tLoss: 1.047353\n",
      "Train Epoch: 500 [1280/19804 (6%)]\tLoss: 0.994745\n",
      "Train Epoch: 500 [1920/19804 (10%)]\tLoss: 1.000286\n",
      "Train Epoch: 500 [2560/19804 (13%)]\tLoss: 0.921327\n",
      "Train Epoch: 500 [3200/19804 (16%)]\tLoss: 0.944064\n",
      "Train Epoch: 500 [3840/19804 (19%)]\tLoss: 0.783670\n",
      "Train Epoch: 500 [4480/19804 (23%)]\tLoss: 0.962846\n",
      "Train Epoch: 500 [5120/19804 (26%)]\tLoss: 0.904937\n",
      "Train Epoch: 500 [5760/19804 (29%)]\tLoss: 0.921727\n",
      "Train Epoch: 500 [6400/19804 (32%)]\tLoss: 0.888685\n",
      "Train Epoch: 500 [7040/19804 (35%)]\tLoss: 0.908548\n",
      "Train Epoch: 500 [7680/19804 (39%)]\tLoss: 0.914342\n",
      "Train Epoch: 500 [8320/19804 (42%)]\tLoss: 0.895238\n",
      "Train Epoch: 500 [8960/19804 (45%)]\tLoss: 0.943581\n",
      "Train Epoch: 500 [9600/19804 (48%)]\tLoss: 1.020714\n",
      "Train Epoch: 500 [10240/19804 (52%)]\tLoss: 1.066576\n",
      "Train Epoch: 500 [10880/19804 (55%)]\tLoss: 0.773959\n",
      "Train Epoch: 500 [11520/19804 (58%)]\tLoss: 1.044004\n",
      "Train Epoch: 500 [12160/19804 (61%)]\tLoss: 0.939599\n",
      "Train Epoch: 500 [12800/19804 (65%)]\tLoss: 0.839880\n",
      "Train Epoch: 500 [13440/19804 (68%)]\tLoss: 1.007985\n",
      "Train Epoch: 500 [14080/19804 (71%)]\tLoss: 1.012341\n",
      "Train Epoch: 500 [14720/19804 (74%)]\tLoss: 0.894580\n",
      "Train Epoch: 500 [15360/19804 (77%)]\tLoss: 0.896083\n",
      "Train Epoch: 500 [16000/19804 (81%)]\tLoss: 0.915208\n",
      "Train Epoch: 500 [16640/19804 (84%)]\tLoss: 0.870333\n",
      "Train Epoch: 500 [17280/19804 (87%)]\tLoss: 0.866826\n",
      "Train Epoch: 500 [17920/19804 (90%)]\tLoss: 0.941893\n",
      "Train Epoch: 500 [18560/19804 (94%)]\tLoss: 0.793144\n",
      "Train Epoch: 500 [19200/19804 (97%)]\tLoss: 0.879692\n",
      "\n",
      "Test set: Avg. loss: 0.9269, Accuracy: 2527/4950 (51%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 600 [0/19804 (0%)]\tLoss: 0.795480\n",
      "Train Epoch: 600 [640/19804 (3%)]\tLoss: 0.898538\n",
      "Train Epoch: 600 [1280/19804 (6%)]\tLoss: 0.911106\n",
      "Train Epoch: 600 [1920/19804 (10%)]\tLoss: 0.829501\n",
      "Train Epoch: 600 [2560/19804 (13%)]\tLoss: 0.912030\n",
      "Train Epoch: 600 [3200/19804 (16%)]\tLoss: 0.824499\n",
      "Train Epoch: 600 [3840/19804 (19%)]\tLoss: 0.943429\n",
      "Train Epoch: 600 [4480/19804 (23%)]\tLoss: 0.831233\n",
      "Train Epoch: 600 [5120/19804 (26%)]\tLoss: 0.662155\n",
      "Train Epoch: 600 [5760/19804 (29%)]\tLoss: 0.937714\n",
      "Train Epoch: 600 [6400/19804 (32%)]\tLoss: 1.000555\n",
      "Train Epoch: 600 [7040/19804 (35%)]\tLoss: 0.911641\n",
      "Train Epoch: 600 [7680/19804 (39%)]\tLoss: 0.881441\n",
      "Train Epoch: 600 [8320/19804 (42%)]\tLoss: 0.972729\n",
      "Train Epoch: 600 [8960/19804 (45%)]\tLoss: 0.959499\n",
      "Train Epoch: 600 [9600/19804 (48%)]\tLoss: 0.957285\n",
      "Train Epoch: 600 [10240/19804 (52%)]\tLoss: 0.860109\n",
      "Train Epoch: 600 [10880/19804 (55%)]\tLoss: 1.101224\n",
      "Train Epoch: 600 [11520/19804 (58%)]\tLoss: 0.897294\n",
      "Train Epoch: 600 [12160/19804 (61%)]\tLoss: 0.917026\n",
      "Train Epoch: 600 [12800/19804 (65%)]\tLoss: 0.839451\n",
      "Train Epoch: 600 [13440/19804 (68%)]\tLoss: 0.883082\n",
      "Train Epoch: 600 [14080/19804 (71%)]\tLoss: 0.981382\n",
      "Train Epoch: 600 [14720/19804 (74%)]\tLoss: 0.915612\n",
      "Train Epoch: 600 [15360/19804 (77%)]\tLoss: 0.964903\n",
      "Train Epoch: 600 [16000/19804 (81%)]\tLoss: 0.885380\n",
      "Train Epoch: 600 [16640/19804 (84%)]\tLoss: 0.950898\n",
      "Train Epoch: 600 [17280/19804 (87%)]\tLoss: 0.964652\n",
      "Train Epoch: 600 [17920/19804 (90%)]\tLoss: 0.875036\n",
      "Train Epoch: 600 [18560/19804 (94%)]\tLoss: 0.986966\n",
      "Train Epoch: 600 [19200/19804 (97%)]\tLoss: 0.869649\n",
      "\n",
      "Test set: Avg. loss: 0.9202, Accuracy: 2556/4950 (52%)\n",
      "\n",
      "Train Epoch: 700 [0/19804 (0%)]\tLoss: 0.874496\n",
      "Train Epoch: 700 [640/19804 (3%)]\tLoss: 0.931736\n",
      "Train Epoch: 700 [1280/19804 (6%)]\tLoss: 0.902911\n",
      "Train Epoch: 700 [1920/19804 (10%)]\tLoss: 0.944089\n",
      "Train Epoch: 700 [2560/19804 (13%)]\tLoss: 0.927755\n",
      "Train Epoch: 700 [3200/19804 (16%)]\tLoss: 0.964365\n",
      "Train Epoch: 700 [3840/19804 (19%)]\tLoss: 0.866025\n",
      "Train Epoch: 700 [4480/19804 (23%)]\tLoss: 0.905745\n",
      "Train Epoch: 700 [5120/19804 (26%)]\tLoss: 0.803389\n",
      "Train Epoch: 700 [5760/19804 (29%)]\tLoss: 0.929293\n",
      "Train Epoch: 700 [6400/19804 (32%)]\tLoss: 1.060650\n",
      "Train Epoch: 700 [7040/19804 (35%)]\tLoss: 1.035590\n",
      "Train Epoch: 700 [7680/19804 (39%)]\tLoss: 0.865064\n",
      "Train Epoch: 700 [8320/19804 (42%)]\tLoss: 0.932418\n",
      "Train Epoch: 700 [8960/19804 (45%)]\tLoss: 0.936208\n",
      "Train Epoch: 700 [9600/19804 (48%)]\tLoss: 0.930313\n",
      "Train Epoch: 700 [10240/19804 (52%)]\tLoss: 0.878603\n",
      "Train Epoch: 700 [10880/19804 (55%)]\tLoss: 0.920639\n",
      "Train Epoch: 700 [11520/19804 (58%)]\tLoss: 0.932293\n",
      "Train Epoch: 700 [12160/19804 (61%)]\tLoss: 1.003021\n",
      "Train Epoch: 700 [12800/19804 (65%)]\tLoss: 1.006435\n",
      "Train Epoch: 700 [13440/19804 (68%)]\tLoss: 0.973633\n",
      "Train Epoch: 700 [14080/19804 (71%)]\tLoss: 0.908823\n",
      "Train Epoch: 700 [14720/19804 (74%)]\tLoss: 0.859430\n",
      "Train Epoch: 700 [15360/19804 (77%)]\tLoss: 0.927038\n",
      "Train Epoch: 700 [16000/19804 (81%)]\tLoss: 0.947878\n",
      "Train Epoch: 700 [16640/19804 (84%)]\tLoss: 1.162261\n",
      "Train Epoch: 700 [17280/19804 (87%)]\tLoss: 0.880373\n",
      "Train Epoch: 700 [17920/19804 (90%)]\tLoss: 0.822470\n",
      "Train Epoch: 700 [18560/19804 (94%)]\tLoss: 0.909188\n",
      "Train Epoch: 700 [19200/19804 (97%)]\tLoss: 0.919258\n",
      "\n",
      "Test set: Avg. loss: 0.9135, Accuracy: 2626/4950 (53%)\n",
      "\n",
      "Train Epoch: 800 [0/19804 (0%)]\tLoss: 0.829623\n",
      "Train Epoch: 800 [640/19804 (3%)]\tLoss: 0.938847\n",
      "Train Epoch: 800 [1280/19804 (6%)]\tLoss: 0.883319\n",
      "Train Epoch: 800 [1920/19804 (10%)]\tLoss: 0.954374\n",
      "Train Epoch: 800 [2560/19804 (13%)]\tLoss: 0.888318\n",
      "Train Epoch: 800 [3200/19804 (16%)]\tLoss: 0.911923\n",
      "Train Epoch: 800 [3840/19804 (19%)]\tLoss: 0.953478\n",
      "Train Epoch: 800 [4480/19804 (23%)]\tLoss: 0.891036\n",
      "Train Epoch: 800 [5120/19804 (26%)]\tLoss: 0.800100\n",
      "Train Epoch: 800 [5760/19804 (29%)]\tLoss: 0.913042\n",
      "Train Epoch: 800 [6400/19804 (32%)]\tLoss: 0.813335\n",
      "Train Epoch: 800 [7040/19804 (35%)]\tLoss: 0.972067\n",
      "Train Epoch: 800 [7680/19804 (39%)]\tLoss: 0.900425\n",
      "Train Epoch: 800 [8320/19804 (42%)]\tLoss: 0.880214\n",
      "Train Epoch: 800 [8960/19804 (45%)]\tLoss: 0.934809\n",
      "Train Epoch: 800 [9600/19804 (48%)]\tLoss: 0.993214\n",
      "Train Epoch: 800 [10240/19804 (52%)]\tLoss: 0.926596\n",
      "Train Epoch: 800 [10880/19804 (55%)]\tLoss: 0.926359\n",
      "Train Epoch: 800 [11520/19804 (58%)]\tLoss: 0.809639\n",
      "Train Epoch: 800 [12160/19804 (61%)]\tLoss: 0.911994\n",
      "Train Epoch: 800 [12800/19804 (65%)]\tLoss: 0.886985\n",
      "Train Epoch: 800 [13440/19804 (68%)]\tLoss: 0.891624\n",
      "Train Epoch: 800 [14080/19804 (71%)]\tLoss: 0.884997\n",
      "Train Epoch: 800 [14720/19804 (74%)]\tLoss: 0.897101\n",
      "Train Epoch: 800 [15360/19804 (77%)]\tLoss: 0.912957\n",
      "Train Epoch: 800 [16000/19804 (81%)]\tLoss: 0.893242\n",
      "Train Epoch: 800 [16640/19804 (84%)]\tLoss: 0.922566\n",
      "Train Epoch: 800 [17280/19804 (87%)]\tLoss: 0.990100\n",
      "Train Epoch: 800 [17920/19804 (90%)]\tLoss: 0.946462\n",
      "Train Epoch: 800 [18560/19804 (94%)]\tLoss: 0.886856\n",
      "Train Epoch: 800 [19200/19804 (97%)]\tLoss: 0.934745\n",
      "\n",
      "Test set: Avg. loss: 0.9042, Accuracy: 2708/4950 (55%)\n",
      "\n",
      "Train Epoch: 900 [0/19804 (0%)]\tLoss: 0.942860\n",
      "Train Epoch: 900 [640/19804 (3%)]\tLoss: 0.854570\n",
      "Train Epoch: 900 [1280/19804 (6%)]\tLoss: 0.848565\n",
      "Train Epoch: 900 [1920/19804 (10%)]\tLoss: 0.871042\n",
      "Train Epoch: 900 [2560/19804 (13%)]\tLoss: 0.826473\n",
      "Train Epoch: 900 [3200/19804 (16%)]\tLoss: 0.882662\n",
      "Train Epoch: 900 [3840/19804 (19%)]\tLoss: 1.006212\n",
      "Train Epoch: 900 [4480/19804 (23%)]\tLoss: 0.962168\n",
      "Train Epoch: 900 [5120/19804 (26%)]\tLoss: 0.827421\n",
      "Train Epoch: 900 [5760/19804 (29%)]\tLoss: 0.861951\n",
      "Train Epoch: 900 [6400/19804 (32%)]\tLoss: 0.869966\n",
      "Train Epoch: 900 [7040/19804 (35%)]\tLoss: 0.986384\n",
      "Train Epoch: 900 [7680/19804 (39%)]\tLoss: 1.020311\n",
      "Train Epoch: 900 [8320/19804 (42%)]\tLoss: 0.858320\n",
      "Train Epoch: 900 [8960/19804 (45%)]\tLoss: 0.918732\n",
      "Train Epoch: 900 [9600/19804 (48%)]\tLoss: 0.773325\n",
      "Train Epoch: 900 [10240/19804 (52%)]\tLoss: 0.898602\n",
      "Train Epoch: 900 [10880/19804 (55%)]\tLoss: 0.877018\n",
      "Train Epoch: 900 [11520/19804 (58%)]\tLoss: 0.919086\n",
      "Train Epoch: 900 [12160/19804 (61%)]\tLoss: 0.691729\n",
      "Train Epoch: 900 [12800/19804 (65%)]\tLoss: 0.953671\n",
      "Train Epoch: 900 [13440/19804 (68%)]\tLoss: 0.853079\n",
      "Train Epoch: 900 [14080/19804 (71%)]\tLoss: 0.899360\n",
      "Train Epoch: 900 [14720/19804 (74%)]\tLoss: 0.916962\n",
      "Train Epoch: 900 [15360/19804 (77%)]\tLoss: 0.900088\n",
      "Train Epoch: 900 [16000/19804 (81%)]\tLoss: 0.790273\n",
      "Train Epoch: 900 [16640/19804 (84%)]\tLoss: 0.841788\n",
      "Train Epoch: 900 [17280/19804 (87%)]\tLoss: 0.884865\n",
      "Train Epoch: 900 [17920/19804 (90%)]\tLoss: 0.718996\n",
      "Train Epoch: 900 [18560/19804 (94%)]\tLoss: 0.955570\n",
      "Train Epoch: 900 [19200/19804 (97%)]\tLoss: 0.807363\n",
      "\n",
      "Test set: Avg. loss: 0.8813, Accuracy: 2860/4950 (58%)\n",
      "\n",
      "Train Epoch: 1000 [0/19804 (0%)]\tLoss: 0.922383\n",
      "Train Epoch: 1000 [640/19804 (3%)]\tLoss: 0.808451\n",
      "Train Epoch: 1000 [1280/19804 (6%)]\tLoss: 0.949365\n",
      "Train Epoch: 1000 [1920/19804 (10%)]\tLoss: 1.012626\n",
      "Train Epoch: 1000 [2560/19804 (13%)]\tLoss: 0.814073\n",
      "Train Epoch: 1000 [3200/19804 (16%)]\tLoss: 0.828533\n",
      "Train Epoch: 1000 [3840/19804 (19%)]\tLoss: 0.893480\n",
      "Train Epoch: 1000 [4480/19804 (23%)]\tLoss: 0.768531\n",
      "Train Epoch: 1000 [5120/19804 (26%)]\tLoss: 0.847436\n",
      "Train Epoch: 1000 [5760/19804 (29%)]\tLoss: 0.986375\n",
      "Train Epoch: 1000 [6400/19804 (32%)]\tLoss: 0.865867\n",
      "Train Epoch: 1000 [7040/19804 (35%)]\tLoss: 0.890263\n",
      "Train Epoch: 1000 [7680/19804 (39%)]\tLoss: 0.842874\n",
      "Train Epoch: 1000 [8320/19804 (42%)]\tLoss: 0.858372\n",
      "Train Epoch: 1000 [8960/19804 (45%)]\tLoss: 0.844803\n",
      "Train Epoch: 1000 [9600/19804 (48%)]\tLoss: 0.838867\n",
      "Train Epoch: 1000 [10240/19804 (52%)]\tLoss: 0.829874\n",
      "Train Epoch: 1000 [10880/19804 (55%)]\tLoss: 0.783805\n",
      "Train Epoch: 1000 [11520/19804 (58%)]\tLoss: 0.834587\n",
      "Train Epoch: 1000 [12160/19804 (61%)]\tLoss: 0.930544\n",
      "Train Epoch: 1000 [12800/19804 (65%)]\tLoss: 0.857680\n",
      "Train Epoch: 1000 [13440/19804 (68%)]\tLoss: 0.800782\n",
      "Train Epoch: 1000 [14080/19804 (71%)]\tLoss: 0.924801\n",
      "Train Epoch: 1000 [14720/19804 (74%)]\tLoss: 0.810102\n",
      "Train Epoch: 1000 [15360/19804 (77%)]\tLoss: 0.910346\n",
      "Train Epoch: 1000 [16000/19804 (81%)]\tLoss: 0.941968\n",
      "Train Epoch: 1000 [16640/19804 (84%)]\tLoss: 0.807439\n",
      "Train Epoch: 1000 [17280/19804 (87%)]\tLoss: 0.974701\n",
      "Train Epoch: 1000 [17920/19804 (90%)]\tLoss: 0.959959\n",
      "Train Epoch: 1000 [18560/19804 (94%)]\tLoss: 0.814474\n",
      "Train Epoch: 1000 [19200/19804 (97%)]\tLoss: 0.778778\n",
      "\n",
      "Test set: Avg. loss: 0.8341, Accuracy: 3047/4950 (62%)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.8341, Accuracy: 3047/4950 (62%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "network = SimpleLinear4(1)\n",
    "# network = Simple4()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "def train(epoch, is_print=False):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and is_print:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(is_print=False):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    if is_print:\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    if epoch % 100 == 0:\n",
    "        train(epoch, True)\n",
    "        test(True)\n",
    "    else:\n",
    "        test(False)\n",
    "test(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, cluster_data, all_embeded = get_images()\n",
    "B, weight_vec, true_labels, datapoints = get_chain_graph_data(X, y, cluster_data, all_embeded, \n",
    "                                                                  h1=1, num_neig=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# The SimpleLinear which is for MNIST experiment\n",
    "class CombinedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, h1=2048):\n",
    "        super().__init__()\n",
    "        self.models = []\n",
    "        for i in range(40):\n",
    "            model = SimpleLinear(h1)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        preds = []\n",
    "        for i, model in enumerate(self.models):\n",
    "            x = xs[i]\n",
    "            pred = model.forward(x)\n",
    "#             pred = np.argmax(model.forward(x))\n",
    "            preds.append(pred)\n",
    "        return preds\n",
    "    \n",
    "    def parameters(self, recurse: bool = True):\n",
    "        for model in self.models:\n",
    "            for name, param in model.named_parameters(recurse=recurse):\n",
    "                yield param\n",
    "\n",
    "\n",
    "n_epochs = 400\n",
    "\n",
    "train_batch_size = 100\n",
    "test_batch_size = 25\n",
    "learning_rate = 0.1\n",
    "# momentum = 0.2\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "B, weight_vec, true_labels, datapoints = get_chain_graph_data(X, y, cluster_data, all_embeded, \n",
    "                                                                  h1=1, num_neig=3)\n",
    "weight_vec = torch.tensor(weight_vec)\n",
    "\n",
    "_cluster_data = np.concatenate((cluster_data[0], cluster_data[1]))\n",
    "features = X[_cluster_data]\n",
    "labels = y[_cluster_data]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "network = CombinedModel(1)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "\n",
    "def my_loss(output, target, network, B, weight_vec):\n",
    "    loss = 0\n",
    "    E, N = B.shape\n",
    "    for idx in range(N):\n",
    "        loss += F.cross_entropy(output[idx], target[idx])\n",
    "             \n",
    "        neighs = np.where(B[:, idx] == -1)[0]\n",
    "        for e in neighs:\n",
    "            j = np.where(B[e]==1)[0][0]\n",
    "            loss += weight_vec[e] * torch.sqrt(torch.mean((\n",
    "                network.models[j].linear1.weight.data - network.models[idx].linear1.weight.data)**2))\n",
    "            loss += weight_vec[e] * torch.sqrt(torch.mean((\n",
    "                network.models[j].linear2.weight.data - network.models[idx].linear2.weight.data)**2))\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def train(epoch, datapoints, is_print=False):\n",
    "    network.train()\n",
    "    n_data = len(datapoints[0]['features'])\n",
    "    shuffle_indices = [i for i in range(n_data)]\n",
    "    random.shuffle(shuffle_indices)\n",
    "    for batch_idx in range(n_data//train_batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        splited_data = [datapoints[i]['features'][batch_idx*train_batch_size:(batch_idx+1)*train_batch_size]\n",
    "                         for i in datapoints]\n",
    "        splited_target = [datapoints[i]['label'][batch_idx*train_batch_size:(batch_idx+1)*train_batch_size]\n",
    "                         for i in datapoints]        \n",
    "        output = network(splited_data)\n",
    "        loss = my_loss(output, splited_target, network, B, weight_vec)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and is_print:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * train_batch_size, train_batch_size,\n",
    "            100. * batch_idx / (n_data//train_batch_size), loss.item()))\n",
    "            \n",
    "def test(datapoints, is_print=False):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    totals = 0\n",
    "    n_data = len(datapoints[0]['test_features'])\n",
    "    with torch.no_grad():\n",
    "        splited_data = [datapoints[i]['test_features']\n",
    "                     for i in datapoints]\n",
    "        splited_target = [datapoints[i]['test_label']\n",
    "                     for i in datapoints]\n",
    "        output = network(splited_data)\n",
    "        test_loss = my_loss(output, splited_target, network, B, weight_vec)\n",
    "        preds = [item.data.max(1, keepdim=True)[1] for item in output]\n",
    "        for i, pred in enumerate(preds):\n",
    "            correct += pred.eq(splited_target[i].data.view_as(pred)).sum()\n",
    "            totals += len(splited_target[i])\n",
    "    test_loss /= totals\n",
    "    test_losses.append(test_loss)\n",
    "    if is_print:\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, totals,\n",
    "        100. * correct / totals))\n",
    "    return correct / totals\n",
    "    \n",
    "pytorch_accs = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch, datapoints, False)\n",
    "    acc = test(datapoints, False)\n",
    "    pytorch_accs.append(float(acc))\n",
    "    \n",
    "\n",
    "# with open('experiments/data/MNIST/pytorch_train_acc.json', 'w') as f:\n",
    "#     f.write(json.dumps(pytorch_accs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IFCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: {'m': 2400, 'm_test': 400, 'p': 2, 'n': 100, 'h1': 200, 'num_epochs': 100, 'batch_size': 400, 'tau': 20, 'lr': 0.1, 'data_seed': 0, 'train_seed': 0}\n",
      "Epoch 0 tr: l 0.677 a 0.520 clct[40, 0] lr 0.100000 0.542sec(train) 0.023sec(infer)\n",
      "Epoch 0 tst: l 0.678 a 0.518 clct[40, 0] 0.015sec\n",
      "Epoch 20 tr: l 0.093 a 0.973 clct[40, 0] lr 0.100000 0.432sec(train) 0.021sec(infer)\n",
      "Epoch 20 tst: l 0.099 a 0.971 clct[40, 0] 0.015sec\n",
      "Epoch 40 tr: l 0.078 a 0.976 clct[40, 0] lr 0.100000 0.437sec(train) 0.021sec(infer)\n",
      "Epoch 40 tst: l 0.087 a 0.972 clct[40, 0] 0.015sec\n",
      "Epoch 60 tr: l 0.073 a 0.977 clct[40, 0] lr 0.100000 0.511sec(train) 0.035sec(infer)\n",
      "Epoch 60 tst: l 0.085 a 0.973 clct[40, 0] 0.023sec\n",
      "Epoch 80 tr: l 0.069 a 0.978 clct[40, 0] lr 0.100000 0.469sec(train) 0.022sec(infer)\n",
      "Epoch 80 tst: l 0.083 a 0.973 clct[40, 0] 0.016sec\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "LR_DECAY = True\n",
    "# LR_DECAY = False\n",
    "\n",
    "\n",
    "def chunk(a, i, n):\n",
    "    a2 = chunkify(a, n)\n",
    "    return a2[i]\n",
    "\n",
    "\n",
    "def chunkify(a, n):\n",
    "    # splits list into even size list of lists\n",
    "    # [1,2,3,4] -> [1,2], [3,4]\n",
    "\n",
    "    k, m = divmod(len(a), n)\n",
    "    gen = (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "    return list(gen)\n",
    "\n",
    "\n",
    "def main(datapoints):\n",
    "\n",
    "    config = get_config()\n",
    "\n",
    "    config['train_seed'] = config['data_seed']\n",
    "\n",
    "    print(\"config:\",config)\n",
    "\n",
    "    exp = TrainMNISTCluster(config, datapoints)\n",
    "    exp.setup()\n",
    "    return exp.run()\n",
    "\n",
    "\n",
    "def get_config():\n",
    "\n",
    "    config = {\n",
    "        \"m\" : 2400,            # number of machines (used for splitting the train dataset and setting the number of parallel worker machines)\n",
    "        \"m_test\" : 400,        # number of machines (used for splitting the test dataset)\n",
    "        \"p\" : 2,               # number of cluster groups (expects m % p == 0 and m_test % p == 0)\n",
    "        \"n\" : 100,             # number of images for each ma\n",
    "        \"h1\": 200,             # hidden layer size for the NN \n",
    "        \"num_epochs\": 100,     # number of data\n",
    "        \"batch_size\":400,      # batch size of local update\n",
    "        \"tau\":20,              # number of local epochs in worker machines\n",
    "        \"lr\":0.1,              # learning rate\n",
    "        \"data_seed\":0,         # random seed for generating data\n",
    "        \"train_seed\":0         # random seed for weight initiailization and training\n",
    "    }\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "class TrainMNISTCluster(object):\n",
    "    def __init__(self, config, datapoints):\n",
    "        self.config = config\n",
    "        self.datapoints = datapoints\n",
    "\n",
    "        assert self.config['m'] % self.config['p'] == 0\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "#         os.makedirs(self.config['project_dir'], exist_ok = True)\n",
    "\n",
    "#         self.result_fname = os.path.join(self.config['project_dir'], 'results.pickle')\n",
    "#         self.checkpoint_fname = os.path.join(self.config['project_dir'], 'checkpoint.pt')\n",
    "\n",
    "        self.setup_datasets(self.datapoints)\n",
    "        self.setup_models()\n",
    "\n",
    "        self.epoch = None\n",
    "        self.lr = None\n",
    "\n",
    "\n",
    "    def setup_datasets(self, datapoints):\n",
    "        \n",
    "        N = len(datapoints)\n",
    "        self.config['m'] = N\n",
    "        dataset = {}\n",
    "        dataset['config'] = self.config\n",
    "        dataset['params'] = None\n",
    "        dataset['train'] = {\n",
    "            'data': []\n",
    "        }\n",
    "        dataset['test'] = {\n",
    "            'data': []\n",
    "        }\n",
    "\n",
    "        # generate dataset for each machine\n",
    "        cluster_assignment = [0 for i in range(N//2)] + [1 for i in range(N//2)]# ex: [0,0,0,0, 1,1,1,1, 2,2,2,2] for m = 12, p = 3\n",
    "        dataset['cluster_assignment'] = cluster_assignment\n",
    "\n",
    "        for m_i in range(N):\n",
    "            p_i = cluster_assignment[m_i]\n",
    "            \n",
    "            dataset['train']['data'].append((datapoints[m_i]['features'], datapoints[m_i]['label']))\n",
    "            dataset['test']['data'].append((datapoints[m_i]['test_features'], datapoints[m_i]['test_label']))\n",
    "\n",
    "        self.dataset = dataset\n",
    "        return dataset\n",
    "\n",
    "    def setup_models(self):\n",
    "        np.random.seed(self.config['train_seed'])\n",
    "        torch.manual_seed(self.config['train_seed'])\n",
    "\n",
    "        p = self.config['p']\n",
    "\n",
    "        self.models = [ SimpleLinear(h1 = 1) for p_i in range(p)] # p models with p different params of dimension(1,d)\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        num_epochs = self.config['num_epochs']\n",
    "        lr = self.config['lr']\n",
    "\n",
    "        results = []\n",
    "        accs = []\n",
    "\n",
    "        # epoch -1\n",
    "        self.epoch = -1\n",
    "\n",
    "        result = {}\n",
    "        result['epoch'] = -1\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=True)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['train'] = res\n",
    "\n",
    "        self.print_epoch_stats(res)\n",
    "\n",
    "        t0 = time.time()\n",
    "        res = self.test(train=False)\n",
    "        t1 = time.time()\n",
    "        res['infer_time'] = t1-t0\n",
    "        result['test'] = res\n",
    "        self.print_epoch_stats(res)\n",
    "        results.append(result)\n",
    "\n",
    "        # this will be used in next epoch\n",
    "#         cluster_assign = self.dataset['cluster_assignment']\n",
    "#         cluster_assign = [random.choice([0, 1]) for i in range(len(self.dataset['cluster_assignment']))]\n",
    "        cluster_assign = [0 for i in range(len(self.dataset['cluster_assignment']))]\n",
    "\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.epoch = epoch\n",
    "\n",
    "            result = {}\n",
    "            result['epoch'] = epoch\n",
    "\n",
    "            lr = self.lr_schedule(epoch)\n",
    "            result['lr'] = lr\n",
    "\n",
    "            t0 = time.time()\n",
    "            result['train'] = self.train(cluster_assign, lr = lr)\n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=True)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            res['train_time'] = train_time\n",
    "            res['lr'] = lr\n",
    "            result['train'] = res\n",
    "\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            t0 = time.time()\n",
    "            res = self.test(train=False)\n",
    "            t1 = time.time()\n",
    "            res['infer_time'] = t1-t0\n",
    "            result['test'] = res\n",
    "            self.print_epoch_stats(res)\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "            # this will be used in next epoch's gradient update\n",
    "            cluster_assign = result['train']['cluster_assignment']\n",
    "            accs.append(result['test']['acc'])\n",
    "\n",
    "#             if epoch % 10 == 0 or epoch == num_epochs - 1 :\n",
    "#                 with open(self.result_fname, 'wb') as outfile:\n",
    "#                     pickle.dump(results, outfile)\n",
    "#                     print(f'result written at {self.result_fname}')\n",
    "#                 self.save_checkpoint()\n",
    "#                 print(f'checkpoint written at {self.checkpoint_fname}')\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        return accs\n",
    "\n",
    "    def lr_schedule(self, epoch):\n",
    "        if self.lr is None:\n",
    "            self.lr = self.config['lr']\n",
    "\n",
    "        if epoch % 100 == 0 and epoch != 0 and LR_DECAY:\n",
    "            self.lr = self.lr * 0.5\n",
    "\n",
    "        return self.lr\n",
    "\n",
    "\n",
    "    def print_epoch_stats(self, res):\n",
    "        if res['is_train']:\n",
    "            data_str = 'tr'\n",
    "        else:\n",
    "            data_str = 'tst'\n",
    "\n",
    "        if 'train_time' in res:\n",
    "            time_str = f\"{res['train_time']:.3f}sec(train) {res['infer_time']:.3f}sec(infer)\"\n",
    "        else:\n",
    "            time_str = f\"{res['infer_time']:.3f}sec\"\n",
    "\n",
    "        if 'lr' in res:\n",
    "            lr_str = f\" lr {res['lr']:4f}\"\n",
    "        else:\n",
    "            lr_str = \"\"\n",
    "\n",
    "        str0 = f\"Epoch {self.epoch} {data_str}: l {res['loss']:.3f} a {res['acc']:.3f} clct{res['cl_ct']}{lr_str} {time_str}\"\n",
    "        if self.epoch % 20 == 0:\n",
    "            print(str0)\n",
    "\n",
    "    def train(self, cluster_assign, lr):\n",
    "        VERBOSE = 0\n",
    "\n",
    "        cfg = self.config\n",
    "        m = cfg['m']\n",
    "        p = cfg['p']\n",
    "        tau = cfg['tau']\n",
    "\n",
    "        # run local update\n",
    "        t0 = time.time()\n",
    "\n",
    "\n",
    "        updated_models = []\n",
    "        for m_i in range(m):\n",
    "            if VERBOSE and m_i % 100 == 0: print(f'm {m_i}/{m} processing \\r', end ='')\n",
    "\n",
    "            (X, y) = self.load_data(m_i)\n",
    "\n",
    "            p_i = cluster_assign[m_i]\n",
    "            model = copy.deepcopy(self.models[p_i])\n",
    "\n",
    "            for step_i in range(tau):\n",
    "\n",
    "                y_logit = model(X)\n",
    "                loss = self.criterion(y_logit, y)\n",
    "\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                self.local_param_update(model, lr)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            updated_models.append(model)\n",
    "\n",
    "        t02 = time.time()\n",
    "        # print(f'running single ..took {t02-t01:.3f}sec')\n",
    "\n",
    "\n",
    "        t1 = time.time()\n",
    "        if VERBOSE: print(f'local update {t1-t0:.3f}sec')\n",
    "\n",
    "        # apply gradient update\n",
    "        t0 = time.time()\n",
    "\n",
    "        local_models = [[] for p_i in range(p)]\n",
    "        for m_i in range(m):\n",
    "            p_i = cluster_assign[m_i]\n",
    "            local_models[p_i].append(updated_models[m_i])\n",
    "\n",
    "        for p_i, models in enumerate(local_models):\n",
    "            if len(models) >0:\n",
    "                self.global_param_update(models, self.models[p_i])\n",
    "        t1 = time.time()\n",
    "\n",
    "        if VERBOSE: print(f'global update {t1-t0:.3f}sec')\n",
    "\n",
    "    def check_local_model_loss(self, local_models):\n",
    "        # for debugging\n",
    "        m = self.config['m']\n",
    "\n",
    "        losses = []\n",
    "        for m_i in range(m):\n",
    "            (X, y) = self.load_data(m_i)\n",
    "            \n",
    "            y_logit = local_models[m_i](X)\n",
    "            loss = self.criterion(y_logit, y)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        return np.array(losses)\n",
    "\n",
    "\n",
    "    def get_inference_stats(self, train = True):\n",
    "        cfg = self.config\n",
    "        if train:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset\n",
    "        else:\n",
    "            m = cfg['m']\n",
    "            dataset = self.dataset\n",
    "\n",
    "        p = cfg['p']\n",
    "\n",
    "\n",
    "        num_data = 0\n",
    "        losses = {}\n",
    "        corrects = {}\n",
    "        for m_i in range(m):\n",
    "            (X, y) = self.load_data(m_i, train=train) # load batch data rotated\n",
    "            for p_i in range(p):\n",
    "                y_logit = self.models[p_i](X)\n",
    "                loss = self.criterion(y_logit, y) # loss of\n",
    "                n_correct = self.n_correct(y_logit, y)\n",
    "\n",
    "                losses[(m_i,p_i)] = loss.item()\n",
    "                corrects[(m_i,p_i)] = n_correct\n",
    "\n",
    "            num_data += X.shape[0]\n",
    "\n",
    "        # calculate loss and cluster the machines\n",
    "        cluster_assign = []\n",
    "        for m_i in range(m):\n",
    "            machine_losses = [ losses[(m_i,p_i)] for p_i in range(p) ]\n",
    "            min_p_i = np.argmin(machine_losses)\n",
    "            cluster_assign.append(min_p_i)\n",
    "\n",
    "        # calculate optimal model's loss, acc over all models\n",
    "        min_corrects = []\n",
    "        min_losses = []\n",
    "        for m_i, p_i in enumerate(cluster_assign):\n",
    "\n",
    "            min_loss = losses[(m_i,p_i)]\n",
    "            min_losses.append(min_loss)\n",
    "\n",
    "            min_correct = corrects[(m_i,p_i)]\n",
    "            min_corrects.append(min_correct)\n",
    "\n",
    "        loss = np.mean(min_losses)\n",
    "        acc = np.sum(min_corrects) / num_data\n",
    "\n",
    "\n",
    "        # check cluster assignment acc\n",
    "        cl_acc = np.mean(np.array(cluster_assign) == np.array(dataset['cluster_assignment']))\n",
    "        cl_ct = [np.sum(np.array(cluster_assign) == p_i ) for p_i in range(p)]\n",
    "\n",
    "        res = {} # results\n",
    "        # res['losses'] = losses\n",
    "        # res['corrects'] = corrects\n",
    "        res['cluster_assignment'] = cluster_assign\n",
    "        res['num_data'] = num_data\n",
    "        res['loss'] = loss\n",
    "        res['acc'] = acc\n",
    "        res['cl_acc'] = cl_acc\n",
    "        res['cl_ct'] = cl_ct\n",
    "        res['is_train'] = train\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return res\n",
    "\n",
    "    def n_correct(self, y_logit, y):\n",
    "        _, predicted = torch.max(y_logit.data, 1)\n",
    "        correct = (predicted == y).sum().item()\n",
    "\n",
    "        return correct\n",
    "\n",
    "    def load_data(self, m_i, train=True):\n",
    "        # this part is very fast since its just rearranging models\n",
    "#         cfg = self.config\n",
    "\n",
    "#         if train:\n",
    "#             dataset = self.dataset['train']\n",
    "#         else:\n",
    "#             dataset = self.dataset['test']\n",
    "\n",
    "#         indices = dataset['data_indices'][m_i]\n",
    "#         p_i = dataset['cluster_assign'][m_i]\n",
    "\n",
    "#         X_batch = dataset['X'][indices]\n",
    "#         y_batch = dataset['y'][indices]\n",
    "\n",
    "#         # k : how many times rotate 90 degree\n",
    "#         # k =1 : 90 , k=2 180, k=3 270\n",
    "\n",
    "#         if cfg['p'] == 4:\n",
    "#             k = p_i\n",
    "#         elif cfg['p'] == 2:\n",
    "#             k = (p_i % 2) * 2\n",
    "#         elif cfg['p'] == 1:\n",
    "#             k = 0\n",
    "#         else:\n",
    "#             raise NotImplementedError(\"only p=1,2,4 supported\")\n",
    "\n",
    "#         X_batch2 = torch.rot90(X_batch, k=int(k), dims = (1,2))\n",
    "#         X_batch3 = X_batch2.reshape(-1, 28 * 28)\n",
    "\n",
    "        if train:\n",
    "            X_batch3, y_batch = self.dataset['train']['data'][m_i]\n",
    "        else:\n",
    "            X_batch3, y_batch = self.dataset['test']['data'][m_i]\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "        return X_batch3, y_batch\n",
    "\n",
    "\n",
    "    def local_param_update(self, model, lr):\n",
    "\n",
    "        # gradient update manually\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data -= lr * param.grad\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # import ipdb; ipdb.set_trace() # we need to check the output of name, check if duplicate exists\n",
    "\n",
    "\n",
    "    def global_param_update(self, local_models, global_model):\n",
    "\n",
    "        # average of each weight\n",
    "\n",
    "        weights = {}\n",
    "\n",
    "        for m_i, local_model in enumerate(local_models):\n",
    "            for name, param in local_model.named_parameters():\n",
    "                if name not in weights:\n",
    "                    weights[name] = torch.zeros_like(param.data)\n",
    "\n",
    "                weights[name] += param.data\n",
    "\n",
    "        for name, param in global_model.named_parameters():\n",
    "            weights[name] /= len(local_models)\n",
    "            param.data = weights[name]\n",
    "\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "\n",
    "\n",
    "    def test(self, train=False):\n",
    "\n",
    "        return self.get_inference_stats(train=train)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        models_to_save = [model.state_dict() for model in self.models]\n",
    "        torch.save({'models':models_to_save}, self.checkpoint_fname)\n",
    "\n",
    "        \n",
    "ifca_scores = main(datapoints)\n",
    "\n",
    "with open('experiments/data/MNIST/ifca_train_acc_journal.json', 'w') as f:\n",
    "    f.write(json.dumps(ifca_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/data/MNIST/ifca_train_acc_journal.json', 'r') as f:\n",
    "    ifca_scores  = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/data/MNIST/pFedMe_train_acc.json', 'r') as f:\n",
    "    pfedme_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "E, N = B.shape\n",
    "samplingset = [i for i in range(N)]\n",
    "iter_scores, predicted_w, predicted_u = algorithm_1(70, B, weight_vec, datapoints, true_labels, samplingset, \n",
    "                         1.0, penalty_func, calculate_score=True)\n",
    "print(get_algorithm1_error(datapoints, predicted_w, samplingset))\n",
    "\n",
    "alg1_scores = [item['test'] for item in iter_scores]\n",
    "\n",
    "with open('experiments/data/MNIST/alg1_train_acc_journal.json', 'w') as f:\n",
    "    f.write(json.dumps(alg1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('experiments/data/MNIST/alg1_train_acc_journal.json', 'r') as f:\n",
    "    alg1_scores = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plt.figure()\n",
    "x = [i for i in range(70)]\n",
    "plt.plot(x, alg1_scores[:len(x)], label='primal-dual')\n",
    "plt.plot(x, ifca_scores[:len(x)], label='ifca')\n",
    "plt.plot(x, pfedme_data[:len(x)], label='pfedme')\n",
    "plt.plot(x, pytorch_accs[:len(x)], label='pytorch')\n",
    "# plt.title('Comparing algorithms')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('test accuracy')\n",
    "# plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# f.savefig(\"experiments/data/MNIST/comparing_algs_acc.pdf\", bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
